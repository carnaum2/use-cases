{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from datetime import date, timedelta, datetime\n",
    "\n",
    "from numpy import nan as np_nan\n",
    "\n",
    "from pyspark.sql.functions import udf, col, decode, when, lit, lower, translate\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('201703', '201610')\n"
     ]
    }
   ],
   "source": [
    "month_for_verifying_in_pospago = \"201703\"\n",
    "\n",
    "month_for_getting_prepaid_data = (datetime.strptime(month_for_verifying_in_pospago, \"%Y%m\") \n",
    "                                  - timedelta(4*365/12)).strftime(\"%Y%m\")\n",
    "\n",
    "print(month_for_verifying_in_pospago, month_for_getting_prepaid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "useful_columns_from_acFinalPrepago = [\"FECHA_EJECUCION\",\n",
    "                                      \"MSISDN\",\n",
    "                                      \"NUM_DOCUMENTO_CLIENTE\",\n",
    "                                      \"NACIONALIDAD\", \n",
    "                                      \"NUM_PREPAGO\", \n",
    "                                      \"NUM_POSPAGO\",\n",
    "                                      #\"Tipo_Documento_Cliente\", Very uninformed\n",
    "                                      \"Tipo_Documento_Comprador\",\n",
    "                                      \"X_FECHA_NACIMIENTO\"]\n",
    "\n",
    "def empty_str_to_null(string_value):\n",
    "    if string_value == \"\":\n",
    "        result = None\n",
    "    elif string_value == u\"\":\n",
    "        result = None\n",
    "    else:\n",
    "        result = string_value\n",
    "    return result\n",
    "\n",
    "empty_string_to_null = udf(empty_str_to_null, StringType())\n",
    "\n",
    "def get_customer_age_raw(birthdate, month_for_getting_prepaid_data):\n",
    "        if birthdate is None:\n",
    "            return np_nan\n",
    "        parsed_date = datetime.strptime(str(int(birthdate)), \"%Y%m%d\")\n",
    "        timedelta = datetime.strptime(month_for_getting_prepaid_data, \"%Y%m\") - parsed_date\n",
    "        return timedelta.days / 365.25\n",
    "\n",
    "def get_customer_age_udf(birthdate, month):\n",
    "    return udf(partial(get_customer_age_raw, month_for_getting_prepaid_data=month), DoubleType())(birthdate)\n",
    "\n",
    "def subsitute_crappy_characters(string_column):\n",
    "    return (string_column\n",
    "            .replace(u\"\\ufffd\", u\"ñ\")\n",
    "            # add more here in the future\n",
    "           )\n",
    "\n",
    "substitute_crappy_characters_udf = udf(subsitute_crappy_characters, StringType())\n",
    "\n",
    "acFinalPrepago = (sqlContext.read.table(\"raw_es.vf_pre_ac_final\")\n",
    "                  .where((col(\"year\") == int(month_for_getting_prepaid_data[:4]))\n",
    "                         & (col(\"month\") == int(month_for_getting_prepaid_data[4:]))\n",
    "                        )\n",
    "                  .select(*useful_columns_from_acFinalPrepago)\n",
    "                  .withColumn(\"X_FECHA_NACIMIENTO\", empty_string_to_null(col(\"X_FECHA_NACIMIENTO\")))\n",
    "                  .withColumn(\"NUM_DOCUMENTO_CLIENTE\", empty_string_to_null(col(\"NUM_DOCUMENTO_CLIENTE\")))\n",
    "                  .withColumn(\"age_in_years\", get_customer_age_udf(col(\"X_FECHA_NACIMIENTO\"),\n",
    "                                                                   month_for_getting_prepaid_data)\n",
    "                             )\n",
    "                  .withColumn(\"NACIONALIDAD\", substitute_crappy_characters_udf(col(\"NACIONALIDAD\")))\n",
    "                  #.withColumn(\"\")\n",
    "                 )\n",
    "\n",
    "acFinalPrepago = acFinalPrepago.repartition(int(acFinalPrepago.count() / 500)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_frequent_countries = [u\"España\",\n",
    "                           u\"Marruecos\",\n",
    "                           u\"Rumania\",\n",
    "                           u\"Colombia\",\n",
    "                           u\"Italia\",\n",
    "                           u\"Ecuador\",\n",
    "                           u\"Alemania\",\n",
    "                           u\"Estados Unidos\",\n",
    "                           u\"Francia\",\n",
    "                           u\"Brasil\",\n",
    "                           u\"Argentina\",\n",
    "                           u\"Afganistan\",\n",
    "                           u\"Bolivia\",\n",
    "                           u\"Gran Bretaña\",\n",
    "                           u\"Portugal\",\n",
    "                           u\"Paraguay\",\n",
    "                           u\"China\",\n",
    "                           u\"Gran Bretana\",\n",
    "                           u\"Venezuela\",\n",
    "                           u\"Honduras\",\n",
    "                           u\"Corea del Sur\"]\n",
    "\n",
    "\n",
    "acFinalPrepago_2 = acFinalPrepago.withColumn(\"NACIONALIDAD\", when(col(\"NACIONALIDAD\").isin(most_frequent_countries),\n",
    "                                                                  col(\"NACIONALIDAD\")\n",
    "                                                                 ).otherwise(lit(\"Other\"))\n",
    "                                            )\n",
    "\n",
    "for column in most_frequent_countries + [\"Other\"]:\n",
    "    acFinalPrepago_2 = acFinalPrepago_2.withColumn(\"nationality_\"+column.replace(\" \",\"_\"), \n",
    "                                                   when(col(\"NACIONALIDAD\")==lit(column),1.0)\n",
    "                                                   .otherwise(0.0)\n",
    "                                                   )\n",
    "    \n",
    "acFinalPrepago_2 = acFinalPrepago_2.drop(\"NACIONALIDAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_frequent_documents = [\"nif\",\n",
    "                           \"pasaporte\",\n",
    "                           \"tarj_residente\",\n",
    "                           \"cif\"]\n",
    "\n",
    "\n",
    "acFinalPrepago_3 = acFinalPrepago_2.withColumn(\"Tipo_Documento_Comprador\",\n",
    "                                               translate(translate(lower(col(\"Tipo_Documento_Comprador\")), \".\", \"\"),\" \",\"_\")\n",
    "                                              )\n",
    "\n",
    "\n",
    "acFinalPrepago_3 = acFinalPrepago_3.withColumn(\"Tipo_Documento_Comprador\", \n",
    "                                               when(col(\"Tipo_Documento_Comprador\").isin(most_frequent_documents),\n",
    "                                                    col(\"Tipo_Documento_Comprador\")\n",
    "                                                   ).otherwise(lit(\"Other\"))\n",
    "                                            )\n",
    "\n",
    "for column in most_frequent_documents + [\"Other\"]:\n",
    "    acFinalPrepago_3 = acFinalPrepago_3.withColumn(\"documenttype_\"+column.replace(\" \",\"_\"), \n",
    "                                                   when(col(\"Tipo_Documento_Comprador\")==lit(column),1.0)\n",
    "                                                   .otherwise(0.0)\n",
    "                                                   )\n",
    "\n",
    "acFinalPrepago_3 = acFinalPrepago_3.drop(\"Tipo_Documento_Comprador\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acFinalPospago_4monthsLater = (sqlContext.read.table(\"raw_es.vf_pos_ac_final\")\n",
    "                               .where((col(\"year\") == int(month_for_verifying_in_pospago[:4]))\n",
    "                                      & (col(\"month\") == int(month_for_verifying_in_pospago[4:]))\n",
    "                                     )\n",
    "                               .select(\"x_id_red\",\"x_num_ident\")\n",
    "                               .na.drop()\n",
    "                              )\n",
    "\n",
    "acFinalPospago_4monthsLater = acFinalPospago_4monthsLater.repartition(int(acFinalPospago_4monthsLater.count() / 500)+1)\n",
    "\n",
    "join_prepago_pospago = (acFinalPrepago_3.join(acFinalPospago_4monthsLater,\n",
    "                                              on=(acFinalPrepago_3[\"MSISDN\"]==acFinalPospago_4monthsLater[\"x_id_red\"])\n",
    "                                               & (acFinalPrepago_3[\"NUM_DOCUMENTO_CLIENTE\"]==acFinalPospago_4monthsLater[\"x_num_ident\"]),\n",
    "                                             how=\"left\"\n",
    "                                            )\n",
    "                        .withColumn(\"migrated_to_postpaid\", (~col(\"x_id_red\").isNull()).cast(IntegerType()))\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "useful_columns_from_tarificadorPre = ['MSISDN',\n",
    "                                      'MOU',\n",
    "                                      'TOTAL_LLAMADAS',\n",
    "                                      'TOTAL_SMS',\n",
    "                                      'MOU_Week',\n",
    "                                      'LLAM_Week',\n",
    "                                      'SMS_Week',\n",
    "                                      'MOU_Weekend',\n",
    "                                      'LLAM_Weekend',\n",
    "                                      'SMS_Weekend',\n",
    "                                      'MOU_VF',\n",
    "                                      'LLAM_VF',\n",
    "                                      'SMS_VF',\n",
    "                                      'MOU_Fijo',\n",
    "                                      'LLAM_Fijo',\n",
    "                                      'SMS_Fijo',\n",
    "                                      'MOU_OOM',\n",
    "                                      'LLAM_OOM',\n",
    "                                      'SMS_OOM',\n",
    "                                      'MOU_Internacional',\n",
    "                                      'LLAM_Internacional',\n",
    "                                      'SMS_Internacional',\n",
    "                                      'ActualVolume',\n",
    "                                      'Num_accesos',\n",
    "                                      'Plan',\n",
    "                                      'Num_Cambio_Planes',\n",
    "                                      #'TOP_Internacional', # No idea of what is\n",
    "                                      'LLAM_COMUNIDAD_SMART',\n",
    "                                      'MOU_COMUNIDAD_SMART',\n",
    "                                      'LLAM_SMS_COMUNIDAD_SMART',\n",
    "                                      'Flag_Uso_Etnica',\n",
    "                                      'cuota_SMART8',\n",
    "                                      'cuota_SMART12',\n",
    "                                      'cuota_SMART16']\n",
    "\n",
    "\n",
    "tarificadorPre = (sqlContext.read.table(\"raw_es.vf_pre_info_tarif\")\n",
    "                  .where((col(\"year\") == int(month_for_getting_prepaid_data[:4]))\n",
    "                         & (col(\"month\") == int(month_for_getting_prepaid_data[4:]))\n",
    "                        )\n",
    "                  .select(*useful_columns_from_tarificadorPre)\n",
    "                 )\n",
    "\n",
    "tarificadorPre = tarificadorPre.repartition(int(tarificadorPre.count() / 500)+1)\n",
    "\n",
    "plan_categories = ['PPIB7',\n",
    "                   'PPFCL',\n",
    "                   'PPIB4',\n",
    "                   'PPXS8',\n",
    "                   'PPIB8',\n",
    "                   'PPIB9',\n",
    "                   'PPTIN',\n",
    "                   'PPIB1',\n",
    "                   'PPVIS',\n",
    "                   'PPREX',\n",
    "                   'PPIB5',\n",
    "                   'PPREU',\n",
    "                   'PPRET',\n",
    "                   'PPFCS',\n",
    "                   'PPIB6',\n",
    "                   'PPREY',\n",
    "                   'PPVSP',\n",
    "                   'PPIB2',\n",
    "                   'PPIB3',\n",
    "                   'PPRE2',\n",
    "                   'PPRE5',\n",
    "                   'PPVE2',\n",
    "                   'PPVE1',\n",
    "                   'PPRES',\n",
    "                   'PPJ24',\n",
    "                   'PPVE3',\n",
    "                   'PPJAT',\n",
    "                   'PPJMI']\n",
    "\n",
    "tarificadorPre_2 = tarificadorPre.withColumn(\"Plan\", \n",
    "                                             when(col(\"Plan\").isin(plan_categories),\n",
    "                                                  col(\"Plan\")\n",
    "                                                 ).otherwise(lit(\"Other\"))\n",
    "                                            )\n",
    "\n",
    "for column in plan_categories + [\"Other\"]:\n",
    "    tarificadorPre_2 = tarificadorPre_2.withColumn(\"plan_\"+column.replace(\" \",\"_\"), \n",
    "                                                   when(col(\"Plan\")==lit(column), 1.0)\n",
    "                                                   .otherwise(0.0)\n",
    "                                                   )\n",
    "\n",
    "tarificadorPre_3 = tarificadorPre_2.drop(\"Plan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prepaid_dataset_1 = join_prepago_pospago.join(tarificadorPre_3,\n",
    "                                               how=\"inner\",\n",
    "                                               on=\"MSISDN\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|migrated_to_postpaid|  count|\n",
      "+--------------------+-------+\n",
      "|                   1|  15545|\n",
      "|                   0|3066276|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prepaid_dataset_1.groupBy(\"migrated_to_postpaid\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_columns = ['NUM_PREPAGO',\n",
    "                   'NUM_POSPAGO',\n",
    "                   'age_in_years',\n",
    "                   #'documenttype_Other',\n",
    "                   'documenttype_cif',\n",
    "                   #'documenttype_nif',\n",
    "                   'documenttype_pasaporte',\n",
    "                   #'documenttype_tarj_residente',\n",
    "                   #'nationality_Afganistan',\n",
    "                   #'nationality_Alemania',\n",
    "                   #'nationality_Argentina',\n",
    "                   #'nationality_Bolivia',\n",
    "                   #'nationality_Brasil',\n",
    "                   #'nationality_China',\n",
    "                   #'nationality_Colombia',\n",
    "                   #'nationality_Corea_del_Sur',\n",
    "                   #'nationality_Ecuador',\n",
    "                   #'nationality_España',\n",
    "                   #'nationality_Estados_Unidos',\n",
    "                   #'nationality_Francia',\n",
    "                   #'nationality_Gran_Bretana',\n",
    "                   #'nationality_Gran_Bretaña',\n",
    "                   #'nationality_Honduras',\n",
    "                   #'nationality_Italia',\n",
    "                   #'nationality_Marruecos',\n",
    "                   #'nationality_Other',\n",
    "                   #'nationality_Paraguay',\n",
    "                   #'nationality_Portugal',\n",
    "                   #'nationality_Rumania',\n",
    "                   #'nationality_Venezuela',\n",
    "                   #'migrated_to_postpaid',\n",
    "                   'MOU',\n",
    "                   'TOTAL_LLAMADAS',\n",
    "                   'TOTAL_SMS',\n",
    "                   'MOU_Week',\n",
    "                   'LLAM_Week',\n",
    "                   'SMS_Week',\n",
    "                   'MOU_Weekend',\n",
    "                   'LLAM_Weekend',\n",
    "                   #'SMS_Weekend',\n",
    "                   'MOU_VF',\n",
    "                   'LLAM_VF',\n",
    "                   'SMS_VF',\n",
    "                   'MOU_Fijo',\n",
    "                   'LLAM_Fijo',\n",
    "                   #'SMS_Fijo',\n",
    "                   'MOU_OOM',\n",
    "                   'LLAM_OOM',\n",
    "                   'SMS_OOM',\n",
    "                   'MOU_Internacional',\n",
    "                   #'LLAM_Internacional',\n",
    "                   #'SMS_Internacional',\n",
    "                   'ActualVolume',\n",
    "                   'Num_accesos',\n",
    "                   #'Num_Cambio_Planes',\n",
    "                   'LLAM_COMUNIDAD_SMART',\n",
    "                   'MOU_COMUNIDAD_SMART',\n",
    "                   'LLAM_SMS_COMUNIDAD_SMART',\n",
    "                   #'Flag_Uso_Etnica',\n",
    "                   'cuota_SMART8',\n",
    "                   #'cuota_SMART12',\n",
    "                   #'cuota_SMART16',\n",
    "                   'plan_PPFCL',\n",
    "                   #'plan_PPFCS',\n",
    "                   #'plan_PPIB1',\n",
    "                   #'plan_PPIB2',\n",
    "                   #'plan_PPIB3',\n",
    "                   #'plan_PPIB4',\n",
    "                   #'plan_PPIB5',\n",
    "                   #'plan_PPIB6',\n",
    "                   #'plan_PPIB7',\n",
    "                   'plan_PPIB8',\n",
    "                   #'plan_PPIB9',\n",
    "                   #'plan_PPJ24',\n",
    "                   #'plan_PPJAT',\n",
    "                   #'plan_PPJMI',\n",
    "                   #'plan_PPRE2',\n",
    "                   #'plan_PPRE5',\n",
    "                   #'plan_PPRES',\n",
    "                   #'plan_PPRET',\n",
    "                   #'plan_PPREU',\n",
    "                   #'plan_PPREX',\n",
    "                   #'plan_PPREY',\n",
    "                   'plan_PPTIN',\n",
    "                   #'plan_PPVE1',\n",
    "                   #'plan_PPVE2',\n",
    "                   #'plan_PPVE3',\n",
    "                   #'plan_PPVIS',\n",
    "                   #'plan_PPVSP',\n",
    "                   #'plan_PPXS8'\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prepaid_dataset_2 = prepaid_dataset_1\n",
    "\n",
    "for column in feature_columns:\n",
    "    prepaid_dataset_2 = prepaid_dataset_2.withColumn(column, col(column).cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = prepaid_dataset_2.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198.2337777777778"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_pos_weight = float(train.where(col(\"migrated_to_postpaid\")==0).count()) / train.where(col(\"migrated_to_postpaid\")==1).count()\n",
    "\n",
    "scale_pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "\n",
    "vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "train_assembled = vector_assembler.transform(train)\n",
    "\n",
    "string_indexer_label = StringIndexer(inputCol=\"migrated_to_postpaid\", outputCol=\"label\")\n",
    "string_indexer_label_model = string_indexer_label.fit(train_assembled)\n",
    "\n",
    "train_prepared = string_indexer_label_model.transform(train_assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(train_prepared\n",
    " .repartition(int(train_prepared.count() / 500)+1)\n",
    " .write\n",
    " .saveAsTable(\"tests_es.training_set_pre2post_201610\",\n",
    "              format=\"parquet\",\n",
    "              mode=\"overwrite\")\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.linalg.VectorUDT"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_assembled.schema[\"features\"].dataType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o808.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 73.0 failed 4 times, most recent failure: Lost task 0.3 in stage 73.0 (TID 193321, vgddp531hr.dc.sedc.internal.vodafone.com, executor 670): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/python/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/python/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/python/pyspark/worker.py\", line 106, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/python/pyspark/worker.py\", line 92, in <lambda>\n    mapper = lambda a: udf(*a)\n  File \"/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/python/pyspark/worker.py\", line 68, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"<ipython-input-28-a3b24b173ce8>\", line 3, in <lambda>\nAttributeError: 'SparseVector' object has no attribute 'toDense'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:124)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:68)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:779)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:779)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1624)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1613)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1893)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1906)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:347)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:39)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2193)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2546)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2192)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2199)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1935)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1934)\n\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2576)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:1934)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2149)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:239)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/python/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/python/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/python/pyspark/worker.py\", line 106, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/python/pyspark/worker.py\", line 92, in <lambda>\n    mapper = lambda a: udf(*a)\n  File \"/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/python/pyspark/worker.py\", line 68, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"<ipython-input-28-a3b24b173ce8>\", line 3, in <lambda>\nAttributeError: 'SparseVector' object has no attribute 'toDense'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:124)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:68)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:779)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:779)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-a3b24b173ce8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtoDense_udf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mudf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVectorUDT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain_assembled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoDense_udf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"features\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate)\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[1;33m+\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \"\"\"\n\u001b[1;32m--> 287\u001b[1;33m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/python/lib/py4j-0.10.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/python/lib/py4j-0.10.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o808.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 73.0 failed 4 times, most recent failure: Lost task 0.3 in stage 73.0 (TID 193321, vgddp531hr.dc.sedc.internal.vodafone.com, executor 670): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/python/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/python/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/python/pyspark/worker.py\", line 106, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/python/pyspark/worker.py\", line 92, in <lambda>\n    mapper = lambda a: udf(*a)\n  File \"/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/python/pyspark/worker.py\", line 68, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"<ipython-input-28-a3b24b173ce8>\", line 3, in <lambda>\nAttributeError: 'SparseVector' object has no attribute 'toDense'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:124)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:68)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:779)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:779)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1624)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1613)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1893)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1906)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:347)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:39)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2193)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2546)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2192)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2199)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1935)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1934)\n\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2576)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:1934)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2149)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:239)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/python/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/python/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/python/pyspark/worker.py\", line 106, in <lambda>\n    func = lambda _, it: map(mapper, it)\n  File \"/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/python/pyspark/worker.py\", line 92, in <lambda>\n    mapper = lambda a: udf(*a)\n  File \"/opt/cloudera/parcels/SPARK2-2.0.0.cloudera2-1.cdh5.7.0.p0.118100/lib/spark2/python/pyspark/worker.py\", line 68, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"<ipython-input-28-a3b24b173ce8>\", line 3, in <lambda>\nAttributeError: 'SparseVector' object has no attribute 'toDense'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:124)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1.apply(BatchEvalPythonExec.scala:68)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:779)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:779)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.ml.linalg import VectorUDT\n",
    "\n",
    "#toDense_udf = udf(lambda x: x.toDense(), VectorUDT())\n",
    "\n",
    "#train_assembled.select(toDense_udf(col(\"features\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
