{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App for training + saving ML model for Pre2Post\n",
    "\n",
    "\n",
    "In machine learning, the first thing to do is to train a model. Then you usually save it for further usage (predictions).\n",
    "\n",
    "In this notebook, we will train a bunch of models, select the best one, and save it in order to use it in notebooks 2 & 3. In order to run this, the following assumptions should be fullfilled:\n",
    "\n",
    "1. The following tables exist in Hive's metastore:\n",
    "    + `raw_es.vf_pre_ac_final`\n",
    "    + `raw_es.vf_pos_ac_final`\n",
    "    + `raw_es.vf_pre_info_tarif`\n",
    "    + `raw_es.campaign_msisdncontacthist`\n",
    "    + `raw_es.campaign_msisdnresponsehist`\n",
    "    + `raw_es.anonymisation_lookup_msisdn` --> This is a temporary workaround. Chris knows about it.\n",
    "2. Date and time clocks in the Spark driver are accurate. This is important because we rely heavilly on date in order to compute *which is the next month after the current one* and that sort of stuff.\n",
    "\n",
    "With all that said, let's start:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The one and only line that we have to change between executions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand better the whole workflow:\n",
    "\n",
    "1. We use historical data from one month (eg. 2017/04) to train a couple of models, and keep/save the best one (this is done in notebook #1, which you do not currently have). Once we have got a good model, we just save it to HDFS. This *does not has to happen every single month*; as long as the model is not extremely outated, there should be no need for running this all months. This notebook covers that.\n",
    "2. We then use historical data from next month (2017/05) to get an unbiased measure on how good our saved model is. This should run all months; it is always important to keep track of model performance on a monthly basis.\n",
    "3. Finally, in order to make predictions, we will use the most recent data to predict customer behaviour. This can be ran as many times as we want, when needed (usually once a month).\n",
    "\n",
    "Given that this notebook (#2) has to run on a monthly basis, I took care to structure the code so we just need to change one line from one monthly execution to another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "months = [\"201703\",\"201704\",\"201705\", \"201706\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and app setup\n",
    "\n",
    "Your usual stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard Library stuff:\n",
    "from functools import partial\n",
    "from datetime import date, timedelta, datetime\n",
    "\n",
    "# Numpy stuff\n",
    "from numpy import nan as np_nan\n",
    "\n",
    "# Spark stuff\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import (udf, col, decode, when, lit, lower, \n",
    "                                   translate, count, sum as sql_sum, max as sql_max, isnull)\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .appName(\"Pre2Post Spain training\")\n",
    "         .master(\"yarn\")\n",
    "         .config(\"spark.submit.deployMode\", \"client\")\n",
    "         .config(\"spark.ui.showConsoleProgress\", \"true\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate()\n",
    "         )\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data imports and first transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function will be very handy:\n",
    "def get_next_month(dt):\n",
    "    \"\"\"\n",
    "    Given a yyyymm string, returns the yyyymm string\n",
    "    for the next month.\n",
    "    \"\"\"\n",
    "    current_month = datetime.strptime(dt, \"%Y%m\")\n",
    "    return (datetime(current_month.year, current_month.month, 28) + timedelta(days=4)).strftime(\"%Y%m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('201610', '201612', '201703')\n",
      "('201611', '201701', '201704')\n",
      "('201612', '201702', '201705')\n",
      "('201701', '201703', '201706')\n"
     ]
    }
   ],
   "source": [
    "historical_dataframes = []\n",
    "\n",
    "for month_for_verifying_in_pospago in months:\n",
    "    # For the next 4 months\n",
    "    time_delta = (datetime.strptime(month_for_verifying_in_pospago, \"%Y%m\")\n",
    "                                      - timedelta(5*365/12))\n",
    "\n",
    "    if time_delta.day > 16:\n",
    "        if time_delta.month > 11:\n",
    "            date_prepaid = datetime(time_delta.year + 1, 1, 1)\n",
    "        else:\n",
    "            date_prepaid = datetime(time_delta.year, time_delta.month + 1, 1)\n",
    "    else:\n",
    "        date_prepaid = time_delta\n",
    "\n",
    "    month_for_getting_prepaid_data = date_prepaid.strftime(\"%Y%m\")\n",
    "\n",
    "    # Do they migrate next month? (Well, really next 2 months due to delays in lists):\n",
    "    month_for_appearance_in_pospago = get_next_month(get_next_month(month_for_getting_prepaid_data))\n",
    "    print(month_for_getting_prepaid_data, month_for_appearance_in_pospago, month_for_verifying_in_pospago)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # I know, it is atrocious, but the most straightforward/simple/robust\n",
    "    # way to select columns is with a simple, hardcoded Python list:\n",
    "\n",
    "    useful_columns_from_acFinalPrepago = [\"FECHA_EJECUCION\",\n",
    "                                          \"MSISDN\",\n",
    "                                          \"NUM_DOCUMENTO_CLIENTE\",\n",
    "                                          \"NACIONALIDAD\",\n",
    "                                          \"NUM_PREPAGO\",\n",
    "                                          \"NUM_POSPAGO\",\n",
    "                                          #\"Tipo_Documento_Cliente\", Very uninformed\n",
    "                                          \"Tipo_Documento_Comprador\",\n",
    "                                          \"X_FECHA_NACIMIENTO\"]\n",
    "\n",
    "    # Lots of tables in Hive have empty string instead\n",
    "    # of null for missing values in StringType columns:\n",
    "    def empty_str_to_null(string_value):\n",
    "        if string_value == \"\":\n",
    "            result = None\n",
    "        elif string_value == u\"\":\n",
    "            result = None\n",
    "        else:\n",
    "            result = string_value\n",
    "        return result\n",
    "\n",
    "    # We register previous function as a udf:\n",
    "    empty_string_to_null = udf(empty_str_to_null, StringType())\n",
    "\n",
    "    # Function that returns customer age out of his/her birthdate:\n",
    "    def get_customer_age_raw(birthdate, month_for_getting_prepaid_data):\n",
    "            if birthdate is None:\n",
    "                return np_nan\n",
    "            parsed_date = datetime.strptime(str(int(birthdate)), \"%Y%m%d\")\n",
    "            timedelta = datetime.strptime(month_for_getting_prepaid_data, \"%Y%m\") - parsed_date\n",
    "            return timedelta.days / 365.25\n",
    "\n",
    "    # We register previous function as a udf:\n",
    "    def get_customer_age_udf(birthdate, month):\n",
    "        return udf(partial(get_customer_age_raw, month_for_getting_prepaid_data=month), DoubleType())(birthdate)\n",
    "\n",
    "    # Self-explanatory.\n",
    "    def subsitute_crappy_characters(string_column):\n",
    "        \"\"\"\n",
    "        I really hate charset encoding.\n",
    "        \"\"\"\n",
    "        return (string_column\n",
    "                .replace(u\"\\ufffd\", u\"Ã±\")\n",
    "                # add more here in the future\n",
    "               )\n",
    "\n",
    "    # We register previous function as a udf:\n",
    "    substitute_crappy_characters_udf = udf(subsitute_crappy_characters, StringType())\n",
    "\n",
    "    # And we finally read raw_es.vf_pre_ac_final,\n",
    "    # filtering by date, and with new columns\n",
    "    # that we create using our UDFs:\n",
    "    acFinalPrepago = (spark.read.table(\"raw_es.vf_pre_ac_final\")\n",
    "                      .where((col(\"year\") == int(month_for_getting_prepaid_data[:4]))\n",
    "                             & (col(\"month\") == int(month_for_getting_prepaid_data[4:]))\n",
    "                            )\n",
    "                      #.select(*useful_columns_from_acFinalPrepago)\n",
    "                      .withColumn(\"X_FECHA_NACIMIENTO\", empty_string_to_null(col(\"X_FECHA_NACIMIENTO\")))\n",
    "                      .withColumn(\"NUM_DOCUMENTO_CLIENTE\", empty_string_to_null(col(\"NUM_DOCUMENTO_CLIENTE\")))\n",
    "                      .withColumn(\"NUM_DOCUMENTO_COMPRADOR\", empty_string_to_null(col(\"NUM_DOCUMENTO_COMPRADOR\")))\n",
    "                      .withColumn(\"age_in_years\", get_customer_age_udf(col(\"X_FECHA_NACIMIENTO\"),\n",
    "                                                                       month_for_getting_prepaid_data)\n",
    "                                 )\n",
    "                      .withColumn(\"NACIONALIDAD\", substitute_crappy_characters_udf(col(\"NACIONALIDAD\")))\n",
    "                     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # In this acFinalPrepago DF we have a column (nationality)\n",
    "    # with lot's of different values (high cardinality), which\n",
    "    # is terrible for ML models, so we will get the most frequent\n",
    "    # countries, and replace all others with \"Other\":\n",
    "    most_frequent_countries = [u\"EspaÃ±a\",\n",
    "                               u\"Marruecos\",\n",
    "                               u\"Rumania\",\n",
    "                               u\"Colombia\",\n",
    "                               u\"Italia\",\n",
    "                               u\"Ecuador\",\n",
    "                               u\"Alemania\",\n",
    "                               u\"Estados Unidos\",\n",
    "                               u\"Francia\",\n",
    "                               u\"Brasil\",\n",
    "                               u\"Argentina\",\n",
    "                               u\"Afganistan\",\n",
    "                               u\"Bolivia\",\n",
    "                               u\"Gran BretaÃ±a\",\n",
    "                               u\"Portugal\",\n",
    "                               u\"Paraguay\",\n",
    "                               u\"China\",\n",
    "                               u\"Gran Bretana\",\n",
    "                               u\"Venezuela\",\n",
    "                               u\"Honduras\",\n",
    "                               u\"Corea del Sur\"]\n",
    "\n",
    "\n",
    "    acFinalPrepago = acFinalPrepago.withColumn(\"NACIONALIDAD\", when(col(\"NACIONALIDAD\").isin(most_frequent_countries),\n",
    "                                                                      col(\"NACIONALIDAD\")\n",
    "                                                                     ).otherwise(lit(\"Other\"))\n",
    "                                                )\n",
    "\n",
    "\n",
    "\n",
    "    # Now we read another table: raw_es.vf_pos_ac_final,\n",
    "    # with some yyyymm predicate, and only two columns:\n",
    "    acFinalPospago_nextMonth = (spark.read.table(\"raw_es.vf_pos_ac_final\")\n",
    "                                   .where((col(\"year\") == int(month_for_appearance_in_pospago[:4]))\n",
    "                                          & (col(\"month\") == int(month_for_appearance_in_pospago[4:]))\n",
    "                                         )\n",
    "                                   .select(\"x_id_red\",\"x_num_ident\")\n",
    "                                   .na.drop()\n",
    "                                   .withColumnRenamed(\"x_id_red\", \"x_id_red_NextMonth\")\n",
    "                                   .withColumnRenamed(\"x_num_ident\", \"x_num_ident_NextMonth\")\n",
    "                                  )\n",
    "\n",
    "\n",
    "\n",
    "    # And yet again, we read the same table, but with \n",
    "    # different yyyymm predicate:\n",
    "    acFinalPospago_4monthsLater = (spark.read.table(\"raw_es.vf_pos_ac_final\")\n",
    "                                   .where((col(\"year\") == int(month_for_verifying_in_pospago[:4]))\n",
    "                                          & (col(\"month\") == int(month_for_verifying_in_pospago[4:]))\n",
    "                                         )\n",
    "                                   .select(\"x_id_red\",\"x_num_ident\")\n",
    "                                   .na.drop()\n",
    "                                  )\n",
    "\n",
    "\n",
    "    # And we perform one join:\n",
    "\n",
    "    join_prepago_pospago_1 = (acFinalPrepago\n",
    "                             .join(acFinalPospago_nextMonth,\n",
    "                                   how=\"left\",\n",
    "                                   on=(acFinalPrepago[\"MSISDN\"]==acFinalPospago_nextMonth[\"x_id_red_NextMonth\"])\n",
    "                                    & (acFinalPrepago[\"NUM_DOCUMENTO_COMPRADOR\"]==acFinalPospago_nextMonth[\"x_num_ident_NextMonth\"])\n",
    "                                 )\n",
    "                           )\n",
    "\n",
    "\n",
    "\n",
    "    # And another:\n",
    "\n",
    "    join_prepago_pospago_2 = (join_prepago_pospago_1\n",
    "                             .join(acFinalPospago_4monthsLater,\n",
    "                                  on=(join_prepago_pospago_1[\"x_id_red_NextMonth\"]==acFinalPospago_4monthsLater[\"x_id_red\"])\n",
    "                                      & (join_prepago_pospago_1[\"x_num_ident_NextMonth\"]==acFinalPospago_4monthsLater[\"x_num_ident\"]),\n",
    "                                  how=\"left\"\n",
    "                                 )\n",
    "                             )\n",
    "\n",
    "\n",
    "\n",
    "    # Beautiful datetime manipulations:\n",
    "    datetime_for_appearance_in_pospago = datetime.strptime(month_for_appearance_in_pospago, \"%Y%m\")\n",
    "\n",
    "    datetime_min_contact = datetime((datetime_for_appearance_in_pospago - timedelta(days=8)).year,\n",
    "                                    (datetime_for_appearance_in_pospago - timedelta(days=8)).month,\n",
    "                                    1)\n",
    "\n",
    "    datetime_max_contact = datetime((datetime_for_appearance_in_pospago + timedelta(days=8)).year,\n",
    "                                    (datetime_for_appearance_in_pospago + timedelta(days=8)).month,\n",
    "                                    7)\n",
    "\n",
    "    month_for_getting_prepaid_data, month_for_appearance_in_pospago, datetime_min_contact, datetime_max_contact\n",
    "\n",
    "\n",
    "    # Now, we read raw_es.campaign_msisdncontacthist\n",
    "    # with yyyymm predicates and other stupid business filters\n",
    "    # that I do not fully understand:\n",
    "    contacts = (spark.read.table(\"raw_es.campaign_msisdncontacthist\")\n",
    "                .where(col(\"contactdatetime\") >= datetime_min_contact.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                .where(col(\"contactdatetime\") < datetime_max_contact.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                #.where(col(\"contactdatetime\") >= \"2017-05-01 00:00:00\")\n",
    "                #.where(col(\"contactdatetime\") < \"2017-06-08 00:00:00\")\n",
    "                .where(col(\"CampaignCode\").isin(['AUTOMMES_PXXXP_MIG_PROPENSOS']))\n",
    "                .where(~(col(\"Canal\").like(\"PO%\")))\n",
    "                .where(~(col(\"Canal\").like(\"NBA%\")))\n",
    "                .where(col(\"Canal\")==\"TEL\")\n",
    "                .where(col(\"Flag_Borrado\") == 0)\n",
    "                )\n",
    "\n",
    "    # We read raw_es.campaign_msisdnresponsehist:\n",
    "    responses = (spark.read.table(\"raw_es.campaign_msisdnresponsehist\")\n",
    "                )\n",
    "\n",
    "    # We are going to join contacts DF with responses DF, and they\n",
    "    # happen to have columns with same names (but not same data),\n",
    "    # so we rename all columns in responses DF, adding responses_\n",
    "    # at the beggining:\n",
    "    responses_columns = [(column,\"responses_\"+column) for column in responses.columns]\n",
    "\n",
    "    for existing, new in responses_columns:\n",
    "        responses = responses.withColumnRenamed(existing, new)\n",
    "\n",
    "    # Beautiful join. I do not expect you to understand\n",
    "    # it, because neither do I. I just translated some\n",
    "    # Teradata Query that VF Spain's CVM department uses\n",
    "    # to Spark DF syntax. It runs quite fast...\n",
    "    contacts_and_responses = (contacts.join(responses,\n",
    "                                           how=\"left_outer\",\n",
    "                                           on=(contacts[\"TREATMENTCODE\"]==responses[\"responses_TREATMENTCODE\"])\n",
    "                                              & (contacts[\"MSISDN\"]==responses[\"responses_MSISDN\"])\n",
    "                                              & (contacts[\"CampaignCode\"]==responses[\"responses_CampaignCode\"])\n",
    "                                           )\n",
    "                                      .groupBy(\"MSISDN\",\n",
    "                                               \"CAMPAIGNCODE\",\n",
    "                                               \"CREATIVIDAD\",\n",
    "                                               \"CELLCODE\",\n",
    "                                               \"CANAL\",\n",
    "                                               \"contactdatetime\",\n",
    "                                               \"responses_responsedatetime\")\n",
    "                                      .agg(sql_max(\"responses_responsedatetime\"))\n",
    "                                      .select(col(\"MSISDN\"), \n",
    "                                              col(\"CAMPAIGNCODE\"), \n",
    "                                              col(\"CREATIVIDAD\"), \n",
    "                                              col(\"CELLCODE\"), \n",
    "                                              col(\"CANAL\"), \n",
    "                                              col(\"contactdatetime\").alias(\"DATEID\"),\n",
    "                                              when(isnull(col(\"max(responses_responsedatetime)\")), \"0\")\n",
    "                                                  .otherwise(\"1\").alias(\"EsRespondedor\")\n",
    "\n",
    "                                             )\n",
    "                             ).withColumnRenamed(\"msisdn\",\"msisdn_contact\")\n",
    "\n",
    "\n",
    "    # Here is the lookup table\n",
    "    lookup_msisdn = spark.read.table(\"raw_es.anonymisation_lookup_msisdn\")\n",
    "\n",
    "    # Join between customer data and lookup table:\n",
    "    join_prepago_pospago_3 = join_prepago_pospago_2.join(lookup_msisdn,\n",
    "                                                         how=\"left\",\n",
    "                                                         on=join_prepago_pospago_2[\"x_id_red\"]==lookup_msisdn[\"cvm_value\"]\n",
    "                                                        )\n",
    "\n",
    "    # Another join, where we\n",
    "    # also create the target column for our \n",
    "    # machine learning model:\n",
    "    join_prepago_pospago_4 = (join_prepago_pospago_3.join(contacts_and_responses,\n",
    "                                                        how=\"left\",\n",
    "                                                        on=join_prepago_pospago_3[\"correct_value\"]==contacts_and_responses[\"msisdn_contact\"]\n",
    "                                                       )\n",
    "                                                  .withColumn(\"migrated_to_postpaid\", ((~col(\"msisdn_contact\").isNull())\n",
    "                                                                                      #&(~col(\"EsRespondedor\").isNull())\n",
    "                                                                                      ).cast(IntegerType()))\n",
    "                            )\n",
    "\n",
    "    join_prepago_pospago_5 = (join_prepago_pospago_2\n",
    "                              .where(col(\"x_id_red\").isNull())\n",
    "                              .join(lookup_msisdn,\n",
    "                                    how=\"left\",\n",
    "                                    on=join_prepago_pospago_2[\"MSISDN\"]==lookup_msisdn[\"cvm_value\"]\n",
    "                                   )\n",
    "                              )\n",
    "\n",
    "    join_prepago_pospago_6 = (join_prepago_pospago_5.join(contacts_and_responses,\n",
    "                                                         how=\"left\",\n",
    "                                                         on=join_prepago_pospago_3[\"correct_value\"]==contacts_and_responses[\"msisdn_contact\"]\n",
    "                                                         )\n",
    "                              .withColumn(\"migrated_to_postpaid\", lit(\"0\").cast(IntegerType()))\n",
    "                              .where(~(col(\"EsRespondedor\").isNull()))\n",
    "                              )\n",
    "\n",
    "    join_prepago_pospago = (join_prepago_pospago_4\n",
    "                            .where(col(\"migrated_to_postpaid\")==1)\n",
    "                            .union(join_prepago_pospago_6)\n",
    "                            )\n",
    "\n",
    "\n",
    "    # We will read raw_es.vf_pre_info_tarif.\n",
    "    # The columns that we care about are the following:\n",
    "\n",
    "    useful_columns_from_tarificadorPre = ['MSISDN',\n",
    "                                          'MOU',\n",
    "                                          'TOTAL_LLAMADAS',\n",
    "                                          'TOTAL_SMS',\n",
    "                                          'MOU_Week',\n",
    "                                          'LLAM_Week',\n",
    "                                          'SMS_Week',\n",
    "                                          'MOU_Weekend',\n",
    "                                          'LLAM_Weekend',\n",
    "                                          'SMS_Weekend',\n",
    "                                          'MOU_VF',\n",
    "                                          'LLAM_VF',\n",
    "                                          'SMS_VF',\n",
    "                                          'MOU_Fijo',\n",
    "                                          'LLAM_Fijo',\n",
    "                                          'SMS_Fijo',\n",
    "                                          'MOU_OOM',\n",
    "                                          'LLAM_OOM',\n",
    "                                          'SMS_OOM',\n",
    "                                          'MOU_Internacional',\n",
    "                                          'LLAM_Internacional',\n",
    "                                          'SMS_Internacional',\n",
    "                                          'ActualVolume',\n",
    "                                          'Num_accesos',\n",
    "                                          'Plan',\n",
    "                                          'Num_Cambio_Planes',\n",
    "                                          #'TOP_Internacional', # No idea of what is\n",
    "                                          'LLAM_COMUNIDAD_SMART',\n",
    "                                          'MOU_COMUNIDAD_SMART',\n",
    "                                          'LLAM_SMS_COMUNIDAD_SMART',\n",
    "                                          'Flag_Uso_Etnica',\n",
    "                                          'cuota_SMART8',\n",
    "                                          'cuota_SMART12',\n",
    "                                          'cuota_SMART16']\n",
    "\n",
    "\n",
    "    # Read raw_es.vf_pre_info_tarif + yyyymm predicates + \n",
    "    # column selection:\n",
    "    tarificadorPre = (spark.read.table(\"raw_es.vf_pre_info_tarif\")\n",
    "                      .where((col(\"year\") == int(month_for_getting_prepaid_data[:4]))\n",
    "                             & (col(\"month\") == int(month_for_getting_prepaid_data[4:]))\n",
    "                            )\n",
    "                      .select(*useful_columns_from_tarificadorPre)\n",
    "                     )\n",
    "\n",
    "    \n",
    "    \n",
    "    # Just as it happend with Nationlity column,\n",
    "    # Plan is a column with very high cardenality.\n",
    "    # We will replace any category not included\n",
    "    # in the following list with \"Other\":\n",
    "    plan_categories = ['PPIB7',\n",
    "                       'PPFCL',\n",
    "                       'PPIB4',\n",
    "                       'PPXS8',\n",
    "                       'PPIB8',\n",
    "                       'PPIB9',\n",
    "                       'PPTIN',\n",
    "                       'PPIB1',\n",
    "                       'PPVIS',\n",
    "                       'PPREX',\n",
    "                       'PPIB5',\n",
    "                       'PPREU',\n",
    "                       'PPRET',\n",
    "                       'PPFCS',\n",
    "                       'PPIB6',\n",
    "                       'PPREY',\n",
    "                       'PPVSP',\n",
    "                       'PPIB2',\n",
    "                       'PPIB3',\n",
    "                       'PPRE2',\n",
    "                       'PPRE5',\n",
    "                       'PPVE2',\n",
    "                       'PPVE1',\n",
    "                       'PPRES',\n",
    "                       'PPJ24',\n",
    "                       'PPVE3',\n",
    "                       'PPJAT',\n",
    "                       'PPJMI']\n",
    "\n",
    "    tarificadorPre_2 = tarificadorPre.withColumn(\"Plan\",\n",
    "                                                 when(col(\"Plan\").isin(plan_categories),\n",
    "                                                      col(\"Plan\")\n",
    "                                                     ).otherwise(lit(\"Other\"))\n",
    "                                                )\n",
    "\n",
    "    # Only one step left:\n",
    "    prepaid_dataset_1 = join_prepago_pospago.join(tarificadorPre_2,\n",
    "                                                   how=\"inner\",\n",
    "                                                   on=\"MSISDN\")\n",
    "    \n",
    "    historical_dataframes.append(prepaid_dataset_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prepaid_dataset_1 = historical_dataframes[0]\n",
    "\n",
    "for dataframe in historical_dataframes[1:]:\n",
    "    prepaid_dataset_1 = prepaid_dataset_1.union(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now, we have only integrated different sources, done some data cleansing, validation and preparation according to business rules. Now, we have to perform further preprocessing before feeding data to our machine learning model trained in notebook #1.\n",
    "\n",
    "Let's start by removing some columns that, after a lot of local testing, we decided that are pretty much useless.\n",
    "\n",
    "Also, we will separate numeric columns (IntegerType, DoubleType) from categorical columns (StringType), since it is a requirement to treat them differently before feeding them to any machine learning model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numeric_columns = ['NUM_PREPAGO',\n",
    "                   'NUM_POSPAGO',\n",
    "                   'age_in_years',\n",
    "                   #'documenttype_Other',\n",
    "                   #'documenttype_cif',\n",
    "                   #'documenttype_nif',\n",
    "                   #'documenttype_pasaporte',\n",
    "                   #'documenttype_tarj_residente',\n",
    "                   #'nationality_Afganistan',\n",
    "                   #'nationality_Alemania',\n",
    "                   #'nationality_Argentina',\n",
    "                   #'nationality_Bolivia',\n",
    "                   #'nationality_Brasil',\n",
    "                   #'nationality_China',\n",
    "                   #'nationality_Colombia',\n",
    "                   #'nationality_Corea_del_Sur',\n",
    "                   #'nationality_Ecuador',\n",
    "                   #'nationality_EspaÃ±a',\n",
    "                   #'nationality_Estados_Unidos',\n",
    "                   #'nationality_Francia',\n",
    "                   #'nationality_Gran_Bretana',\n",
    "                   #'nationality_Gran_BretaÃ±a',\n",
    "                   #'nationality_Honduras',\n",
    "                   #'nationality_Italia',\n",
    "                   #'nationality_Marruecos',\n",
    "                   #'nationality_Other',\n",
    "                   #'nationality_Paraguay',\n",
    "                   #'nationality_Portugal',\n",
    "                   #'nationality_Rumania',\n",
    "                   #'nationality_Venezuela',\n",
    "                   #'migrated_to_postpaid',\n",
    "                   'MOU',\n",
    "                   'TOTAL_LLAMADAS',\n",
    "                   'TOTAL_SMS',\n",
    "                   'MOU_Week',\n",
    "                   'LLAM_Week',\n",
    "                   'SMS_Week',\n",
    "                   'MOU_Weekend',\n",
    "                   'LLAM_Weekend',\n",
    "                   'SMS_Weekend',\n",
    "                   'MOU_VF',\n",
    "                   'LLAM_VF',\n",
    "                   'SMS_VF',\n",
    "                   'MOU_Fijo',\n",
    "                   'LLAM_Fijo',\n",
    "                   'SMS_Fijo',\n",
    "                   'MOU_OOM',\n",
    "                   'LLAM_OOM',\n",
    "                   'SMS_OOM',\n",
    "                   'MOU_Internacional',\n",
    "                   'LLAM_Internacional',\n",
    "                   'SMS_Internacional',\n",
    "                   'ActualVolume',\n",
    "                   'Num_accesos',\n",
    "                   'Num_Cambio_Planes',\n",
    "                   'LLAM_COMUNIDAD_SMART',\n",
    "                   'MOU_COMUNIDAD_SMART',\n",
    "                   'LLAM_SMS_COMUNIDAD_SMART',\n",
    "                   #'Flag_Uso_Etnica',\n",
    "                   'cuota_SMART8',\n",
    "                   #'cuota_SMART12',\n",
    "                   #'cuota_SMART16',\n",
    "                   #'plan_PPFCL',\n",
    "                   #'plan_PPFCS',\n",
    "                   #'plan_PPIB1',\n",
    "                   #'plan_PPIB2',\n",
    "                   #'plan_PPIB3',\n",
    "                   #'plan_PPIB4',\n",
    "                   #'plan_PPIB5',\n",
    "                   #'plan_PPIB6',\n",
    "                   #'plan_PPIB7',\n",
    "                   #'plan_PPIB8',\n",
    "                   #'plan_PPIB9',\n",
    "                   #'plan_PPJ24',\n",
    "                   #'plan_PPJAT',\n",
    "                   #'plan_PPJMI',\n",
    "                   #'plan_PPRE2',\n",
    "                   #'plan_PPRE5',\n",
    "                   #'plan_PPRES',\n",
    "                   #'plan_PPRET',\n",
    "                   #'plan_PPREU',\n",
    "                   #'plan_PPREX',\n",
    "                   #'plan_PPREY',\n",
    "                   #'plan_PPTIN',\n",
    "                   #'plan_PPVE1',\n",
    "                   #'plan_PPVE2',\n",
    "                   #'plan_PPVE3',\n",
    "                   #'plan_PPVIS',\n",
    "                   #'plan_PPVSP',\n",
    "                   #'plan_PPXS8'\n",
    "                  ]\n",
    "\n",
    "categorical_columns = [\"tipo_documento_comprador\", \"NACIONALIDAD\", \"Plan\"]\n",
    "\n",
    "# We just rename our big DF...\n",
    "prepaid_dataset_2 = prepaid_dataset_1\n",
    "\n",
    "# In order to perform an easy, recursive\n",
    "# typecasting:\n",
    "for column in numeric_columns:\n",
    "    prepaid_dataset_2 = prepaid_dataset_2.withColumn(column, col(column).cast(DoubleType()))\n",
    "    \n",
    "# Good old repartition for underpartitioned tables + \n",
    "# disk persistence:\n",
    "prepaid_dataset_2 = (prepaid_dataset_2\n",
    "                     #.repartition(int(prepaid_dataset_2.count() / 50000)+1)\n",
    "                     .persist(StorageLevel.DISK_ONLY_2)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start with the ML stuff. First of all, we have to treat categorical columns differently from numeric ones.\n",
    "\n",
    "In order for our ML model to \"understand\" correctly these columns, we have to pass them through a Spark ML transformer called StringIndexer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "\n",
    "string_indexer_document = (StringIndexer(inputCol=\"tipo_documento_comprador\", outputCol=\"documentType_indexed\")\n",
    "                           .fit(prepaid_dataset_2)\n",
    "                          )\n",
    "                           \n",
    "string_indexer_nation = (StringIndexer(inputCol=\"NACIONALIDAD\", outputCol=\"nationality_indexed\")\n",
    "                         .fit(prepaid_dataset_2)\n",
    "                         )\n",
    "                         \n",
    "string_indexer_plan = (StringIndexer(inputCol=\"Plan\", outputCol=\"tariffPlan_indexed\")\n",
    "                       .fit(prepaid_dataset_2)\n",
    "                       )\n",
    "                       \n",
    "string_indexer_label = (StringIndexer(inputCol=\"migrated_to_postpaid\", outputCol=\"label\")\n",
    "                        .fit(prepaid_dataset_2)\n",
    "                        )\n",
    "\n",
    "# A list with the new columns that these \n",
    "# StringIndexers generate (except for the\n",
    "# label one, which has to be treated differently):\n",
    "categorical_columns_indexed = [\"documentType_indexed\", \"nationality_indexed\", \"tariffPlan_indexed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This \"recursive\" function call\n",
    "# applies all StringIndexer transformations in\n",
    "# only one statement.\n",
    "#\n",
    "# Beautiful, huh?\n",
    "prepaid_dataset_3 = string_indexer_label.transform(\n",
    "    string_indexer_plan.transform (\n",
    "        string_indexer_nation.transform (\n",
    "            string_indexer_document.transform (\n",
    "                prepaid_dataset_2)\n",
    "        )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filling with extreme value\n",
    "#prepaid_dataset_3 = prepaid_dataset_3.na.fill(-9999.0, subset=[\"age_in_years\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In (supervised) machine learning, you pretty much always\n",
    "# have to separate your dataset in two subsets, \n",
    "# the training one and the testing one.\n",
    "\n",
    "# Furthermore, we will also use another testing methodology,\n",
    "# which is the whole notebook #2. This allows us to really\n",
    "# make sure that our model is robust and stable.\n",
    "train, test = prepaid_dataset_3.randomSplit([0.8, 0.2])\n",
    "\n",
    "#number_of_partitions_train = int(train.count() / 50000)+1\n",
    "\n",
    "# Good old repartition for underpartitioned tables:\n",
    "#train = train.repartition(number_of_partitions_train).cache()\n",
    "#test = test.repartition(int(test.count() / 50000)+1).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One column (`age_in_years`) in both `train` and `test` DFs has a very uninformed column (with lots of missing values). It turns out that most ML models cannot handle nulls, so we will have to *impute* that column (which means replacing the nulls with some value).\n",
    "\n",
    "Here we will do *median imputation* (replacing nulls with the median of the values of that column, excluding the nulls for the median computation obviously).\n",
    "\n",
    "The correct way to perform this median imputation is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## NOT IN USE. See above\n",
    "\n",
    "# We compute the median age in the\n",
    "# train DF:\n",
    "age_median_train = (train\n",
    "                    .na.drop(subset=[\"age_in_years\"])\n",
    "                    .approxQuantile(\"age_in_years\",\n",
    "                                    probabilities=[0.5],\n",
    "                                    relativeError=0.0\n",
    "                                   )\n",
    "                   )[0]\n",
    "\n",
    "# And we impute this value in BOTH train and test.\n",
    "# No, this is not an error, but rather a technique\n",
    "# to deal with a common ML problem called \"data leakage\",\n",
    "# which leads to overfitting (another common ML problem):\n",
    "train_filled = train.na.fill(age_median_train, subset=[\"age_in_years\"])\n",
    "test_filled = test.na.fill(age_median_train, subset=[\"age_in_years\"])\n",
    "\n",
    "\n",
    "#train_filled = train\n",
    "#test_filled = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all our columns prepared for the model, we have to `VectorAssemble` them before feeding them to the ML model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vector_assembler = VectorAssembler(inputCols=numeric_columns + categorical_columns_indexed, outputCol=\"features\")\n",
    "\n",
    "train_assembled = vector_assembler.transform(train_filled)\n",
    "test_assembled = vector_assembler.transform(test_filled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Machine Learning hyperparameter tuning through Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Random Forest for this project.\n",
    "\n",
    "Given that the Random Forest accepts lots of different configurations, we have to try a couple of them and see which one performs the best.\n",
    "\n",
    "This is done in Spark through a *CrossValidator* object, which depends on an *Estimator* (a model, in this case our Random Forest) + a *ParamGridBuilder* (the different Random Forest configurations that we want to try) + an *Evaluator* (a metric that will be used to decide which Random Forest configuration is the best one).\n",
    "\n",
    "We will start by creating our Random Forest Estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\",\n",
    "                            labelCol=\"label\",\n",
    "                            maxBins=128,\n",
    "                            maxMemoryInMB=4076,\n",
    "                            cacheNodeIds=True,\n",
    "                            checkpointInterval=5\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will define our Param Grid, and all the different configurations that we want to try. The more configurations we try, the more time the whole CrossValidation process will take.\n",
    "\n",
    "After a lot of *pruning*, I have determined that the following configurations are a good compromise between processing time and good results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "hyperparam_grid_pipeline_random_forest = (ParamGridBuilder()\n",
    "                                          .addGrid(rf.maxDepth, [14, 12, 10, 8, 6])\n",
    "                                          #.addGrid(rf.maxDepth, [3])\n",
    "                                          .addGrid(rf.numTrees, [256])\n",
    "                                          #.addGrid(rf.numTrees, [5])\n",
    "                                          .addGrid(rf.featureSubsetStrategy, [\"all\",\"0.6\",\"onethird\",\"0.1\",\"sqrt\"])\n",
    "                                          #.addGrid(rf.featureSubsetStrategy, [\"sqrt\"])\n",
    "                                          .addGrid(rf.minInstancesPerNode, [1,6,12,32,64])\n",
    "                                          .build()\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the recursive behaviour of Random Forest, Spark gives us the option to perform model checkpointing in order to remove some pressure from executor RAM. It is important to set this checkpoint directory, although the path itself does not really matter (any temporary directory in HDFS is ok) because it will be removed when the Spark process finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feel free to change it at will.\n",
    "sc.setCheckpointDir(\"hdfs:///user/jsotovi2/spark_checkpoints/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define our Evaluator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "random_forest_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\",\n",
    "                                                        labelCol=\"label\",\n",
    "                                                        metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, the CrossValidator which uses all the other components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "cross_validator_pipeline_random_forest = CrossValidator(estimator=rf,\n",
    "                                                        estimatorParamMaps=hyperparam_grid_pipeline_random_forest,\n",
    "                                                        evaluator=random_forest_evaluator,\n",
    "                                                        numFolds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done. Now, let the process run!\n",
    "\n",
    "NOTE: this will take quite some time (70h approx):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_validator_model_rf = cross_validator_pipeline_random_forest.fit(train_assembled.coalesce(16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Get best model from cross validation and its results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are done with our cross validation. Now, there are a couple things in our TODO list:\n",
    "\n",
    "1. Get the model which performed the best\n",
    "2. Save the configuration for this best model. This is important for the future, because it will help us troubleshoot and determine other useful configurations in future releases (so, this configurations will be read by a human being at some point)\n",
    "3. Save the performance results (using both the train and test DFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is the best model:\n",
    "best_rf = cross_validator_model_rf.bestModel\n",
    "\n",
    "# The configuration of this \"winner\" model, as a string that we will\n",
    "# save to HDFS as a TextFile for human consumption and/or logging:\n",
    "string_best_model = best_rf._call_java(\"parent\").extractParamMap().toString()\n",
    "\n",
    "# Performance results:\n",
    "auc_train = random_forest_evaluator.evaluate(best_rf.transform(train_assembled))\n",
    "\n",
    "auc_test = random_forest_evaluator.evaluate(best_rf.transform(test_assembled))\n",
    "\n",
    "print(auc_train)\n",
    "print(auc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(zip(numeric_columns + categorical_columns_indexed, best_rf.featureImportances.toArray()),\n",
    "       key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(zip(cross_validator_model_rf.avgMetrics, cross_validator_model_rf.getEstimatorParamMaps()),\n",
    "       key=lambda x: -x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost done. But if we really want to take advantage of best practises, we *should* re-train our \"winner\" model using both the train and test DFs at the same time.\n",
    "\n",
    "It turns out that this part has to be quite manual due to some Spark API limitations, but that's OK. First of all, we will *manually* extract the configurations that happen to be the best ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_max_depth = best_rf._call_java(\"parent\").getMaxDepth()\n",
    "best_num_trees = best_rf._call_java(\"parent\").getNumTrees()\n",
    "best_num_features = best_rf._call_java(\"parent\").getFeatureSubsetStrategy()\n",
    "best_min_instances_per_node = best_rf._call_java(\"parent\").getMinInstancesPerNode()\n",
    "\n",
    "print(best_max_depth, best_num_trees, best_num_features, best_min_instances_per_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"DONEEE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we will train again a Random Forest, using this configuration, and the whole train + test DFs.\n",
    "\n",
    "But there is one thing left: we have to recompute and re-impute the median, now using the whole train + test DFs (which happens to be no other but `prepaid_dataset_2`, since `prepaid_dataset_3` already has the StringIndexer + VectorAssembler transformers applied, and we actually wan to re-apply them also):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Impute median in whole dataset\n",
    "age_median_whole = (prepaid_dataset_2\n",
    "                    .na.drop(subset=[\"age_in_years\"])\n",
    "                    .approxQuantile(\"age_in_years\",\n",
    "                                    probabilities=[0.5],\n",
    "                                    relativeError=0.0\n",
    "                                   )\n",
    "                   )[0]\n",
    "\n",
    "#age_median_whole = -9999.0\n",
    "\n",
    "prepaid_dataset_4_filled = prepaid_dataset_2.na.fill(age_median_whole, subset=[\"age_in_years\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the StringIndexers again, but now using our new full DF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string_indexer_document_whole = (StringIndexer(inputCol=\"tipo_documento_comprador\", outputCol=\"documentType_indexed\")\n",
    "                                 .fit(prepaid_dataset_4_filled)\n",
    "                                )\n",
    "                           \n",
    "string_indexer_nation_whole = (StringIndexer(inputCol=\"NACIONALIDAD\", outputCol=\"nationality_indexed\")\n",
    "                               .fit(prepaid_dataset_4_filled)\n",
    "                              )\n",
    "                         \n",
    "string_indexer_plan_whole = (StringIndexer(inputCol=\"Plan\", outputCol=\"tariffPlan_indexed\")\n",
    "                             .fit(prepaid_dataset_4_filled)\n",
    "                            )\n",
    "                       \n",
    "string_indexer_label_whole = (StringIndexer(inputCol=\"migrated_to_postpaid\", outputCol=\"label\")\n",
    "                              .fit(prepaid_dataset_4_filled)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we apply them in conjunction to our VectorAssembler (which we don't have to re-recreate; the *old* one is fine):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prepaid_dataset_5 = vector_assembler.transform(\n",
    "    string_indexer_label_whole.transform(\n",
    "        string_indexer_plan_whole.transform(\n",
    "            string_indexer_nation_whole.transform(\n",
    "                string_indexer_document_whole.transform(\n",
    "                    prepaid_dataset_4_filled)\n",
    "            ))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our `prepaid_dataset_5` is ready. We can now train our Random Forest with the best configuration parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_rf_trained = RandomForestClassifier(featuresCol=\"features\",\n",
    "                                          labelCol=\"label\",\n",
    "                                          maxBins=128,\n",
    "                                          maxMemoryInMB=4076,\n",
    "                                          cacheNodeIds=True,\n",
    "                                          checkpointInterval=1,\n",
    "                                          featureSubsetStrategy=best_num_features,\n",
    "                                          maxDepth=best_max_depth,\n",
    "                                          numTrees=best_num_trees,\n",
    "                                          minInstancesPerNode=best_min_instances_per_node\n",
    "                                         ).fit(prepaid_dataset_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final model is trained, and ready to be further tested (in notebook #2) and used for actual predictions (notebook #3). We just have to save it in order to be able to use it in other Spark programs.\n",
    "\n",
    "But rather than only saving the Random Forest itself, we will save a Machine Learning Pipeline, with all our StringIndexers + VectorAssembler + Random Forest. When used, this Pipeline will apply each of the elements sequentially to any provided dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "final_pipeline_model = PipelineModel([string_indexer_document_whole,\n",
    "                                      string_indexer_nation_whole,\n",
    "                                      string_indexer_plan_whole,\n",
    "                                      string_indexer_label_whole,\n",
    "                                      vector_assembler,\n",
    "                                      final_rf_trained])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is prepared. We will save two files:\n",
    "\n",
    "1. A TextFile containing the human-readable best configuration for our model + model performance metrics + the median value for the age, because we will want in the future to impute the exact same value to that column (again, in order to prevent data leakage).\n",
    "2. Our Pipeline with all the transformers + the Random Forest.\n",
    "\n",
    "We will name the files with the current timestamp, which will make easier the task of retreiveing the latest model + results in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files_surname = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Best Randfom Forest config + \n",
    "# results in AUC for both train and test\n",
    "# + median value for age:\n",
    "rdd_results = sc.parallelize([(\"month_of_training\", month_for_verifying_in_pospago),\n",
    "                              (\"best_model\",string_best_model), \n",
    "                              (\"auc_train\",auc_train), \n",
    "                              (\"auc_test\",auc_test),\n",
    "                              (\"age_median_value\",age_median_whole)])\n",
    "\n",
    "rdd_results.saveAsTextFile(\"hdfs:///user/jsotovi2/pre2post_v2/best_model_pre2post_results_\"\n",
    "                           + files_surname + \".txt\")\n",
    "\n",
    "# Save Pipeline\n",
    "final_pipeline_model.save(\"hdfs:///user/jsotovi2/pre2post_v2/best_model_pre2post_\"\n",
    "                          + files_surname + \".sparkModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Not really needed anymore\n",
    "prepaid_dataset_1.unpersist()\n",
    "train.unpersist()\n",
    "test.unpersist()\n",
    "\n",
    "spark.stop()\n",
    "\n",
    "\n",
    "print(\"Finished!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
