{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEVEL_SRC must contain the directory use-cases and pykhaos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20191226-084704 [INFO ] Logging to file /var/SP/data/home/csanc109/logging/out_20191226_084704.log\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, sys\n",
    "import datetime as dt\n",
    "DEVEL_SRC = os.path.join(os.environ.get('BDA_USER_HOME', ''), \"src\", \"devel\")\n",
    "if DEVEL_SRC not in sys.path:\n",
    "    sys.path.append(DEVEL_SRC)\n",
    "\n",
    "USECASES_SRC = os.path.join(DEVEL_SRC, \"use-cases\") # TODO when '-' is removed from name, remove also this line and adapt imports \n",
    "if USECASES_SRC not in sys.path: \n",
    "    sys.path.append(USECASES_SRC)\n",
    "    \n",
    "# AMDOCS_SRC = os.path.join(DEVEL_SRC, \"amdocs_informational_dataset\") # TODO when - is removed, remove also this line and adapt imports\n",
    "# if AMDOCS_SRC not in sys.path: \n",
    "#     sys.path.append(AMDOCS_SRC)\n",
    "    \n",
    "import pykhaos.utils.custom_logger as clogger\n",
    "logging_file = os.path.join(os.environ.get('BDA_USER_HOME', ''), \"logging\",\n",
    "                                    \"out_\" + dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".log\")\n",
    "logger = clogger.configure_logger(log_filename=logging_file, std_channel=sys.stderr, logger_name=\"\")\n",
    "logger.info(\"Logging to file {}\".format(logging_file))    \n",
    "        \n",
    "from project.project_generic import Project\n",
    "\n",
    "import pykhaos.utils.notebooks as nb\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "RUNNING_FROM_NOTEBOOK = nb.isnotebook()\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "if RUNNING_FROM_NOTEBOOK:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    %matplotlib inline  \n",
    "    EXTERNAL_LIB = os.path.join(os.environ.get('BDA_USER_HOME', ''), \"lib\", \"external_libs\")\n",
    "    if EXTERNAL_LIB not in sys.path:\n",
    "        sys.path.append(EXTERNAL_LIB)\n",
    "    # feel free from commenting this line and the other ones that begin with \"%%notify\" if you do not have \n",
    "    # the extension installed or copy de lib from /var/SP/data/home/csanc109/lib/external_libs/jupyternotify/\n",
    "    %load_ext jupyternotify \n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import Row, DataFrame, Column, Window\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType, DateType, ArrayType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler, SQLTransformer, OneHotEncoder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.sql.functions import collect_set, concat, size, coalesce, col, lpad, struct, count as sql_count, lit, min as sql_min, max as sql_max, collect_list, udf, when, desc, asc, to_date, create_map, sum as sql_sum\n",
    "from pyspark.sql.types import StringType, ArrayType, MapType, StructType, StructField, IntegerType\n",
    "from pyspark.sql.functions import array, regexp_extract\n",
    "from itertools import chain\n",
    "from churn.datapreparation.general.data_loader import get_unlabeled_car, get_port_requests_table, get_numclients_under_analysis\n",
    "from churn.utils.constants import PORT_TABLE_NAME\n",
    "from churn.utils.udf_manager import Funct_to_UDF\n",
    "from pyspark.sql.functions import substring, datediff, row_number\n",
    "from pykhaos.utils.date_functions import move_date_n_days, move_date_n_cycles\n",
    "from pykhaos.utils.hdfs_functions import check_hdfs_exists\n",
    "from pykhaos.modeling.model_performance import get_lift\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ended spark session: 21.436152935 secs | default parallelism=4\n"
     ]
    }
   ],
   "source": [
    "from churn.utils.general_functions import init_spark\n",
    "spark = init_spark(\"navcomp\")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasa de evaporacion trigger nav comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_date_ = \"20190925\"\n",
    "filter_ = 'comps'\n",
    "save_ = True\n",
    "verbose = True\n",
    "\n",
    "\n",
    "from churn.analysis.triggers.navcomp.navcomp_utils import get_labeled_set_msisdn\n",
    "from churn.analysis.triggers.base_utils.base_utils import get_active_filter, get_disconnection_process_filter, get_churn_call_filter\n",
    "\n",
    "tr_set = get_labeled_set_msisdn(spark, tr_date_, sources='all', save_ = save_, verbose = verbose) \n",
    "\n",
    "# Modeling filters\n",
    "\n",
    "tr_active_filter = get_active_filter(spark, tr_date_, 90)\n",
    "\n",
    "tr_disconnection_filter = get_disconnection_process_filter(spark, tr_date_, 90)\n",
    "\n",
    "tr_churn_call_filter = get_churn_call_filter(spark, tr_date_, 90, 'msisdn')\n",
    "\n",
    "tr_set = tr_set \\\n",
    "    .join(tr_active_filter, ['msisdn'], 'inner') \\\n",
    "    .join(tr_disconnection_filter, ['nif_cliente'], 'inner') \\\n",
    "    .join(tr_churn_call_filter, ['msisdn'], 'inner')\n",
    "\n",
    "tr_set = filter_population(spark, tr_set, filter_)\n",
    "\n",
    "tr_set.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[col_ for col_ in tr_set.columns if \"port\" in col_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = tr_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = [\"PEPEPHONE\", \"ORANGE\", \"JAZZTEL\", \"MOVISTAR\", \"MASMOVIL\", \"YOIGO\", \"VODAFONE\", \"LOWI\", \"O2\", \"unknown\"]\n",
    "\n",
    "def get_most_consulted_operator(consulted_list, days_since_1st_navigation_list):\n",
    "    if not consulted_list: return \"None\"\n",
    "    combined_list = [(a,b,c) for a,b,c in zip(consulted_list, days_since_1st_navigation_list, competitors)]\n",
    "    # sort by consulted list and then by days since 1st navigation\n",
    "    sorted_list = sorted(combined_list, key = lambda x : (x[0], x[1]), reverse=True)\n",
    "    return sorted_list[0][2] if sorted_list else \"None\"\n",
    "\n",
    "\n",
    "get_most_consulted_operator_udf = udf(lambda x,y: get_most_consulted_operator(x,y), StringType())\n",
    "\n",
    "\n",
    "df_test = (df_test\n",
    "             .withColumn('consulted_list', array(*[col_ + \"_sum_count\" for col_ in competitors]))\n",
    "             .withColumn('days_since_1st_navigation_list', array(*[col_ + \"_max_days_since_navigation\" for col_ in competitors]))\n",
    "             .withColumn('most_consulted_operator', get_most_consulted_operator_udf(col('consulted_list'), col(\"days_since_1st_navigation_list\"))))\n",
    "\n",
    "#df_test.select(*([col_ + \"_sum_count\" for col_ in competitors] + [col_ + \"_max_days_since_navigation\" for col_ in competitors] +[\"most_consulted_operator\"])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from churn_nrt.src.data.sopos_dxs import MobPort\n",
    "\n",
    "df_target = MobPort(spark).get_module(tr_date_)\n",
    "\n",
    "\n",
    "tr_set_target = tr_set.join(df_target, on=[\"msisdn\"], how=\"left\")\n",
    "tr_set_churn = tr_set_target.where(col(\"label_mob\")==1)\n",
    "\n",
    "#tr_set_churn.select('portout_date_mob', \"max_days_since_navigation_comps\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tr_set_churn = tr_set_churn.cache()\n",
    "tr_set_churn.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# days_until_churn: days since closing_day to churn_event (first churn event)\n",
    "from pyspark.sql.functions import from_unixtime,unix_timestamp\n",
    "# days_to_churn = fecha_de_sol_porta – fecha_de_la_última_navegación\n",
    "\n",
    "# days_until_churn: days since closing_day to churn_event (first churn event)\n",
    "tr_set_churn = (tr_set_churn.withColumn(\"days_until_sopo\", when(col(\"portout_date_mob\").isNotNull(), \n",
    "                                                                 datediff(col(\"portout_date_mob\"),\n",
    "                                                                          from_unixtime(unix_timestamp(lit(tr_date_), \"yyyyMMdd\")))).otherwise(-1)))\n",
    "\n",
    "tr_set_churn = tr_set_churn.withColumn(\"days_until_churn\", col(\"days_until_sopo\") + col(\"min_days_since_navigation_comps\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_ = \"days_until_churn\"\n",
    "n = 20\n",
    "\n",
    "bins, counts = tr_set_churn.select(col_).rdd.flatMap(lambda x: x).histogram(n)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(bins[:-1], bins=bins, weights=counts)\n",
    "plt.xlabel(\"** {} **\".format(col_))\n",
    "\n",
    "total = tr_set_churn.count()\n",
    "\n",
    "import numpy as np\n",
    "counts_cum = np.cumsum(counts)\n",
    "counts_cum = [100.0 * cc/total for cc in counts_cum]\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(bins[:-1], bins=bins, weights=counts_cum)\n",
    "plt.grid(True)\n",
    "plt.yticks(list(range(10,110,10))) \n",
    "plt.xlabel(\"cumsum {}\".format(col_))\n",
    "plt.ylabel(\"% churners [total churners = {}]\".format(total))\n",
    "\n",
    "pd.DataFrame({\"bins\" : bins[1:], \"counts\" : counts_cum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df.unpersist()\n",
    "\n",
    "from churn.analysis.triggers.base_utils.base_utils import get_customer_base\n",
    "date_ = \"20190930\"\n",
    "add_columns_customer = [\"birth_date\", \"fecha_naci\", 'CLASE_CLI_COD_CLASE_CLIENTE', 'X_CLIENTE_PRUEBA', \"TIPO_DOCUMENTO\"]\n",
    "add_columns=[\"TARIFF\", 'COD_ESTADO_GENERAL', \"srv_basic\"]\n",
    "closing_day = date_\n",
    "date_ = date_\n",
    "base_df = (get_customer_base(spark, date_, add_columns=add_columns, \n",
    "                             add_columns_customer=add_columns_customer,\n",
    "                            active_filter=False).filter(col('rgu')=='mobile'))#.select(*(['msisdn', 'nif_cliente', 'num_cliente'] + \n",
    "                                                                                                                                           #add_columns + add_columns_customer)))\n",
    "\n",
    "print(base_df.columns)\n",
    "\n",
    "base_df = (base_df.withColumn(\"birth_date\", when( (col(\"birth_date\").isNull() | (col(\"birth_date\") == 1753)), col(\"fecha_naci\")).otherwise(col(\"birth_date\")).cast(IntegerType()))\n",
    "                  .withColumn(\"age\", when(col(\"birth_date\") != 1753, lit(int(closing_day[:4])) - col(\"birth_date\")).otherwise(-1))\n",
    "          ).drop(\"birth_date\")\n",
    "\n",
    "base_df = base_df.cache()\n",
    "base_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df.groupby(\"srv_basic\").agg(*[sql_count(\"*\").alias(\"count\")]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df.groupby(\"CLASE_CLI_COD_CLASE_CLIENTE\").agg(*[sql_count(\"*\").alias(\"count\")]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df.select(\"X_CLIENTE_PRUEBA\").dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df.groupby(\"X_CLIENTE_PRUEBA\").agg(*[sql_count(\"*\").alias(\"count\")]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df.groupby(\"COD_ESTADO_GENERAL\").agg(*[sql_count(\"*\").alias(\"count\")]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df.groupby(\"TARIFF\").agg(*[sql_count(\"*\").alias(\"count\")]).show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df.select(\"NIF_CLIENTE\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df.select(\"NUM_CLIENTE\").distinct().count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df.select(\"msisdn\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.load(\"/user/csanc109/data/mobile_base_20190930_parquet\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df_insights.coalesce(1).write.mode('overwrite').save(\"/user/csanc109/data/mobile_base_20190930_parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df_insights = base_df.select(\"msisdn\", \"num_cliente\", \"nif_cliente\", \"srv_basic\", \"rgu\", 'CLASE_CLI_COD_CLASE_CLIENTE', 'COD_ESTADO_GENERAL', \"TARIFF\")\n",
    "base_df_insights.coalesce(1).write.mode('overwrite').format('csv').option('sep', '|').option('header', 'true').save(\"/user/csanc109/data/mobile_base_20190930\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df  = base_df.withColumn(\"age_disc\",  when(col(\"age\").isNull(), \"other\")\n",
    "                                         .when(col(\"age\")<20, \"<20\")\n",
    "                                         .when( (col(\"age\")>=20) & (col(\"age\")<25) , \"[20-25)\")\n",
    "                                         .when( (col(\"age\")>=25) & (col(\"age\")<30) , \"[25-30)\")\n",
    "                                         .when( (col(\"age\")>=30) & (col(\"age\")<35) , \"[30-35)\")\n",
    "                                         .when( (col(\"age\")>=35) & (col(\"age\")<40) , \"[35-40)\")\n",
    "                                         .when( (col(\"age\")>=40) & (col(\"age\")<45) , \"[40-45)\")\n",
    "                                         .when( (col(\"age\")>=45) & (col(\"age\")<50) , \"[45-50)\")\n",
    "                                         .when( (col(\"age\")>=50) & (col(\"age\")<55) , \"[50-55)\")\n",
    "                                         .when( (col(\"age\")>=55) & (col(\"age\")<60) , \"[55-60)\")\n",
    "                                         .when( (col(\"age\")>=60) & (col(\"age\")<65) , \"[60-65)\")\n",
    "                                         .when( col(\"age\")>=65, \">=65\")\n",
    "                                         .otherwise(\"other\"))\n",
    "\n",
    "base_df.groupby(\"age_disc\").agg(sql_count(\"*\").alias(\"count\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tgs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closing_day = \"20191031\"\n",
    "df_tgs = spark.read.load(\"/data/udf/vf_es/churn/extra_feats_mod/tgs/year={}/month={}/day={}\".format(int(closing_day[:4]), int(closing_day[4:6]), int(closing_day[6:])))\n",
    "df_tgs.select('tgs_days_since_f_inicio_bi',\n",
    " 'tgs_days_since_f_inicio_bi_exp',\n",
    " 'tgs_days_until_f_fin_bi',\n",
    " 'tgs_days_until_f_fin_bi_exp',\n",
    " 'tgs_days_until_fecha_fin_dto').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tgs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_tgs = df_tgs.withColumn(\"blindaje_categ\", when(  col(\"tgs_days_until_f_fin_bi\")>60, \"blindado\")\n",
    "                                      .when( (col(\"tgs_days_until_f_fin_bi\")>0) & (col(\"tgs_days_until_f_fin_bi\")<=60), \"blindado proximo\")\n",
    "                                      .when( ((col(\"tgs_blindaje_bi_expirado\")==1) & (col(\"tgs_days_since_f_inicio_bi_exp\")>60)), \"desblindado\")\n",
    "                                      .when( ((col(\"tgs_blindaje_bi_expirado\")==1) & (col(\"tgs_days_since_f_inicio_bi_exp\")>=0) & (col(\"tgs_days_since_f_inicio_bi_exp\")<=60)), \"desblindado reciente\")\n",
    "                    )\n",
    "\n",
    "df_tgs.groupby(\"blindaje_categ\").agg(sql_count(\"*\").alias(\"count\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from churn.analysis.triggers.navcomp.navcomp_utils import get_latest_scores\n",
    "\n",
    "df_scores = get_latest_scores(closing_day)\n",
    "\n",
    "mean_score = df_scores.select(sql_avg('scoring').alias('mean_score')).rdd.first()['mean_score']\n",
    "\n",
    "\n",
    "mean_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING NEW ATTRIBUTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_date_ = \"20191030\"\n",
    "filter_ = 'comps'\n",
    "save_ = True\n",
    "verbose = True\n",
    "\n",
    "\n",
    "from churn.analysis.triggers.navcomp.navcomp_utils import get_labeled_set_msisdn\n",
    "from churn.analysis.triggers.base_utils.base_utils import get_active_filter, get_disconnection_process_filter, get_churn_call_filter\n",
    "\n",
    "tr_set = get_labeled_set_msisdn(spark, tr_date_, sources='all', save_ = save_, verbose = verbose) \n",
    "tr_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _get_customer_master_feats(customer_tr_df):\n",
    "\n",
    "    class UDFclass:\n",
    "\n",
    "        @staticmethod\n",
    "        def compute_rgus_attrib(current_list, rgu_name):\n",
    "            import collections\n",
    "            rgus_dict = collections.Counter(current_list) if current_list is not None else {}\n",
    "            if rgu_name not in rgus_dict:\n",
    "                return 0\n",
    "            else:\n",
    "                return rgus_dict[rgu_name]\n",
    "\n",
    "    compute_rgus_attrib_udf = udf(lambda x, y: UDFclass.compute_rgus_attrib(x, y), IntegerType())\n",
    "\n",
    "    customer_tr_df = customer_tr_df.withColumn(\"nb_rgus_mobile\", compute_rgus_attrib_udf(\"rgus_list\", lit(\"mobile\")))\n",
    "    customer_tr_df = customer_tr_df.withColumn(\"nb_rgus_tv\", compute_rgus_attrib_udf(\"rgus_list\", lit(\"tv\")))\n",
    "    customer_tr_df = customer_tr_df.withColumn(\"nb_rgus_fixed\", compute_rgus_attrib_udf(\"rgus_list\", lit(\"fixed\")))\n",
    "    customer_tr_df = customer_tr_df.withColumn(\"nb_rgus_fbb\", compute_rgus_attrib_udf(\"rgus_list\", lit(\"fbb\")))\n",
    "    customer_tr_df = customer_tr_df.withColumn(\"nb_rgus_bam\", compute_rgus_attrib_udf(\"rgus_list\", lit(\"bam\")))\n",
    "\n",
    "    return customer_tr_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_ = \"20191022\"\n",
    "\n",
    "from churn.analysis.triggers.navcomp.metadata import get_metadata\n",
    "\n",
    "from churn.analysis.triggers.base_utils.base_utils import get_customer_base\n",
    "add_columns_customer = [\"birth_date\", \"fecha_naci\", 'CLASE_CLI_COD_CLASE_CLIENTE', 'X_CLIENTE_PRUEBA', \"TIPO_DOCUMENTO\"]\n",
    "#add_columns = [\"TARIFF\", 'COD_ESTADO_GENERAL', \"srv_basic\"]\n",
    "\n",
    "base_df = get_customer_base(spark, date_, add_columns_customer=add_columns_customer).filter(col('rgu')=='mobile').select('msisdn', 'nif_cliente', 'num_cliente', 'birth_date', 'fecha_naci')\n",
    "base_df = base_df.drop_duplicates(['msisdn', 'nif_cliente', 'num_cliente'])\n",
    "base_df = (base_df.withColumn(\"birth_date\", when((col(\"birth_date\").isNull() | (col(\"birth_date\") == 1753)), col(\"fecha_naci\")).otherwise(col(\"birth_date\")).cast(IntegerType()))\n",
    "                  .withColumn(\"age\", when(col(\"birth_date\") != 1753, lit(int(date_[:4])) - col(\"birth_date\")).otherwise(-1)))\n",
    "base_df = base_df.withColumn(\"age_disc\", when(col(\"age\").isNull(), \"other\").when(col(\"age\") < 20, \"<20\")\n",
    "                                        .when((col(\"age\") >= 20) & (col(\"age\") < 25), \"[20-25)\")\n",
    "                                        .when((col(\"age\") >= 25) & (col(\"age\") < 30), \"[25-30)\")\n",
    "                                        .when((col(\"age\") >= 30) & (col(\"age\") < 35), \"[30-35)\")\n",
    "                                        .when((col(\"age\") >= 35) & (col(\"age\") < 40), \"[35-40)\")\n",
    "                                        .when((col(\"age\") >= 40) & (col(\"age\") < 45), \"[40-45)\")\n",
    "                                        .when((col(\"age\") >= 45) & (col(\"age\") < 50), \"[45-50)\")\n",
    "                                        .when((col(\"age\") >= 50) & (col(\"age\") < 55), \"[50-55)\")\n",
    "                                        .when((col(\"age\") >= 55) & (col(\"age\") < 60), \"[55-60)\")\n",
    "                                        .when((col(\"age\") >= 60) & (col(\"age\") < 65), \"[60-65)\")\n",
    "                                        .when(col(\"age\") >= 65, \">=65\").otherwise(\"other\")).drop(\"birth_date\")\n",
    "# base_df.columns ['msisdn', 'nif_cliente', 'num_cliente', 'fecha_naci', 'age', 'age_disc']\n",
    "\n",
    "from churn.analysis.triggers.orders.customer_master import get_customer_master\n",
    "\n",
    "customer_metadata = get_metadata(spark, sources=['customer'])\n",
    "\n",
    "customer_feats = customer_metadata.select('feature').rdd.map(lambda x: x['feature']).collect() + ['nif_cliente']\n",
    "\n",
    "\n",
    "\n",
    "customer_tr_df = (get_customer_master(spark, date_, unlabeled=True)\n",
    "                        .filter((col('segment_nif') != 'Pure_prepaid') & (col(\"segment_nif\").isNotNull())).drop_duplicates([\"nif_cliente\"])\n",
    "                )\n",
    "\n",
    "customer_tr_df = (_get_customer_master_feats(customer_tr_df).select('nif_cliente', \n",
    " 'segment_nif',\n",
    " 'nb_rgus',\n",
    " 'rgus_list',\n",
    " 'tgs_days_until_fecha_fin_dto',\n",
    " 'nb_rgus_mobile',\n",
    " 'nb_rgus_tv',\n",
    " 'nb_rgus_fixed',\n",
    " 'nb_rgus_fbb',\n",
    " 'nb_rgus_bam'))\n",
    "\n",
    "customer_map_tmp = customer_metadata.select(\"feature\", \"imp_value\", \"type\").rdd.map(lambda x: (x[\"feature\"], x[\"imp_value\"], x[\"type\"])).collect()\n",
    "\n",
    "customer_map = dict([(x[0], str(x[1])) if x[2] == \"categorical\" else (x[0], float(x[1])) for x in customer_map_tmp])\n",
    "\n",
    "base_df = base_df.join(customer_tr_df, ['nif_cliente'], 'inner').na.fill(customer_map)\n",
    "\n",
    "base_df = base_df.select(customer_feats).na.fill(customer_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.load(\"/data/udf/vf_es/churn/triggers/navcomp_msisdn_data/year=2019/month=11/day=7\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mod_navcomp = spark.read.load(\"/data/attributes/vf_es/model_outputs/model_scores/model_name=triggers_navcomp/year=2019\")\n",
    "df_mod_navcomp.select(\"year\", \"month\", \"day\", \"predict_closing_date\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_tr_df.select(\"msisdn\", 'nb_rgus', 'rgus_list', 'tgs_days_until_fecha_fin_dto', 'nb_rgus_mobile', 'nb_rgus_tv', 'nb_rgus_fixed', 'nb_rgus_fbb', 'nb_rgus_bam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_date = \"20190601\"\n",
    "from pykhaos.utils.date_functions import move_date_n_days\n",
    "end_port = move_date_n_days(tt_date, 15)\n",
    "from churn.analysis.triggers.base_utils.base_utils import get_mobile_portout_requests\n",
    "target = get_mobile_portout_requests(spark, tt_date, end_port).withColumnRenamed('label_mob', 'label').select('msisdn', 'label', \"portout_date_mob\")\n",
    "target.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VANISHING RATE DEL TOP-k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.load(\"/data/udf/vf_es/churn/triggers/nav_comp_tests/year=2019/month=11/day=7\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[col_ for col_ in spark.read.load(\"/data/udf/vf_es/churn/triggers/navcomp_msisdn_data/year=2019/month=10/day=24\").columns if \"age\" in col_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "navcomp_date = \"20190921\"\n",
    "target_date = navcomp_date\n",
    "\n",
    "tt_date = -1 # \"20190601\"\n",
    "\n",
    "navcomp_tr_df = spark.read.load(\"/data/udf/vf_es/churn/triggers/nav_comp_tests/year={}/month={}/day={}\".format(int(navcomp_date[:4]), int(navcomp_date[4:6]), int(navcomp_date[6:]))).drop(\"label\")\n",
    "\n",
    "# Labeling\n",
    "\n",
    "from churn.analysis.triggers.base_utils.base_utils import get_mobile_portout_requests\n",
    "\n",
    "from pykhaos.utils.date_functions import move_date_n_days\n",
    "\n",
    "end_port = move_date_n_days(target_date, 15)\n",
    "\n",
    "target = get_mobile_portout_requests(spark, navcomp_date, end_port).withColumnRenamed('label_mob', 'label').select('msisdn', 'label', \"portout_date_mob\")\n",
    "\n",
    "# Modeling filters\n",
    "\n",
    "tt_active_filter = get_active_filter(spark, navcomp_date, 90)\n",
    "\n",
    "tt_disconnection_filter = get_disconnection_process_filter(spark, navcomp_date, 90)\n",
    "\n",
    "tt_churn_call_filter = get_churn_call_filter(spark, navcomp_date, 90, 'msisdn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_set.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from churn.analysis.triggers.navcomp.navcomp_model import filter_population\n",
    "\n",
    "tt_set = navcomp_tr_df \\\n",
    "    .join(tt_active_filter, ['msisdn'], 'inner') \\\n",
    "    .join(tt_disconnection_filter, ['nif_cliente'], 'inner') \\\n",
    "    .join(tt_churn_call_filter, ['msisdn'], 'inner')\n",
    "\n",
    "tt_set = filter_population(spark, tt_set, filter_)\n",
    "\n",
    "tr_set_target = tt_set.join(target, ['msisdn'], 'left').na.fill({'label': 0.0})\n",
    "\n",
    "print \"[Info navcomp_model_production] After all the filters - Sie of the test set: \" + str(tt_set.count()) + \" - Number of distinct MSISDNs in the test set: \" + str(tt_set.select('msisdn').distinct().count())\n",
    "\n",
    "tr_set_churn = tr_set_target.where(col(\"label\")==1)\n",
    "\n",
    "from pyspark.sql.functions import from_unixtime,unix_timestamp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from churn.analysis.triggers.base_utils.base_utils import get_mobile_portout_requests\n",
    "\n",
    "\n",
    "df_predictions = spark.read.load(\"/data/udf/vf_es/churn/triggers/nav_comp_tests_nav_comp_tests_alllabels/year=2019/month=9/day=21/\")\n",
    "\n",
    "target_date = \"20190921\"\n",
    "navcomp_date = target_date\n",
    "\n",
    "from pykhaos.utils.date_functions import move_date_n_days\n",
    "\n",
    "end_port = move_date_n_days(target_date, 15)\n",
    "\n",
    "target = get_mobile_portout_requests(spark, target_date, end_port).withColumnRenamed('label_mob', 'label').select('msisdn', 'label', \"portout_date_mob\")\n",
    "\n",
    "tr_set_target = df_predictions.drop(\"label\").join(target, ['msisdn'], 'left').na.fill({'label': 0.0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "num_nile = int(math.floor(float(tr_set_churn.count()) / 100.0))\n",
    "\n",
    "lift = get_lift(tr_set_churn, 'scoring', 'label', num_nile, 1.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[col_ for col_ in tr_set_target.columns if \"scor\" in col_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import from_unixtime,unix_timestamp\n",
    "\n",
    "col_ = \"days_until_churn\"\n",
    "n = 20\n",
    "\n",
    "myschema = tr_set_target.schema\n",
    "\n",
    "total = tr_set_target.count()\n",
    "print(\"total\", total)\n",
    "\n",
    "for top_ in [2000,3000,5000, 10000, 20000]:\n",
    "    \n",
    "    print(\"************** Analyzing top_{}\".format(top_))\n",
    "    \n",
    "    tr_set_target = tr_set_target.sort(desc(\"model_score\"))\n",
    "    \n",
    "    df_top = spark.createDataFrame(tr_set_target.head(top_), schema=myschema)\n",
    "    \n",
    "    tr_set_churn = df_top.where(col(\"label\")==1)\n",
    "    num_churners = tr_set_churn.count()\n",
    "    \n",
    "    tr_set_churn = (tr_set_churn.withColumn(\"days_until_sopo\", when(col(\"portout_date_mob\").isNotNull(), \n",
    "                                                                 datediff(col(\"portout_date_mob\"), from_unixtime(unix_timestamp(lit(navcomp_date), \"yyyyMMdd\")))).otherwise(-1)))\n",
    "\n",
    "    tr_set_churn = tr_set_churn.withColumn(\"days_until_churn\", col(\"days_until_sopo\") + col(\"min_days_since_navigation_comps\"))\n",
    "\n",
    "    print(\"1) num scores nulos\", tr_set_churn.where(col(\"model_score\").isNull()).count())\n",
    "\n",
    "    tr_set_churn = tr_set_churn.where(col(\"model_score\").isNotNull())\n",
    "                                                \n",
    "    tr_set_churn = tr_set_churn.cache()\n",
    "\n",
    "    print(\"2) churn_rate top{} = {}\".format(top_, 100.0*num_churners/top_))\n",
    "    \n",
    "    \n",
    "    bins, counts = tr_set_churn.select(col_).rdd.flatMap(lambda x: x).histogram(n)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(bins[:-1], bins=bins, weights=counts)\n",
    "    plt.xlabel(\"** {} ** (top = {}) (num_churners={}) (churn_rate={})\".format(col_, top_, num_churners, 100.0*num_churners/top_))\n",
    "\n",
    "    import numpy as np\n",
    "    counts_cum = np.cumsum(counts)\n",
    "    counts_cum = [100.0 * cc/num_churners for cc in counts_cum]\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.hist(bins[:-1], bins=bins, weights=counts_cum)\n",
    "    plt.grid(True)\n",
    "    plt.yticks(list(range(10,110,10))) \n",
    "    plt.xlabel(\"cumsum {} (top = {}) (num_churners={}) (churn_rate={})\".format(col_, top_, num_churners, 100.0*num_churners/top_))\n",
    "\n",
    "    print(top_, pd.DataFrame({\"bins\" : bins[1:], \"counts\" : counts_cum}))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from churn.datapreparation.general.tgs_data_loader import get_tgs\n",
    "from churn_nrt.src.utils.date_functions import get_previous_cycle\n",
    "\n",
    "closing_day_tgs = get_previous_cycle(closing_day)\n",
    "print(\"Getting tgs for date {}\".format(closing_day_tgs))\n",
    "\n",
    "df_tgs = get_tgs(spark, closing_day_tgs, closing_day_tgs[:6], impute_nulls=False).select(\"nif_cliente\", 'tgs_days_until_fecha_fin_dto', 'tgs_days_until_f_fin_bi', 'msisdn')\n",
    "#df_tgs.columns -- ['nif_cliente', 'tgs_days_until_fecha_fin_dto', 'tgs_days_until_f_fin_bi']\n",
    "\n",
    "\n",
    "df_tgs = df_tgs.withColumn(\"blindaje_disc\", when(col(\"tgs_days_until_f_fin_bi\").isNull(), \"none\")\n",
    "                             .when(col(\"tgs_days_until_f_fin_bi\") > 60, \"hard\")\n",
    "                             .when((col(\"tgs_days_until_f_fin_bi\") >   0) & (col(\"tgs_days_until_f_fin_bi\") <= 60), \"soft\")\n",
    "                             .when((col(\"tgs_days_until_f_fin_bi\") <=  0) & (col(\"tgs_days_until_f_fin_bi\") >= -60), \"soft-nobounded\")\n",
    "                             .otherwise(\"none\"))\n",
    "\n",
    "#df_tgs.groupby('tgs_days_until_f_fin_bi', \"blindaje_disc\").agg(sql_count(\"*\").alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closing_day = \"20191016\"\n",
    "date_ = closing_day\n",
    "#from churn.analysis.triggers.orders.customer_master import get_tgs\n",
    "from churn.datapreparation.general.tgs_data_loader import get_tgs\n",
    "from churn_nrt.src.utils.date_functions import get_previous_cycle\n",
    "\n",
    "closing_day_tgs = get_previous_cycle(closing_day)\n",
    "print(\"Getting tgs for date {}\".format(closing_day_tgs))\n",
    "\n",
    "df_tgs = get_tgs(spark, closing_day_tgs, closing_day_tgs[:6], impute_nulls=False).select(\"nif_cliente\", 'tgs_days_until_fecha_fin_dto', 'tgs_days_until_f_fin_bi', 'msisdn')\n",
    "#df_tgs.columns -- ['nif_cliente', 'tgs_days_until_fecha_fin_dto', 'tgs_days_until_f_fin_bi']\n",
    "\n",
    "\n",
    "df_tgs = df_tgs.withColumn(\"blindaje_disc\", when(col(\"tgs_days_until_f_fin_bi\").isNull(), \"none\")\n",
    "                             .when(col(\"tgs_days_until_f_fin_bi\") > 60, \"hard\")\n",
    "                             .when((col(\"tgs_days_until_f_fin_bi\") >   0) & (col(\"tgs_days_until_f_fin_bi\") <= 60), \"soft\")\n",
    "                             .when((col(\"tgs_days_until_f_fin_bi\") <=  0) & (col(\"tgs_days_until_f_fin_bi\") >= -60), \"soft-nobounded\")\n",
    "                             .otherwise(\"none\"))\n",
    "\n",
    "# Desblindado [fcarren: Cliente desblindado donde ese desblindaje se produjo hace más de 2 meses, o que nunca tuvo blindaje]\n",
    "# Desblindado reciente [fcarren: Cliente desblindado, pero su desblindaje se ha producido en los recientes 1 o 2 meses]\n",
    "# Blindado próximo [fcarren: Cliente blindado, pero su le quedan sólo  1 o 2 meses para que se desblinde]\n",
    "# Blindado [fcarren: Cliente blindado, con blindaje activo de más de 2 meses]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tgs.groupby(\"blindaje_disc\", \"tgs_days_until_f_fin_bi\").agg(sql_count(\"*\").alias(\"count\")).sort(asc(\"tgs_days_until_f_fin_bi\")).show(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tgs_raw.groupby(\"blindaje_disc\").agg(sql_count(\"*\").alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from churn.analysis.triggers.navcomp.navcomp_utils import get_navcomp_car_msisdn\n",
    "\n",
    "df_navcomp = get_navcomp_car_msisdn(spark, date_, sources=[\"navcomp\",\"customer\",\"ccc\",\"spinners\",\"scores\"], save_ = False, verbose = False)\n",
    "nb_rgus_col = [col_ for col_ in df_navcomp.columns if col_.startswith(\"nb_rgus_\")]\n",
    "\n",
    "filter_ = ['comps']\n",
    "\n",
    "from churn.analysis.triggers.base_utils.base_utils import get_active_filter, get_disconnection_process_filter, get_churn_call_filter\n",
    "\n",
    "\n",
    "tt_active_filter = get_active_filter(spark, date_, 90)\n",
    "\n",
    "tt_disconnection_filter = get_disconnection_process_filter(spark, date_, 90)\n",
    "\n",
    "tt_churn_call_filter = get_churn_call_filter(spark, date_, 90, 'msisdn')\n",
    "\n",
    "from churn.analysis.triggers.navcomp.navcomp_model import filter_population\n",
    "\n",
    "tt_set = df_navcomp \\\n",
    "    .join(tt_active_filter, ['msisdn'], 'inner') \\\n",
    "    .join(tt_disconnection_filter, ['nif_cliente'], 'inner') \\\n",
    "    .join(tt_churn_call_filter, ['msisdn'], 'inner')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from churn.analysis.triggers.navcomp.navcomp_utils import get_volumen_attributes\n",
    "#df_get_volumen = get_volumen_attributes(spark, date_, nDays=-14)#.select(volumen_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_ = 'comps'\n",
    "\n",
    "tt_set = filter_population(spark, tt_set, filter_)\n",
    "\n",
    "#tr_set_target = tt_set.join(target, ['msisdn'], 'left').na.fill({'label': 0.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from churn.analysis.triggers.base_utils.base_utils import get_mobile_portout_requests\n",
    "\n",
    "from pykhaos.utils.date_functions import move_date_n_days\n",
    "end_port = move_date_n_days(date_, 15)\n",
    "target = get_mobile_portout_requests(spark, date_, end_port).withColumnRenamed('label_mob', 'label').select('msisdn', 'label').withColumnRenamed(\"label\", \"label_1_15\")\n",
    "\n",
    "from pykhaos.utils.date_functions import move_date_n_days\n",
    "end_port_2 = move_date_n_days(end_port, 15)\n",
    "target_2 = get_mobile_portout_requests(spark, end_port, end_port_2).withColumnRenamed('label_mob', 'label').select('msisdn', 'label').withColumnRenamed(\"label\", \"label_16_30\")\n",
    "\n",
    "\n",
    "from pykhaos.utils.date_functions import move_date_n_days\n",
    "end_port_3 = move_date_n_days(date_, 30)\n",
    "target_3 = get_mobile_portout_requests(spark, date_, end_port_3).withColumnRenamed('label_mob', 'label').select('msisdn', 'label').withColumnRenamed(\"label\", \"label_1_30\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_,end_port,end_port_2, end_port_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from churn.analysis.triggers.orders.customer_master import get_customer_master\n",
    "customer_tr_df = (get_customer_master(spark, date_, unlabeled=True).filter((col('segment_nif') != 'Pure_prepaid') & (col(\"segment_nif\").isNotNull())).drop_duplicates([\"nif_cliente\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from churn.analysis.triggers.navcomp.navcomp_utils import get_latest_scores\n",
    "df_scores = get_latest_scores(spark, date_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_navcomp_select = tt_set.select(\"most_consulted_operator\", \"JAZZTEL_max_count\", \"VODAFONE_sum_count\", \"PEPEPHONE_min_days_since_navigation\", \"VODAFONE_min_days_since_navigation\", \n",
    "                                      \"norm_min_days_since_navigation_comps\", \"MOVISTAR_distinct_days_with_navigation\", \"unknown_distinct_days_with_navigation\", \"MOVISTAR_sum_count\", \n",
    "                                      \"norm_max_days_since_navigation_comps\",\"msisdn\", \"age\", \"age_disc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_navcomp_select = df_navcomp_select.cache()\n",
    "df_analysis.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_tgs.select(*['nif_cliente', 'tgs_days_until_fecha_fin_dto', 'tgs_days_until_f_fin_bi', 'blindaje_disc', 'msisdn']).join(df_navcomp_select, on=[\"msisdn\"], how=\"inner\")\n",
    "df_analysis = df_analysis.join(df_scores, on=[\"msisdn\"], how=\"left\")\n",
    "df_analysis = df_analysis.join(customer_tr_df.select(\"nif_cliente\", \"segment_nif\"), ['nif_cliente'], 'inner')\n",
    "\n",
    "df_analysis = df_analysis.join(target, ['msisdn'], 'left').na.fill({'label_1_15': 0.0})\n",
    "df_analysis = df_analysis.join(target_2, ['msisdn'], 'left').na.fill({'label_16_30': 0.0})\n",
    "df_analysis = df_analysis.join(target_3, ['msisdn'], 'left').na.fill({'label_1_30': 0.0})\n",
    "#df_analysis.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.groupby(\"blindaje_disc\").agg(sql_count(\"*\").alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_analysis.withColumn(\"age_disc_simple\", when(col(\"age\").isNull(), \"other\")\n",
    "                             .when((col(\"age\") >   0) & (col(\"age\") < 20), \"<20\")\n",
    "                             .when((col(\"age\") >= 20) & (col(\"age\") < 35), \"[20-35)\")\n",
    "                             .when((col(\"age\") >= 35) & (col(\"age\") < 55), \"[35-55)\")\n",
    "                             .when((col(\"age\") >= 55) & (col(\"age\") < 70), \"[55-70)\")\n",
    "                             .when(col(\"age\") >= 70, \">=70\").otherwise(\"other\"))\n",
    "\n",
    "operadores = ['PEPEPHONE', 'JAZZTEL', 'MOVISTAR', 'MASMOVIL', 'YOIGO', 'VODAFONE', 'LOWI', 'O2', 'ORANGE']\n",
    "\n",
    "\n",
    "for op in operadores:\n",
    "    df_analysis = df_analysis.withColumn(\"flag_{}\".format(op), when(col(\"most_consulted_operator\")==op, 1).otherwise(0))\n",
    "\n",
    "    \n",
    "df_analysis = df_analysis.withColumn(\"most_consulted_operator_simple\", when(col(\"most_consulted_operator\").isin([\"MOVISTAR\", \"VODAFONE\", \"YOIGO\"]), col(\"most_consulted_operator\")).otherwise(\"OTHERS\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg as sql_avg\n",
    "myschema = df_analysis.schema\n",
    "df_analysis = df_analysis.sort(desc(\"scoring\"))\n",
    "df_top20k = spark.createDataFrame(df_analysis.head(20000), schema=myschema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top20k.groupby(\"segment_nif\").agg(sql_count(\"*\").alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top20k.groupby(['blindaje_disc']).agg(*([sql_count(\"*\").alias(\"count\"), sql_sum(\"label_1_15\").alias(\"num_churners_1_15\"),                  \n",
    "                                                                                                             sql_avg(\"scoring\").alias(\"avg_scoring\")])).show(100,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top20k.groupby(\"blindaje_disc\").agg(sql_count(\"*\").alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top20k.groupby(['age_disc', 'blindaje_disc', \"most_consulted_operator\", \"segment_nif\"]).agg(*([sql_count(\"*\").alias(\"count\"), sql_sum(\"label_1_15\").alias(\"num_churners_1_15\"),                  \n",
    "                                                                                                             sql_avg(\"scoring\").alias(\"avg_scoring\")])).show(2500,truncate=False) \n",
    "                                                                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_agg_top20k = df_top20k.groupby(['blindaje_disc', \"most_consulted_operator\", \"segment_nif\"]).agg(*([sql_count(\"*\").alias(\"count\"), \n",
    "                                                                                                             sql_sum(\"label_1_15\").alias(\"num_churners_1_15\"), \n",
    "                                                                                                             sql_sum(\"label_16_30\").alias(\"num_churners_16_30\"), \n",
    "                                                                                                             sql_sum(\"label_1_30\").alias(\"num_churners_1_30\"), \n",
    "                                                                                                             sql_avg(\"scoring\").alias(\"avg_scoring\")] + \n",
    "                                                                                                            [sql_sum(\"flag_{}\".format(op)).alias(\"num_op_{}\".format(op)) for op in operadores]))\n",
    "df_agg_top20k.sort(desc(\"count\")).show(5000, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top20k.groupby(['age_disc_simple', \"blindaje_disc\"]).agg(*[sql_max(\"JAZZTEL_max_count\").alias(\"MAX_JAZZTEL_max_count\"),\n",
    "                                                              sql_min(\"JAZZTEL_max_count\").alias(\"MIN_JAZZTEL_max_count\"), \n",
    "                                                              sql_avg(\"JAZZTEL_max_count\").alias(\"AVG_JAZZTEL_max_count\"), \n",
    "                                                              \n",
    "                                                              sql_sum(\"VODAFONE_sum_count\").alias(\"SUM_VODAFONE_sum_count\"),\n",
    "                                                              sql_avg(\"VODAFONE_sum_count\").alias(\"AVG_VODAFONE_sum_count\"),\n",
    "                                                              sql_min(\"VODAFONE_sum_count\").alias(\"MIN_VODAFONE_sum_count\"),\n",
    "                                                              sql_max(\"VODAFONE_sum_count\").alias(\"MAX_VODAFONE_sum_count\"),\n",
    "                                                              \n",
    "                                                              sql_min(\"PEPEPHONE_min_days_since_navigation\").alias(\"MIN_PEPEPHONE_min_days_since_navigation\"),\n",
    "                                                              sql_max(\"PEPEPHONE_min_days_since_navigation\").alias(\"MAX_PEPEPHONE_min_days_since_navigation\"),\n",
    "                                                              sql_avg(\"PEPEPHONE_min_days_since_navigation\").alias(\"AVG_PEPEPHONE_min_days_since_navigation\"),\n",
    "\n",
    "                                                              sql_min(\"VODAFONE_min_days_since_navigation\").alias(\"MIN_VODAFONE_min_days_since_navigation\"),\n",
    "                                                              sql_max(\"VODAFONE_min_days_since_navigation\").alias(\"MAX_VODAFONE_min_days_since_navigation\"),\n",
    "                                                              sql_avg(\"VODAFONE_min_days_since_navigation\").alias(\"AVG_VODAFONE_min_days_since_navigation\"),\n",
    "\n",
    "                                                              sql_min(\"norm_min_days_since_navigation_comps\").alias(\"MIN_norm_min_days_since_navigation_comps\"),\n",
    "                                                              sql_max(\"norm_min_days_since_navigation_comps\").alias(\"MAX_norm_min_days_since_navigation_comps\"),\n",
    "                                                              sql_count(\"norm_min_days_since_navigation_comps\").alias(\"COUNT_norm_min_days_since_navigation_comps\"),\n",
    "\n",
    "                                                              sql_avg(\"MOVISTAR_distinct_days_with_navigation\").alias(\"AVG_MOVISTAR_distinct_days_with_navigation\"),\n",
    "                                                              sql_min(\"MOVISTAR_distinct_days_with_navigation\").alias(\"MIN_MOVISTAR_distinct_days_with_navigation\"),\n",
    "                                                              sql_max(\"MOVISTAR_distinct_days_with_navigation\").alias(\"MAX_MOVISTAR_distinct_days_with_navigation\"),\n",
    "\n",
    "                                                              sql_avg(\"unknown_distinct_days_with_navigation\").alias(\"AVG_unknown_distinct_days_with_navigation\"),\n",
    "                                                              sql_min(\"unknown_distinct_days_with_navigation\").alias(\"MIN_unknown_distinct_days_with_navigation\"),\n",
    "                                                              sql_max(\"unknown_distinct_days_with_navigation\").alias(\"MAX_unknown_distinct_days_with_navigation\"),\n",
    "                                                              \n",
    "                                                              sql_avg(\"MOVISTAR_sum_count\").alias(\"AVG_MOVISTAR_sum_count\"),\n",
    "                                                              sql_min(\"MOVISTAR_sum_count\").alias(\"MIN_MOVISTAR_sum_count\"),\n",
    "                                                              sql_max(\"MOVISTAR_sum_count\").alias(\"MAX_MOVISTAR_sum_count\"),\n",
    "\n",
    "                                                              sql_max(\"norm_max_days_since_navigation_comps\").alias(\"MAX_norm_max_days_since_navigation_comps\"),\n",
    "                                                              sql_min(\"norm_max_days_since_navigation_comps\").alias(\"MIN_norm_max_days_since_navigation_comps\"),\n",
    "                                                              sql_avg(\"norm_max_days_since_navigation_comps\").alias(\"AVG_norm_max_days_since_navigation_comps\"),\n",
    "\n",
    "                                                              sql_count(\"*\").alias(\"count\"), \n",
    "                                                                                                                          \n",
    "                                                             ]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_2 = df_top20k.groupby(['blindaje_disc', \"most_consulted_operator\"]).agg(*[sql_count(\"*\").alias(\"count\"), sql_avg(\"scoring\").alias(\"avg_scoring\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_2.sort(desc(\"count\")).show(50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_3 = df_analysis.groupby([\"most_consulted_operator\", 'age_disc_simple']).agg(*[sql_count(\"*\").alias(\"count\"), sql_avg(\"scoring\").alias(\"avg_scoring\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_3.sort(desc(\"count\")).show(150, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from churn_nrt.src.projects_utils.analysis.exploratory_analysis import attributes_exploration\n",
    "\n",
    "df_predictions = spark.read.load(\"/data/udf/vf_es/churn/triggers/nav_comp_tests_nav_comp_tests_alllabels/year=2019/month=9/day=21/\")\n",
    "\n",
    "target_date = \"20190921\"\n",
    "\n",
    "dfSalidaGaus = attributes_exploration(spark, df_predictions, ['age', 'latest_score'] + nb_rgus_col, percentile_inc=5)\n",
    "\n",
    "dfSalidaGaus.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from churn_nrt.src.projects_utils.analysis.exploratory_analysis import attributes_exploration\n",
    "\n",
    "df_predictions_2 = spark.read.load(\"/data/udf/vf_es/churn/triggers/nav_comp_tests_all_labels/year=2019/month=8/day=15/time=1574273309\")\n",
    "\n",
    "dfSalidaGaus_2 = attributes_exploration(spark, df_predictions_2, ['age', 'latest_score'] + nb_rgus_col, percentile_inc=5)\n",
    "\n",
    "dfSalidaGaus_2.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_scores = spark.read.load(\"/user/csanc109/data/mobile_base_20190930_parquet\")\n",
    "df_model_scores.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model_scores.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_model_scores = spark.read.load(\"/user/csanc109/data/mobile_base_20190930_parquet\")\n",
    "df_model_scores = df_model_scores.withColumn(\"scoring\", lit(0.0))\n",
    "df_model_scores = df_model_scores.withColumnRenamed(\"nif_cliente\", \"nif\")\n",
    "df_model_scores = df_model_scores.withColumnRenamed(\"num_cliente\", \"client_id\")\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import length, concat_ws, regexp_replace, split\n",
    "from churn.datapreparation.general.model_outputs_manager import ensure_types_model_scores_columns\n",
    "\n",
    "EXTRA_INFO_COLS = ['srv_basic',\n",
    " 'rgu',\n",
    " 'CLASE_CLI_COD_CLASE_CLIENTE',\n",
    " 'COD_ESTADO_GENERAL',\n",
    " 'TARIFF']\n",
    "\n",
    "MODEL_OUTPUTS_NULL_TAG = \"\"\n",
    "\n",
    "executed_at = dt.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S \")\n",
    "\n",
    "return_feed_execution = \"20190930\"\n",
    "\n",
    "day_partition = int(return_feed_execution[6:])\n",
    "month_partition = int(return_feed_execution[4:6])\n",
    "year_partition = int(return_feed_execution[:4])\n",
    "training_closing_date = \"20190930\"\n",
    "\n",
    "print(\"Going to insert with partition {} {} {} \".format(year_partition, month_partition, day_partition))\n",
    "\n",
    "'''\n",
    "MODEL PARAMETERS\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "df_pandas = pd.DataFrame({\n",
    "    \"model_name\": [\"mobile_base\"],\n",
    "    \"executed_at\": [executed_at],\n",
    "    \"model_level\": [\"msisdn\"],\n",
    "    \"training_closing_date\": [training_closing_date if training_closing_date else closing_day],\n",
    "    \"target\": [\"\"],\n",
    "    \"model_path\": [\"\"],\n",
    "    \"metrics_path\": [\"\"],\n",
    "    \"metrics_train\": [\"\"],\n",
    "    \"metrics_test\": [\"\"],\n",
    "    \"varimp\": [\"\"],\n",
    "    \"algorithm\": [\"\"],\n",
    "    \"author_login\": [\"csanc109\"],\n",
    "    \"extra_info\": [\"\"],\n",
    "    \"scores_extra_info_headers\": [\";\".join(EXTRA_INFO_COLS)],\n",
    "    \"year\":  [year_partition],\n",
    "    \"month\": [month_partition],\n",
    "    \"day\":   [day_partition],\n",
    "    \"time\": [int(executed_at.split(\" \")[1].replace(\":\", \"\"))]\n",
    "})\n",
    "\n",
    "df_parameters = spark.createDataFrame(df_pandas).withColumn(\"day\", col(\"day\").cast(\"integer\"))\\\n",
    "                                                .withColumn(\"month\", col(\"month\").cast(\"integer\"))\\\n",
    "                                                .withColumn(\"year\", col(\"year\").cast(\"integer\"))\\\n",
    "                                                .withColumn(\"time\", col(\"time\").cast(\"integer\"))\n",
    "\n",
    "'''\n",
    "MODEL SCORES\n",
    "'''\n",
    "\n",
    "# set to null the columns that go to extra_info field and have no value\n",
    "for col_ in EXTRA_INFO_COLS:\n",
    "    df_model_scores = df_model_scores.withColumn(col_,\n",
    "                                                 when(coalesce(length(col(col_)), lit(0)) == 0, MODEL_OUTPUTS_NULL_TAG).otherwise(\n",
    "                                                     col(col_)))\n",
    "\n",
    "df_model_scores = (\n",
    "        df_model_scores.withColumn(\"extra_info\", concat_ws(\";\", *[col(col_name) for col_name in EXTRA_INFO_COLS]))\n",
    "        .withColumn(\"prediction\", lit(\"0\"))\n",
    "        .drop(*EXTRA_INFO_COLS)\n",
    "        .withColumnRenamed(\"comb_score\", \"scoring\")\n",
    "        .withColumn(\"model_name\", lit(\"mobile_base\"))\n",
    "        .withColumn(\"executed_at\", lit(executed_at))\n",
    "        .withColumn(\"model_executed_at\", lit(executed_at))\n",
    "        .withColumn(\"year\", lit(year_partition).cast(\"integer\"))\n",
    "        .withColumn(\"month\",lit(month_partition).cast(\"integer\"))\n",
    "        .withColumn(\"day\", lit(day_partition).cast(\"integer\"))\n",
    "        .withColumn(\"time\", regexp_replace(split(col(\"executed_at\"), \" \")[1], \":\", \"\").cast(\"integer\"))\n",
    "        .withColumn(\"predict_closing_date\", lit(\"20190930\"))\n",
    "        .withColumn(\"model_output\", lit(\"\"))\n",
    "    )\n",
    "\n",
    "df_model_scores = ensure_types_model_scores_columns(df_model_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = \"/user/csanc109/data/mobile_base/model_scores\"\n",
    "\n",
    "# (df_model_scores.coalesce(1)\n",
    "#   .write\n",
    "#   .partitionBy('model_name', 'year', 'month', 'day')\n",
    "#   .mode(\"append\")\n",
    "#   .format(\"parquet\")\n",
    "#   .save(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_path1 = \"/data/udf/vf_es/churn/triggers/nav_comp_tests_all_labels/year=2019/month=10/day=21/time=1574782575\"\n",
    "day_path2 = \"/data/udf/vf_es/churn/triggers/nav_comp_tests_all_labels/year=2019/month=10/day=22/time=1574791124\"\n",
    "day_path3 = \"/data/udf/vf_es/churn/triggers/nav_comp_tests_all_labels/year=2019/month=10/day=23/time=1574863576\"\n",
    "day_path4 = \"/data/udf/vf_es/churn/triggers/nav_comp_tests_all_labels/year=2019/month=10/day=24/time=1574791742\"\n",
    "day_path5 = \"/data/udf/vf_es/churn/triggers/nav_comp_tests_all_labels/year=2019/month=10/day=25/time=1574886360\"\n",
    "day_path6 = \"/data/udf/vf_es/churn/triggers/nav_comp_tests_all_labels/year=2019/month=10/day=26/time=1574887287\"\n",
    "day_path7 = \"/data/udf/vf_es/churn/triggers/nav_comp_tests_all_labels/year=2019/month=10/day=27/time=1574888086\"\n",
    "\n",
    "df_labels = spark.read.load(day_path2)\n",
    "#label_kind = \"mobile\"\n",
    "label_kind = \"mobile+fix\" \n",
    "closing_day = \"20191022\"\n",
    "\n",
    "\n",
    "print(\"NUM LABELS ORIG\", df_labels.select(sql_sum('label').alias('num_churners')).rdd.first()['num_churners'])\n",
    "\n",
    "if label_kind in  [\"mobile+fix\"]:\n",
    "    df_lab = get_label(spark, closing_day, kind=label_kind, churn_window=15).select(\"msisdn\", \"label\")\n",
    "    df_labels = df_labels.drop(\"label\")\n",
    "    df_labels = df_labels.join(df_lab, on=[\"msisdn\"], how=\"left\").na.fill({'label': 0.0})\n",
    "elif label_kind != \"mobile\":\n",
    "    print(\"Unknown kind {}. Program will exit here!\".format(kind))\n",
    "    import sys\n",
    "    sys.exit()\n",
    "\n",
    "from pyspark.sql.functions import avg as sql_avg\n",
    "myschema = df_labels.schema\n",
    "df_labels = df_labels.sort(desc(\"model_score\"))\n",
    "df_top_9286 = spark.createDataFrame(df_labels.head(9286), schema=myschema)\n",
    "\n",
    "tt_churn_ref = df_top_9286.select(sql_avg('label').alias('churn_ref')).rdd.first()['churn_ref']\n",
    "tt_churn_ref * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_A = spark.read.load(day_path5)\n",
    "df_labels_B = spark.read.load(day_path7)\n",
    "df_labels_join = df_labels_A.select(\"msisdn\").join(df_labels_B.select(\"msisdn\", \"label\", \"model_score\"), ['msisdn'], 'right').where(df_labels_A['msisdn'].isNull())\n",
    "df_labels_join = df_labels_join.cache()\n",
    "df_labels_join.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_churn_ref = df_labels_join.select(sql_avg('label').alias('churn_ref')).rdd.first()['churn_ref']\n",
    "tt_churn_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myschema3 = df_labels_join.schema\n",
    "from churn_nrt.src.projects_utils.models.modeler import get_cumulative_lift_fix_step\n",
    "cum_churn_rate = get_cumulative_lift_fix_step(spark, df_labels_join, ord_col =  'model_score', label_col = 'label', step_=500)\n",
    "#cum_churn_rate.show(150, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencia de Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from churn_nrt.src.utils.date_functions import move_date_n_days\n",
    "from pyspark.sql.functions import avg as sql_avg\n",
    "\n",
    "\n",
    "msisdn_df = spark.read.table(\"raw_es.campaign_msisdncontacthist\")\n",
    "closing_day = \"20191023\"\n",
    "label_kind = \"mobile+fix\"\n",
    "\n",
    "\n",
    "insights_df = msisdn_df.filter((col('year')==int(closing_day[:4])) & (col('month') == int(closing_day[4:6])) & (col('day') == int(closing_day[6:])) & (col(\"CampaignCode\").contains(\"COMP_WEB\")) & (col(\"Creatividad\").startswith(\"LLAM_\"))).select(\"msisdn\").distinct()\n",
    "print(\"clients\", insights_df.count())\n",
    "#start_port = move_date_n_days(closing_day, n=1)\n",
    "end_port = move_date_n_days(closing_day, n=15)\n",
    "\n",
    "from churn.analysis.triggers.base_utils.base_utils import get_mobile_portout_requests, get_customer_base\n",
    "#insights_df.join(get_mobile_portout_requests(spark, start_port, end_port).select(\"msisdn\", \"label_mob\").withColumnRenamed(\"label_mob\", \"label\"), ['msisdn'], 'left').na.fill({'label': 0.0}).select(sql_avg('label')).show()\n",
    "insights_df.join(get_label(spark, closing_day, kind=label_kind, churn_window=15).select(\"msisdn\", \"label\"), ['msisdn'], 'left').na.fill({'label': 0.0}).select(sql_avg('label')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msisdn_df = spark.read.table(\"raw_es.campaign_msisdncontacthist\")\n",
    "msisdn_df = msisdn_df.filter((col(\"CampaignCode\").contains(\"COMP_WEB\")) & (col(\"Creatividad\").startswith(\"LLAM_\"))).drop_duplicates([\"msisdn\"])\n",
    "msisdn_df = msisdn_df.where(col(\"year\")==2019).where(col(\"month\")>=10)\n",
    "\n",
    "msisdn_df.groupby(\"year\", \"month\", \"day\").agg(sql_count(\"*\").alias(\"count\")).sort(desc(\"year\"), desc(\"month\"), desc(\"day\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_label(spark, closing_day, kind=\"mobile\", churn_window=15):\n",
    "    from churn_nrt.src.utils.date_functions import move_date_n_days\n",
    "    #start_port = move_date_n_days(closing_day, n=1)\n",
    "    end_port = move_date_n_days(closing_day, n=churn_window)\n",
    "\n",
    "    if kind==\"mobile\":\n",
    "\n",
    "        from churn.analysis.triggers.base_utils.base_utils import get_mobile_portout_requests, get_customer_base\n",
    "        df_target = get_mobile_portout_requests(spark, closing_day, end_port).select(\"msisdn\", \"label_mob\").withColumnRenamed(\"label_mob\", \"label\")\n",
    "        return df_target\n",
    "    \n",
    "    elif kind==\"mobile+fix\":\n",
    "        \n",
    "        print(\"Computing target with mobile and fix sopo\")\n",
    "        \n",
    "\n",
    "        from churn_nrt.src.data.sopos_dxs import FixPort\n",
    "        from churn_nrt.src.data.customer_base import CustomerBase\n",
    "\n",
    "        # Getting portout requests for fix and mobile services, and disconnections of fbb services\n",
    "        print(\"******* Asking for FixPort...\")\n",
    "        df_sopo_fix = FixPort(spark).get_module(closing_day, save=False, churn_window=churn_window)\n",
    "\n",
    "        # The base of active services on closing_day\n",
    "        from churn.analysis.triggers.base_utils.base_utils import get_mobile_portout_requests, get_customer_base\n",
    "        base_df = get_customer_base(spark, closing_day).filter(col('rgu') == 'mobile').select('msisdn', \"nif_cliente\")\n",
    "        \n",
    "        base_df = base_df\\\n",
    "                 .join(get_mobile_portout_requests(spark, closing_day, end_port).select('msisdn', 'label_mob'), ['msisdn'], 'left').na.fill(0.0)\n",
    "\n",
    "        df_sopos = (base_df.join(df_sopo_fix, ['msisdn'], \"left\").na.fill({'label_srv': 0.0}))\n",
    "\n",
    "        # 1 if any of the services of this nif is 1\n",
    "        window_nc = Window.partitionBy(\"nif_cliente\")\n",
    "\n",
    "        df_sopos = df_sopos.withColumn('tmp', when((col('label_srv') == 1.0) | (col('label_mob') == 1.0), 1.0).otherwise(0.0))\n",
    "        df_sopos = df_sopos.withColumn('label', sql_max('tmp').over(window_nc)).drop(\"tmp\").na.fill({'label': 0.0})     \n",
    "        \n",
    "        return df_sopos\n",
    "    \n",
    "    else:\n",
    "        print(\"Unknown kind {}. Program will exit here!\".format(kind))\n",
    "        return None\n",
    "                \n",
    "        \n",
    "#get_label(spark, closing_day=\"20191024\", kind=\"mobile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = spark.read.load(day_path2)\n",
    "#label_kind = \"mobile\"\n",
    "label_kind = \"mobile\" \n",
    "closing_day = \"20191022\"\n",
    "\n",
    "print(\"NUM LABELS ORIG\", df_labels.select(sql_sum('label').alias('num_churners')).rdd.first()['num_churners'])\n",
    "\n",
    "if label_kind in  [\"mobile+fix\", \"mobile\"]:\n",
    "    df_lab = get_label(spark, closing_day, kind=label_kind, churn_window=15).select(\"msisdn\", \"label\")\n",
    "    df_labels = df_labels.drop(\"label\")\n",
    "    df_labels = df_labels.join(df_lab, on=[\"msisdn\"], how=\"left\").na.fill({'label': 0.0})\n",
    "elif label_kind != \"mobile\":\n",
    "    print(\"Unknown kind {}. Program will exit here!\".format(kind))\n",
    "    import sys\n",
    "    sys.exit()\n",
    "\n",
    "#from pyspark.sql.functions import avg as sql_avg\n",
    "#myschema = df_labels.schema\n",
    "#df_labels = df_labels.sort(desc(\"model_score\"))\n",
    "#df_top_9286 = spark.createDataFrame(df_labels.head(9286), schema=myschema)\n",
    "\n",
    "num_churners = df_labels.select(sql_sum('label').alias('num_churners')).rdd.first()['num_churners']\n",
    "num_churners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from churn_nrt.src.data.navcomp_data import NavCompData\n",
    "\n",
    "\n",
    "df_navcomp_data = NavCompData(spark).get_module(\"20191016\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.load(\"/data/udf/vf_es/churn_nrt/customer_base/year=2019/month=10/day=16\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_date = \"20191001\"\n",
    "process_date = \"20191016\"\n",
    "level= \"msisdn\"\n",
    "orig_path = True\n",
    "\n",
    "from pyspark.sql.functions import (col,\n",
    "                                    when,\n",
    "                                    lit,\n",
    "                                    lower,\n",
    "                                    array,\n",
    "                                    concat,\n",
    "                                    translate,\n",
    "                                    count,\n",
    "                                    sum as sql_sum,\n",
    "                                    max as sql_max,\n",
    "                                    min as sql_min,\n",
    "                                    avg as sql_avg,\n",
    "                                    lpad,\n",
    "                                    greatest,\n",
    "                                    least,\n",
    "                                    isnull,\n",
    "                                    isnan,\n",
    "                                    struct,\n",
    "                                    substring,\n",
    "                                    size,\n",
    "                                    length,\n",
    "                                    udf,\n",
    "                                    year,\n",
    "                                    month,\n",
    "                                    dayofmonth,\n",
    "                                    unix_timestamp,\n",
    "                                    date_format,\n",
    "                                    from_unixtime,\n",
    "                                    datediff,\n",
    "                                    to_date,\n",
    "                                    desc,\n",
    "                                    asc,\n",
    "                                    countDistinct,\n",
    "                                    row_number,\n",
    "                                    regexp_replace,\n",
    "                                    upper,\n",
    "                                    trim)\n",
    "\n",
    "\n",
    "\n",
    "from functools import reduce\n",
    "import itertools\n",
    "from pykhaos.utils.date_functions import get_diff_days\n",
    "\n",
    "if(\"nif\" in level.lower()):\n",
    "    level = \"nif_cliente\"\n",
    "\n",
    "print(\"[Info get_navcomp_attributes] starting_date: \" + starting_date + \" - ending_date: \" + process_date + \" - level: \" + level)\n",
    "\n",
    "window_length = get_diff_days(starting_date, process_date)\n",
    "\n",
    "print(\"[Info get_navcomp_attributes] Starting the computation of the feats\")\n",
    "\n",
    "competitors = [\"PEPEPHONE\", \"ORANGE\", \"JAZZTEL\", \"MOVISTAR\", \"MASMOVIL\", \"YOIGO\", \"VODAFONE\", \"LOWI\", \"O2\", \"unknown\"]\n",
    "\n",
    "count_feats = [p[0] + \"_\" + p[1] for p in list(itertools.product(competitors, [\"sum_count\", \"max_count\"]))]\n",
    "\n",
    "days_feats = [p[0] + \"_\" + p[1] for p in list(itertools.product(competitors, [\"max_days_since_navigation\", \"min_days_since_navigation\", \"distinct_days_with_navigation\"]))]\n",
    "\n",
    "null_imp_dict = dict([(x, 0) for x in count_feats] + [(x, 0) for x in days_feats if((\"distinct_days\" in x) | (\"max_days\" in x))] + [(x, 10000) for x in days_feats if(\"min_days\" in x)])\n",
    "\n",
    "from churn.analysis.triggers.base_utils.base_utils import get_customer_base\n",
    "\n",
    "basedf = get_customer_base(spark, process_date).select('msisdn', 'nif_cliente')\n",
    "#print(\"BASEDF {}\".format(basedf.count()))\n",
    "\n",
    "path_to_read = \"/data/attributes/vf_es/return_feed/data_navigation/\" if orig_path==True else \"/data/udf/vf_es/netscout/dailyMSISDNApplicationName/\"\n",
    "apps_names = [u'WEB_O2_HTTPS',\n",
    "             u'WEB_MOVISTAR_HTTPS',\n",
    "             u'WEB_VODAFONE_HTTPS',\n",
    "             u'WEB_YOIGO_HTTP',\n",
    "             u'WEB_LOWI_HTTPS',\n",
    "             u'WEB_ORANGE_HTTP',\n",
    "             u'WEB_JAZZTEL_HTTP',\n",
    "             u'WEB_MASMOVIL_HTTP',\n",
    "             u'WEB_VODAFONE_HTTP',\n",
    "             u'WEB_MOVISTAR_HTTP',\n",
    "             u'WEB_ORANGE_HTTPS',\n",
    "             u'WEB_PEPEPHONE_HTTPS',\n",
    "             u'WEB_PEPEPHONE_HTTP',\n",
    "             u'WEB_JAZZTEL_HTTPS',\n",
    "             u'WEB_LOWI_HTTP',\n",
    "             u'WEB_MASMOVIL_HTTPS',\n",
    "             u'WEB_YOIGO_HTTPS',\n",
    "             u'WEB_O2_HTTP']\n",
    "\n",
    "print(\"Reading from {}\".format(path_to_read))\n",
    "\n",
    "repart_navigdf = (spark.read.parquet(path_to_read))\n",
    "\n",
    "if not orig_path:\n",
    "    print(\"Adding event_date\")\n",
    "    repart_navigdf = (repart_navigdf.where((col(\"SUM_userplane_upload_bytes_count\") + col(\"SUM_userplane_download_bytes_count\")) > 524288)\n",
    "                                    .where(col(\"application_name\").isin(apps_names))\n",
    "                                    .withColumn(\"event_date\",  concat(col('year'), lit(\"-\"), lpad(col('month'), 2, '0'), lit(\"-\"), lpad(col('day'), 2, '0'))))\n",
    "\n",
    "repart_navigdf = (repart_navigdf.withColumn(\"formatted_event_date\", from_unixtime(unix_timestamp(col(\"event_date\"), \"yyyy-MM-dd\")))\n",
    "        .filter((col(\"formatted_event_date\") >= from_unixtime(unix_timestamp(lit(starting_date), \"yyyyMMdd\"))) & (col(\"formatted_event_date\") <= from_unixtime(unix_timestamp(lit(process_date), \"yyyyMMdd\"))))\n",
    "        .withColumn(\"days_since_navigation\", datediff(from_unixtime(unix_timestamp(lit(process_date), \"yyyyMMdd\")), col(\"formatted_event_date\")).cast(\"double\"))\n",
    "        .withColumn(\"msisdn\", col(\"subscriber_msisdn\").substr(3, 9))\n",
    "        .select(\"msisdn\", \"application_name\", \"count\", \"formatted_event_date\", \"days_since_navigation\")\n",
    "  )\n",
    "\n",
    "print(\"hola1\", repart_navigdf.count())\n",
    "\n",
    "repart_navigdf = (repart_navigdf\n",
    "        .withColumn(\"competitor\", lit(\"unknown\"))\n",
    "        .withColumn(\"competitor\", when(col(\"application_name\").contains(\"PEPEPHONE\"), \"PEPEPHONE\").otherwise(col(\"competitor\")))\n",
    "        .withColumn(\"competitor\", when(col(\"application_name\").contains(\"ORANGE\"), \"ORANGE\").otherwise(col(\"competitor\")))\n",
    "        .withColumn(\"competitor\", when(col(\"application_name\").contains(\"JAZZTEL\"), \"JAZZTEL\").otherwise(col(\"competitor\")))\n",
    "        .withColumn(\"competitor\", when(col(\"application_name\").contains(\"MOVISTAR\"), \"MOVISTAR\").otherwise(col(\"competitor\")))\n",
    "        .withColumn(\"competitor\", when(col(\"application_name\").contains(\"MASMOVIL\"), \"MASMOVIL\").otherwise(col(\"competitor\")))\n",
    "        .withColumn(\"competitor\", when(col(\"application_name\").contains(\"YOIGO\"), \"YOIGO\").otherwise(col(\"competitor\")))\n",
    "        .withColumn(\"competitor\", when(col(\"application_name\").contains(\"VODAFONE\"), \"VODAFONE\").otherwise(col(\"competitor\")))\n",
    "        .withColumn(\"competitor\", when(col(\"application_name\").contains(\"LOWI\"), \"LOWI\").otherwise(col(\"competitor\")))\n",
    "        .withColumn(\"competitor\", when(col(\"application_name\").contains(\"O2\"), \"O2\").otherwise(col(\"competitor\")))\n",
    "        .join(basedf, ['msisdn'], 'inner')\n",
    "        .repartition(400)\n",
    ")\n",
    "\n",
    "print(\"hola2\", repart_navigdf.count())\n",
    "\n",
    "\n",
    "def get_most_consulted_operator(consulted_list, days_since_1st_navigation_list):\n",
    "    if not consulted_list: return \"None\"\n",
    "    combined_list = [(a, b, c) for a, b, c in zip(consulted_list, days_since_1st_navigation_list, competitors)]\n",
    "    # sort by consulted list and then by days since 1st navigation\n",
    "    sorted_list = sorted(combined_list, key=lambda x: (x[0], x[1]), reverse=True)\n",
    "    return sorted_list[0][2] if sorted_list else \"None\"\n",
    "\n",
    "get_most_consulted_operator_udf = udf(lambda x, y: get_most_consulted_operator(x, y), StringType())\n",
    "\n",
    "#print(\"[Info get_navcomp_attributes] After joining the base - Volume of repart_navigdf: \" + str(repart_navigdf.count()) + \" - Distinct msisdn: \" + str(repart_navigdf.select('msisdn').distinct().count()) + \" - Distinct nif: \" + str(repart_navigdf.select('nif_cliente').distinct().count()))\n",
    "\n",
    "repart_navigdf = (\n",
    "    repart_navigdf\n",
    "        .groupBy(level)\n",
    "        .pivot(\"competitor\", competitors)\n",
    "        .agg(sql_sum(\"count\").alias(\"sum_count\"),\n",
    "             sql_max(\"count\").alias(\"max_count\"),\n",
    "             sql_max(\"days_since_navigation\").alias(\"max_days_since_navigation\"),\n",
    "             sql_min(\"days_since_navigation\").alias(\"min_days_since_navigation\"),\n",
    "             countDistinct(\"formatted_event_date\").alias(\"distinct_days_with_navigation\"))\n",
    "        .na\n",
    "        .fill(null_imp_dict)\n",
    "        .withColumn(\"sum_count_vdf\", reduce(lambda a, b: a + b, [col(x) for x in count_feats if((\"sum_count\" in x) & (\"VODAFONE\" in x))]))\n",
    "        .withColumn(\"sum_count_comps\", reduce(lambda a, b: a + b, [col(x) for x in count_feats if ((\"sum_count\" in x) & (\"VODAFONE\" not in x))]))\n",
    "        .withColumn(\"num_distinct_comps\", reduce(lambda a, b: a + b, [when(col(x) > 0, 1.0).otherwise(0.0) for x in count_feats if((\"sum_count\" in x) & (\"VODAFONE\" not in x))]))\n",
    "        .withColumn(\"max_count_comps\", greatest(*[x for x in count_feats if ((\"max_count\" in x) & (\"VODAFONE\" not in x))]))\n",
    "        .withColumn(\"sum_distinct_days_with_navigation_vdf\", reduce(lambda a, b: a + b, [col(x) for x in days_feats if ((\"distinct_days_with_navigation\" in x) & (\"VODAFONE\" in x))]))\n",
    "        .withColumn(\"norm_sum_distinct_days_with_navigation_vdf\", col(\"sum_distinct_days_with_navigation_vdf\").cast('double')/lit(window_length).cast('double'))\n",
    "        .withColumn(\"sum_distinct_days_with_navigation_comps\", reduce(lambda a, b: a + b, [col(x) for x in days_feats if ((\"distinct_days_with_navigation\" in x) & (\"VODAFONE\" not in x))]))\n",
    "        .withColumn(\"norm_sum_distinct_days_with_navigation_comps\", col(\"sum_distinct_days_with_navigation_comps\").cast('double') / lit(window_length).cast('double'))\n",
    "        .withColumn(\"min_days_since_navigation_comps\", least(*[x for x in days_feats if ((\"min_days_since_navigation\" in x) & (\"VODAFONE\" not in x))]))\n",
    "        .withColumn(\"norm_min_days_since_navigation_comps\", col(\"min_days_since_navigation_comps\").cast('double') / lit(window_length).cast('double'))\n",
    "        .withColumn(\"max_days_since_navigation_comps\", greatest(*[x for x in days_feats if ((\"max_days_since_navigation\" in x) & (\"VODAFONE\" not in x))]))\n",
    "        .withColumn(\"norm_max_days_since_navigation_comps\", col(\"max_days_since_navigation_comps\").cast('double') / lit(window_length).cast('double'))\n",
    "        .withColumn('consulted_list', array(*[col_ + \"_sum_count\" for col_ in competitors]))\n",
    "        .withColumn('days_since_1st_navigation_list',  array(*[col_ + \"_max_days_since_navigation\" for col_ in competitors]))\n",
    "        .withColumn('most_consulted_operator', get_most_consulted_operator_udf(col('consulted_list'), col(\"days_since_1st_navigation_list\")))\n",
    "        .drop('consulted_list', 'days_since_1st_navigation_list'))\n",
    "\n",
    "#print(\"[Info get_navcomp_attributes] After pivoting - Volume of repart_navigdf: \" + str(repart_navigdf.count()) + \" - Distinct \" + level + \": \" + str(repart_navigdf.select(level).distinct().count()))\n",
    "\n",
    "print(\"[Infoget_navcomp_attributes] Competitors web extraction completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repart_navigdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "closing_day = \"20190714\"\n",
    "from churn_nrt.src.data.customer_base import CustomerBase\n",
    "base_df = CustomerBase(spark).get_module(closing_day)\n",
    "base_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from churn_nrt.src.data.customers_data import Customer\n",
    "customer_df = Customer(spark).get_module(closing_day, save=False)\n",
    "customer_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.load(\"/data/udf/vf_es/churn_nrt/ccc/year=2019/month=7/day=14\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closing_day = \"20190714\"\n",
    "from churn_nrt.src.data.ccc import CCC\n",
    "tr_ccc_all = CCC(spark, level=\"nif\").get_module(closing_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ccc_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "logging_file = os.path.join(\"/var/SP/data/bdpmdses/deliveries_churn/logging\",\n",
    "                                \"churn_delivery_log_time_\" + dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".log\")\n",
    "logging_file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
