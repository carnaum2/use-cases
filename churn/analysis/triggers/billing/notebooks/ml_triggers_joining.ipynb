{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEVEL_SRC must contain the directory use-cases and pykhaos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20190830-115714 [INFO ] Logging to file /var/SP/data/home/csanc109/logging/out_20190830_115714.log\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, sys\n",
    "import datetime as dt\n",
    "DEVEL_SRC = os.path.join(os.environ.get('BDA_USER_HOME', ''), \"src\", \"devel\")\n",
    "if DEVEL_SRC not in sys.path:\n",
    "    sys.path.append(DEVEL_SRC)\n",
    "\n",
    "USECASES_SRC = os.path.join(DEVEL_SRC, \"use-cases\") # TODO when '-' is removed from name, remove also this line and adapt imports \n",
    "if USECASES_SRC not in sys.path: \n",
    "    sys.path.append(USECASES_SRC)\n",
    "    \n",
    "# AMDOCS_SRC = os.path.join(DEVEL_SRC, \"amdocs_informational_dataset\") # TODO when - is removed, remove also this line and adapt imports\n",
    "# if AMDOCS_SRC not in sys.path: \n",
    "#     sys.path.append(AMDOCS_SRC)\n",
    "    \n",
    "import pykhaos.utils.custom_logger as clogger\n",
    "logging_file = os.path.join(os.environ.get('BDA_USER_HOME', ''), \"logging\",\n",
    "                                    \"out_\" + dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".log\")\n",
    "logger = clogger.configure_logger(log_filename=logging_file, std_channel=sys.stderr, logger_name=\"\")\n",
    "logger.info(\"Logging to file {}\".format(logging_file))    \n",
    "        \n",
    "from project.project_generic import Project\n",
    "\n",
    "import pykhaos.utils.notebooks as nb\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "RUNNING_FROM_NOTEBOOK = nb.isnotebook()\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "if RUNNING_FROM_NOTEBOOK:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    %matplotlib inline  \n",
    "    EXTERNAL_LIB = os.path.join(os.environ.get('BDA_USER_HOME', ''), \"lib\", \"external_libs\")\n",
    "    if EXTERNAL_LIB not in sys.path:\n",
    "        sys.path.append(EXTERNAL_LIB)\n",
    "    # feel free from commenting this line and the other ones that begin with \"%%notify\" if you do not have \n",
    "    # the extension installed or copy de lib from /var/SP/data/home/csanc109/lib/external_libs/jupyternotify/\n",
    "    %load_ext jupyternotify \n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import Row, DataFrame, Column, Window\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType, DateType, ArrayType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler, SQLTransformer, OneHotEncoder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.sql.functions import collect_set, concat, size, coalesce, col, lpad, struct, count as sql_count, lit, min as sql_min, max as sql_max, collect_list, udf, when, desc, asc, to_date, create_map, sum as sql_sum\n",
    "from pyspark.sql.types import StringType, ArrayType, MapType, StructType, StructField, IntegerType\n",
    "from pyspark.sql.functions import array, regexp_extract\n",
    "from itertools import chain\n",
    "from churn.datapreparation.general.data_loader import get_unlabeled_car, get_port_requests_table, get_numclients_under_analysis\n",
    "from churn.utils.constants import PORT_TABLE_NAME\n",
    "from churn.utils.udf_manager import Funct_to_UDF\n",
    "from pyspark.sql.functions import substring, datediff, row_number\n",
    "from pykhaos.utils.date_functions import move_date_n_days, move_date_n_cycles\n",
    "from pykhaos.utils.hdfs_functions import check_hdfs_exists\n",
    "from pykhaos.modeling.model_performance import get_lift\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ended spark session: 39.6994678974 secs | default parallelism=2\n"
     ]
    }
   ],
   "source": [
    "from churn.utils.general_functions import init_spark\n",
    "spark = init_spark(\"billing\")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write here the prediction date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "closing_day = \"20190521\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read output of both models and compute simple stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATS\n",
      "\tNIFs model2 = 50000\n",
      "\tNIFs model1 = 50000\n",
      "\t Common nifs = 15699\n",
      "NIFs nuevos model1 sobre model2 = 34301 [check=50000]\n",
      "NIFs nuevos model2 sobre model1 = 34301 [check=50000]\n",
      "Reference base: churn_rate=3.25 volume=3899811\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean as sql_avg, greatest\n",
    "\n",
    "\n",
    "partition_date = \"year={}/month={}/day={}\".format(int(closing_day[:4]), int(closing_day[4:6]), int(closing_day[6:]))\n",
    "\n",
    "# model1 is Cristina's model\n",
    "df_model1 = (spark.read.load(\"/data/udf/vf_es/churn/triggers/model1_50k/\" + partition_date)\n",
    "                            .select(['NIF_CLIENTE', 'calib_model_score', 'label']).withColumnRenamed(\"calib_model_score\", \"calib_model_score_model1\")                                                                         \n",
    "                 .withColumnRenamed(\"label\", \"label_model1\")).where(col(\"NIF_CLIENTE\").isNotNull()).drop_duplicates([\"NIF_CLIENTE\"])\n",
    "\n",
    "\n",
    "# model2 is Alvaro's model\n",
    "df_model2 = (spark.read.load(\"/data/udf/vf_es/churn/triggers/model2_50k/\" +  partition_date).select(['NIF_CLIENTE', 'calib_model_score', \"label\"])\n",
    "                                                                         .withColumnRenamed(\"calib_model_score\", \"calib_model_score_model2\")\n",
    "                                                                         .withColumnRenamed(\"label\", \"label_model2\")).where(col(\"NIF_CLIENTE\").isNotNull()).drop_duplicates([\"NIF_CLIENTE\"])\n",
    "\n",
    "          \n",
    "print(\"STATS\") # just a check, count must be 50k in both cases\n",
    "print('\\tNIFs model1 = {}'.format(df_model1.count()))\n",
    "print('\\tNIFs model2 = {}'.format(df_model2.count()))\n",
    "\n",
    "# compute common nifs\n",
    "df_inner = df_model1.join(df_model2, on=[\"nif_cliente\"], how=\"inner\")\n",
    "common_nifs = df_inner.count()\n",
    "print(\"\\t Common nifs = {}\".format(common_nifs))\n",
    "\n",
    "# NIFs in Cris' model not included in Alvaro's model\n",
    "df_model1_no_model2 = df_model2.select(\"nif_cliente\").join(df_model1.select(\"nif_cliente\"), ['nif_cliente'], 'right').where(df_model2['nif_cliente'].isNull())\n",
    "\n",
    "# NIFs in Alvaro's model not included in Cris' model\n",
    "df_model2_no_model1 = df_model1.select(\"nif_cliente\").join(df_model2.select(\"nif_cliente\"), ['nif_cliente'], 'right').where(df_model1['nif_cliente'].isNull())\n",
    "\n",
    "# Absurd print, since new NIFs are 50k - common, but good for checking\n",
    "print(\"NIFs nuevos model1 sobre model2 = {} [check={}]\".format(df_model1_no_model2.count(), df_model1_no_model2.count() + common_nifs))\n",
    "print(\"NIFs nuevos model2 sobre model1 = {} [check={}]\".format(df_model2_no_model1.count(), df_model2_no_model1.count() + common_nifs))\n",
    "\n",
    "from churn.analysis.triggers.trigger_billing_ccc.trigger_smart_exploration import get_customer_master\n",
    "\n",
    "_, volume_ref, churn_ref = get_customer_master(spark, closing_day)\n",
    "\n",
    "print(\"Reference base: churn_rate={:.2f} volume={}\".format(100.0*churn_ref, volume_ref))\n",
    "                                                                                                               \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join both dataframes - if NIF is in both, keep the highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination of both models has 84301 unique nifs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pos_max_udf = udf(lambda milist: sorted([(vv, idx) for idx, vv in enumerate(milist)], key=lambda tup: tup[0], reverse=True)[0][1], IntegerType())\n",
    "\n",
    "df_all = df_model2.join(df_model1, on=[\"nif_cliente\"], how=\"full\").fillna(0)  \n",
    "\n",
    "df_all = df_all.withColumn(\"calib_model_score\", greatest(*[\"calib_model_score_model1\", \"calib_model_score_model2\"]))\n",
    "\n",
    "df_all = df_all.withColumn(\"score_array\", array([\"calib_model_score_model1\", \"calib_model_score_model2\"]))  \n",
    "df_all = df_all.withColumn(\"pos_max\", pos_max_udf(col(\"score_array\")))  \n",
    "\n",
    "\n",
    "df_all = df_all.withColumn(\"label\", greatest(*[\"label_model1\", \"label_model2\"])).where(col(\"NIF_CLIENTE\").isNotNull()).drop_duplicates([\"NIF_CLIENTE\"])\n",
    "df_all = df_all.withColumn(\"model\", when(col(\"pos_max\")==0, \"csanc109\").when(col(\"pos_max\")==1, \"asaezco\").otherwise(\"unknown\"))\n",
    "\n",
    "df_all = df_all.drop(\"pos_max\", \"score_array\")\n",
    "df_all = df_all.sort(desc(\"calib_model_score\"))\n",
    "\n",
    "df_all = df_all.cache()\n",
    "\n",
    "num_rows = df_all.count()\n",
    "\n",
    "print(\"Combination of both models has {} unique nifs\".format(num_rows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NIF_CLIENTE',\n",
       " 'calib_model_score_model2',\n",
       " 'label_model2',\n",
       " 'calib_model_score_model1',\n",
       " 'label_model1',\n",
       " 'calib_model_score',\n",
       " 'label',\n",
       " 'model']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write combination "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started saving - /user/csanc109/projects/triggers/trigger_exploration_combined/ for closing_day=20190521\n",
      "Elapsed time saving 0.00830854972204 minutes\n"
     ]
    }
   ],
   "source": [
    "path_to_save = \"/data/udf/vf_es/churn/triggers/model_combined/\"\n",
    "\n",
    "df_all = df_all.withColumn(\"day\", lit(int(closing_day[6:])))\n",
    "df_all = df_all.withColumn(\"month\", lit(int(closing_day[4:6])))\n",
    "df_all = df_all.withColumn(\"year\", lit(int(closing_day[:4])))\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Started saving - {} for closing_day={}\".format(path_to_save, closing_day))\n",
    "(df_all.coalesce(1).write.partitionBy('year', 'month', 'day').mode(\"append\").format(\"parquet\").save(path_to_save))\n",
    "print(\"Elapsed time saving {} minutes\".format((time.time()-start_time)/60.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the volume vs lift for model combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP84301 - LIFT=5.70 churn_rate=18.50% churn_rate_ref=3.25%\n",
      "\t COMMON=15699 [18.62%] || FROM_MODEL1=34301 [40.69%] || FROM_MODEL2=34301 [40.69%]\n",
      "TOP50000 - LIFT=7.21 churn_rate=23.41% churn_rate_ref=3.25%\n",
      "\t COMMON=13538 [27.08%] || FROM_MODEL1=13269 [26.54%] || FROM_MODEL2=23193 [46.39%]\n",
      "TOP40000 - LIFT=8.00 churn_rate=25.98% churn_rate_ref=3.25%\n",
      "\t COMMON=11748 [29.37%] || FROM_MODEL1=10642 [26.61%] || FROM_MODEL2=17610 [44.02%]\n",
      "TOP30000 - LIFT=9.14 churn_rate=29.67% churn_rate_ref=3.25%\n",
      "\t COMMON=9329 [31.10%] || FROM_MODEL1=8155 [27.18%] || FROM_MODEL2=12516 [41.72%]\n",
      "TOP20000 - LIFT=10.95 churn_rate=35.52% churn_rate_ref=3.25%\n",
      "\t COMMON=6534 [32.67%] || FROM_MODEL1=5349 [26.75%] || FROM_MODEL2=8117 [40.59%]\n",
      "TOP15000 - LIFT=12.33 churn_rate=40.00% churn_rate_ref=3.25%\n",
      "\t COMMON=5101 [34.01%] || FROM_MODEL1=3921 [26.14%] || FROM_MODEL2=5978 [39.85%]\n",
      "TOP12000 - LIFT=13.44 churn_rate=43.61% churn_rate_ref=3.25%\n",
      "\t COMMON=4273 [35.61%] || FROM_MODEL1=2978 [24.82%] || FROM_MODEL2=4749 [39.58%]\n",
      "TOP10000 - LIFT=14.33 churn_rate=46.51% churn_rate_ref=3.25%\n",
      "\t COMMON=3715 [37.15%] || FROM_MODEL1=2336 [23.36%] || FROM_MODEL2=3949 [39.49%]\n",
      "TOP5000 - LIFT=17.25 churn_rate=55.98% churn_rate_ref=3.25%\n",
      "\t COMMON=2099 [41.98%] || FROM_MODEL1=1165 [23.30%] || FROM_MODEL2=1736 [34.72%]\n",
      "TOP3000 - LIFT=18.66 churn_rate=60.57% churn_rate_ref=3.25%\n",
      "\t COMMON=1336 [44.53%] || FROM_MODEL1=655 [21.83%] || FROM_MODEL2=1009 [33.63%]\n",
      "TOP2000 - LIFT=19.91 churn_rate=64.60% churn_rate_ref=3.25%\n",
      "\t COMMON=964 [48.20%] || FROM_MODEL1=398 [19.90%] || FROM_MODEL2=638 [31.90%]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "myschema = df_all.schema\n",
    "\n",
    "for ii in [num_rows, 50000, 40000, 30000, 20000, 15000, 12000, 10000, 5000, 3000, 2000]: \n",
    "    \n",
    "    start_time_ii = time.time()\n",
    "    df_top = spark.createDataFrame(df_all.head(ii),  schema=myschema)\n",
    "    churn_rate_top =  df_top.select(sql_avg('label').alias('churn_rate')).rdd.first()['churn_rate']\n",
    "    print(\"TOP{} - LIFT={:.2f} churn_rate={:.2f}% churn_rate_ref={:.2f}%\".format(ii, \n",
    "                                                                                          churn_rate_top/churn_ref,\n",
    "                                                                                          churn_rate_top*100.0,\n",
    "                                                                                          churn_ref*100.0, \n",
    "                                                                                          (time.time()-start_time_ii)/60.0))\n",
    "    inner_nifs = df_top.join(df_inner, on=[\"NIF_CLIENTE\"], how=\"inner\").select(\"NIF_CLIENTE\").count()\n",
    "    from_model1_nifs = df_top.join(df_model1_no_model2, on=[\"NIF_CLIENTE\"], how=\"inner\").select(\"NIF_CLIENTE\").count()\n",
    "    from_model2_nifs = df_top.join(df_model2_no_model1, on=[\"NIF_CLIENTE\"], how=\"inner\").select(\"NIF_CLIENTE\").count()\n",
    "    \n",
    "    print(\"\\t COMMON={} [{:.2f}%] || FROM_MODEL1={} [{:.2f}%] || FROM_MODEL2={} [{:.2f}%]\".format(inner_nifs, 100.0*inner_nifs/ii, from_model1_nifs, 100.0*from_model1_nifs/ii, from_model2_nifs, 100.0*from_model2_nifs/ii))\n",
    "                                                           \n",
    "                                                           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE END - the goal of the script ends here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This block is just for me, check de volume vs lift for model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP50000 - LIFT=6.44 churn_rate=20.91% churn_rate_ref=3.25%\n",
      "TOP40000 - LIFT=6.91 churn_rate=22.42% churn_rate_ref=3.25%\n",
      "TOP30000 - LIFT=7.72 churn_rate=25.06% churn_rate_ref=3.25%\n",
      "TOP20000 - LIFT=9.05 churn_rate=29.36% churn_rate_ref=3.25%\n",
      "TOP16000 - LIFT=9.62 churn_rate=31.23% churn_rate_ref=3.25%\n",
      "TOP12000 - LIFT=10.77 churn_rate=34.96% churn_rate_ref=3.25%\n",
      "TOP10000 - LIFT=11.54 churn_rate=37.44% churn_rate_ref=3.25%\n",
      "TOP5000 - LIFT=14.75 churn_rate=47.88% churn_rate_ref=3.25%\n",
      "TOP3000 - LIFT=16.94 churn_rate=54.97% churn_rate_ref=3.25%\n",
      "TOP2000 - LIFT=18.07 churn_rate=58.65% churn_rate_ref=3.25%\n",
      "TOP1000 - LIFT=19.17 churn_rate=62.20% churn_rate_ref=3.25%\n",
      "TOP2500 - LIFT=17.51 churn_rate=56.84% churn_rate_ref=3.25%\n",
      "TOP8000 - LIFT=12.65 churn_rate=41.05% churn_rate_ref=3.25%\n",
      "TOP15000 - LIFT=9.83 churn_rate=31.90% churn_rate_ref=3.25%\n",
      "TOP25000 - LIFT=8.34 churn_rate=27.08% churn_rate_ref=3.25%\n",
      "TOP35000 - LIFT=7.26 churn_rate=23.56% churn_rate_ref=3.25%\n",
      "TOP45000 - LIFT=6.66 churn_rate=21.60% churn_rate_ref=3.25%\n",
      "TOP84301 - LIFT=6.44 churn_rate=20.91% churn_rate_ref=3.25%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_model1 = (spark.read.load(\"/data/udf/vf_es/churn/triggers/model1_50k/\" + partition_date)\n",
    "                            .select(['NIF_CLIENTE', 'model_score', 'label']))\n",
    "myschema = df_model1.schema\n",
    "\n",
    "df_model1 = df_model1.sort(desc(\"model_score\"))\n",
    "\n",
    "\n",
    "for ii in [50000, 40000, 30000, 20000, 16000, 12000, 10000, 5000, 3000, 2000, 1000, 2500, 8000, 15000, 25000, 35000, 45000, num_rows]:\n",
    "    \n",
    "    start_time_ii = time.time()\n",
    "    df_top = spark.createDataFrame(df_model1.head(ii),  schema=myschema)\n",
    "    churn_rate_top =  df_top.select(sql_avg('label').alias('churn_rate')).rdd.first()['churn_rate']\n",
    "    print(\"TOP{} - LIFT={:.2f} churn_rate={:.2f}% churn_rate_ref={:.2f}%\".format(ii, \n",
    "                                                                                          churn_rate_top/churn_ref,\n",
    "                                                                                          churn_rate_top*100.0,\n",
    "                                                                                          churn_ref*100.0, \n",
    "                                                                                          (time.time()-start_time_ii)/60.0))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check incrementals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#incremental - not finished\n",
    "\n",
    "# df_C = df_cris.select(\"nif_cliente\").join(df_cris2.select(\"nif_cliente\"), ['nif_cliente'], 'right').where(df_cris['nif_cliente'].isNull())\n",
    "# print(\"RESUMEN - INCREMENTALES\", df_C.count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
