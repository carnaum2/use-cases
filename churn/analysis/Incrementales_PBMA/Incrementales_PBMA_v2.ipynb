{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added '/var/SP/data/home/bgmerin1/src/Repositorios/use-cases' to path\n",
      "Added '/var/SP/data/home/bgmerin1/src/Repositorios' to path\n"
     ]
    }
   ],
   "source": [
    "def set_paths():\n",
    "    '''\n",
    "    Deployment should be something like \"dirs/dir1/use-cases\"\n",
    "    This function adds to the path \"dirs/dir1/use-cases\" and \"dirs/dir1/\"\n",
    "    :return:\n",
    "    '''\n",
    "    import imp\n",
    "    from os.path import dirname\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    USE_CASES = \"/var/SP/data/home/bgmerin1/src/Repositorios/use-cases\"#dirname(os.path.abspath(imp.find_module('churn')[1]))\n",
    "\n",
    "    if USE_CASES not in sys.path:\n",
    "        sys.path.append(USE_CASES)\n",
    "        print(\"Added '{}' to path\".format(USE_CASES))\n",
    "\n",
    "    # if deployment is correct, this path should be the one that contains \"use-cases\", \"pykhaos\", ...\n",
    "    # FIXME another way of doing it more general?\n",
    "    DEVEL_SRC = os.path.dirname(USE_CASES)  # dir before use-cases dir\n",
    "    if DEVEL_SRC not in sys.path:\n",
    "        sys.path.append(DEVEL_SRC)\n",
    "        print(\"Added '{}' to path\".format(DEVEL_SRC))\n",
    "set_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "\n",
    "import sys\n",
    "\n",
    "from common.src.main.python.utils.hdfs_generic import *\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pyspark.sql.functions import (udf,\n",
    "                                    col,\n",
    "                                    decode,\n",
    "                                    when,\n",
    "                                    lit,\n",
    "                                    lower,\n",
    "                                    concat,\n",
    "                                    translate,\n",
    "                                    count,\n",
    "                                    sum as sql_sum,\n",
    "                                    max as sql_max,\n",
    "                                    min as sql_min,\n",
    "                                    avg as sql_avg,\n",
    "                                    greatest,\n",
    "                                    least,\n",
    "                                    isnull,\n",
    "                                    isnan,\n",
    "                                    struct, \n",
    "                                    substring,\n",
    "                                    size,\n",
    "                                    length,\n",
    "                                    year,\n",
    "                                    month,\n",
    "                                    dayofmonth,\n",
    "                                    unix_timestamp,\n",
    "                                    date_format,\n",
    "                                    from_unixtime,\n",
    "                                    datediff,\n",
    "                                    to_date, \n",
    "                                    desc,\n",
    "                                    asc,\n",
    "                                    countDistinct,\n",
    "                                    row_number,\n",
    "                                    skewness,\n",
    "                                    kurtosis,\n",
    "                                    concat_ws,\n",
    "                                    upper)\n",
    "\n",
    "from pyspark.sql import Row, DataFrame, Column, Window\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType, DateType, ArrayType, FloatType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler, SQLTransformer, OneHotEncoder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from datetime import datetime\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from churn.models.fbb_churn_amdocs.utils_general import *\n",
    "from pykhaos.utils.date_functions import convert_to_date\n",
    "from churn.models.fbb_churn_amdocs.utils_model import *\n",
    "from churn.models.fbb_churn_amdocs.utils_fbb_churn import *\n",
    "from churn.models.fbb_churn_amdocs.metadata_fbb_churn import *\n",
    "from churn.models.fbb_churn_amdocs.feature_selection_utils import *\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para que no haga falta recargar el notebook si hacemos un cambio en alguna de las funciones que estamos importando\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set BDP parameters\n",
    "def setting_bdp(min_n_executors = 3, max_n_executors = 15, n_cores = 8, executor_memory = \"12g\", driver_memory=\"8g\",\n",
    "                   app_name = \"Python app\", driver_overhead=\"1g\", executor_overhead='3g'):\n",
    "\n",
    "    MAX_N_EXECUTORS = max_n_executors\n",
    "    MIN_N_EXECUTORS = min_n_executors\n",
    "    N_CORES_EXECUTOR = n_cores\n",
    "    EXECUTOR_IDLE_MAX_TIME = 120\n",
    "    EXECUTOR_MEMORY = executor_memory\n",
    "    DRIVER_MEMORY = driver_memory\n",
    "    N_CORES_DRIVER = 1\n",
    "    MEMORY_OVERHEAD = N_CORES_EXECUTOR * 2048\n",
    "    QUEUE = \"root.BDPtenants.es.medium\"\n",
    "    BDA_CORE_VERSION = \"1.0.0\"\n",
    "\n",
    "    SPARK_COMMON_OPTS = os.environ.get('SPARK_COMMON_OPTS', '')\n",
    "    SPARK_COMMON_OPTS += \" --executor-memory %s --driver-memory %s\" % (EXECUTOR_MEMORY, DRIVER_MEMORY)\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.shuffle.manager=tungsten-sort\"\n",
    "    SPARK_COMMON_OPTS += \"  --queue %s\" % QUEUE\n",
    "\n",
    "    # Dynamic allocation configuration\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.dynamicAllocation.enabled=true\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.shuffle.service.enabled=true\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.dynamicAllocation.maxExecutors=%s\" % (MAX_N_EXECUTORS)\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.dynamicAllocation.minExecutors=%s\" % (MIN_N_EXECUTORS)\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.executor.cores=%s\" % (N_CORES_EXECUTOR)\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.dynamicAllocation.executorIdleTimeout=%s\" % (EXECUTOR_IDLE_MAX_TIME)\n",
    "    # SPARK_COMMON_OPTS += \" --conf spark.ui.port=58235\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.port.maxRetries=100\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.app.name='%s'\" % (app_name)\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.submit.deployMode=client\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.ui.showConsoleProgress=true\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.sql.broadcastTimeout=1200\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.yarn.executor.memoryOverhead={}\".format(executor_overhead)\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.yarn.executor.driverOverhead={}\".format(driver_overhead)\n",
    "\n",
    "    BDA_ENV = os.environ.get('BDA_USER_HOME', '')\n",
    "\n",
    "    # Attach bda-core-ra codebase\n",
    "    SPARK_COMMON_OPTS+=\" --files {}/scripts/properties/red_agent/nodes.properties,{}/scripts/properties/red_agent/nodes-de.properties,{}/scripts/properties/red_agent/nodes-es.properties,{}/scripts/properties/red_agent/nodes-ie.properties,{}/scripts/properties/red_agent/nodes-it.properties,{}/scripts/properties/red_agent/nodes-pt.properties,{}/scripts/properties/red_agent/nodes-uk.properties\".format(*[BDA_ENV]*7)\n",
    "\n",
    "    os.environ[\"SPARK_COMMON_OPTS\"] = SPARK_COMMON_OPTS\n",
    "    os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"%s pyspark-shell \" % SPARK_COMMON_OPTS\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_session(app_name=\"default name\", log_level='INFO', min_n_executors = 3, max_n_executors = 15, n_cores = 4, executor_memory = \"12g\", driver_memory=\"8g\"):\n",
    "    HOME_SRC = os.path.join(os.environ.get('BDA_USER_HOME', ''), \"src\")\n",
    "    if HOME_SRC not in sys.path:\n",
    "        sys.path.append(HOME_SRC)\n",
    "\n",
    "\n",
    "    setting_bdp(app_name=app_name, min_n_executors = min_n_executors, max_n_executors = max_n_executors, n_cores = n_cores, executor_memory = executor_memory, driver_memory=driver_memory)\n",
    "    from common.src.main.python.utils.hdfs_generic import run_sc\n",
    "    sc, spark, sql_context = run_sc(log_level=log_level)\n",
    "\n",
    "\n",
    "    return sc, spark, sql_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(app_name, min_n_executors = 3, max_n_executors = 15, n_cores = 4, executor_memory = \"12g\", driver_memory=\"8g\"):\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"_initialize spark\")\n",
    "    #import pykhaos.utils.pyspark_configuration as pyspark_config\n",
    "    sc, spark, sql_context = get_spark_session(app_name=app_name, log_level=\"OFF\", min_n_executors = min_n_executors, max_n_executors = max_n_executors, n_cores = n_cores,\n",
    "                             executor_memory = executor_memory, driver_memory=driver_memory)\n",
    "    print(\"Ended spark session: {} secs | default parallelism={}\".format(time.time() - start_time,\n",
    "                                                                         sc.defaultParallelism))\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_initialize spark\n",
      "Ended spark session: 54.3324978352 secs | default parallelism=2\n"
     ]
    }
   ],
   "source": [
    "spark = initialize(\"PBMA Incrementales v2\",executor_memory = \"32g\",min_n_executors = 6,max_n_executors = 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Averias\n",
    "\n",
    "def averias(spark,starting_day,closing_day):\n",
    "\n",
    "    ga_tickets = spark.read.table('raw_es.callcentrecalls_ticketsow')\n",
    "    ga_tickets_detalle = spark.read.table('raw_es.callcentrecalls_ticketdetailow')\n",
    "    ga_franquicia = spark.read.table('raw_es.callcentrecalls_ticketfranchiseow')\n",
    "    ga_close_case = spark.read.table('raw_es.callcentrecalls_ticketclosecaseow')\n",
    "    clientes = spark.read.table('raw_es.customerprofilecar_customerow').select('OBJID', 'NIF_CLIENTE')\n",
    "    ga_tipo_tickets = spark.read.parquet('/data/raw/vf_es/cvm/GATYPETICKETS/1.0/parquet')\n",
    " \n",
    "    from pyspark.sql.functions import lpad\n",
    " \n",
    "    from pyspark.sql.functions import year, month, dayofmonth\n",
    "    tickets = (ga_tickets\n",
    "                     #.withColumn('year', year('CREATION_TIME'))\n",
    "                     #.withColumn('month', month('CREATION_TIME'))\n",
    "                     #.withColumn('day', dayofmonth('CREATION_TIME'))\n",
    "                     .where( (concat(col('year'),lpad(col('month'),2, '0'),lpad(col('day'),2, '0'))<=closing_day)\n",
    "                            &(concat(col('year'),lpad(col('month'),2, '0'),lpad(col('day'),2, '0'))>=starting_day) )\n",
    " \n",
    "                     .join(ga_tickets_detalle, ga_tickets.OBJID==ga_tickets_detalle.OBJID, 'left_outer')\n",
    "                     .join(ga_franquicia, ga_tickets_detalle.OBJID==ga_franquicia.ID_FRANQUICIA, 'left_outer')\n",
    "                     #.join(ga_centro_local, ga_tickets.CENTRO_LOCAL==ga_centro_local.ID_CENTRO_LOCAL, 'left_outer')\n",
    "                     #.join(ga_centro_regional, ga_tickets.CENTRO_REGIONAL==ga_centro_local.ID_CENTRO_REGIONAL, 'left_outer')\n",
    "                     .join(ga_close_case, ga_tickets.OBJID==ga_close_case.OBJID, 'left_outer')\n",
    "                     .join(clientes, ga_tickets.CASE_REPORTER2YESTE==clientes.OBJID, 'left_outer')\n",
    "                     #.join(ga_severidad, ga_tickets.ID_SEVERIDAD==ga_severidad.ID_SEVERIDAD, 'left_outer')\n",
    "                     .join(ga_tipo_tickets, ga_tickets.ID_TIPO_TICKET==ga_tipo_tickets.ID_TIPO_TICKET, 'left_outer')\n",
    "                     .filter(col('NIF_CLIENTE').isNotNull())\n",
    "                     .filter('NIF_CLIENTE != \"\"')\n",
    "                     .filter('NIF_CLIENTE != \"7\"')\n",
    "          )\n",
    " \n",
    "    averias = tickets.where(\"X_TIPO_OPERACION IN ('Averia')\")\n",
    " \n",
    "    from pyspark.sql.functions import countDistinct\n",
    "    averias_nif = averias.select('NIF_CLIENTE', 'ID_NUMBER').groupby('NIF_CLIENTE').agg(countDistinct('ID_NUMBER').alias('NUM_AVERIAS_NIF'))\n",
    "     \n",
    "    return averias_nif\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reclamaciones \n",
    "def reclamaciones (spark,starting_day,closing_day):\n",
    "    \n",
    "    from pyspark.sql.functions import year, month, dayofmonth\n",
    "    from pyspark.sql.functions import lpad\n",
    "\n",
    "    interac = (spark.read.table('raw_es.callcentrecalls_interactionono')\n",
    "                .withColumn('year', year('FX_CREATE_DATE'))\n",
    "                .withColumn('month', month('FX_CREATE_DATE'))\n",
    "                .withColumn('day', dayofmonth('FX_CREATE_DATE'))\n",
    "                .where( (concat(col('year'),lpad(col('month'),2, '0'),lpad(col('day'),2, '0'))<=closing_day)\n",
    "                       &(concat(col('year'),lpad(col('month'),2, '0'),lpad(col('day'),2, '0'))>=starting_day) )\n",
    "          )\n",
    "    clientes = (spark.read.table('raw_es.customerprofilecar_customerow')\n",
    "                      .select('NUM_CLIENTE', 'NIF_CLIENTE')\n",
    "                      .withColumnRenamed('NUM_CLIENTE', 'NUM_CLIENTE_cli')\n",
    "           )\n",
    "    servicios = spark.read.table('raw_es.customerprofilecar_servicesow').select('NUM_CLIENTE', 'NUM_SERIE')\n",
    "    serv_cli = servicios.join(clientes, servicios.NUM_CLIENTE==clientes.NUM_CLIENTE_cli, 'full_outer').drop('NUM_CLIENTE_cli')\n",
    " \n",
    "  #  serv_cli.groupBy('NUM_SERIE').agg(count('*')).sort('count(1)', ascending=False).show()\n",
    "  #  interac.groupBy('DS_X_PHONE_CONSULTATION').agg(count('*')).sort('count(1)', ascending=False).show()\n",
    "\n",
    "    reclamaciones = (interac.filter(upper(col(\"DS_REASON_1\")).like('%RECLAMA%') | upper(col(\"DS_RESULT\")).like('%RECLAMA%'))\n",
    "                        .filter(upper(col(\"DS_DIRECTION\")).isin('DE ENTRADA', 'ENTRANTE'))\n",
    "                        .filter(upper(col(\"CO_TYPE\")).isin('LLAMADA TELEFONO', '...'))\n",
    "                        .filter(col('DS_X_PHONE_CONSULTATION').isNotNull())\n",
    "                        .filter('DS_X_PHONE_CONSULTATION != \"\"')\n",
    "                        .filter('DS_X_PHONE_CONSULTATION != \"000000000\"')\n",
    "                        .join(serv_cli, interac.DS_X_PHONE_CONSULTATION==serv_cli.NUM_SERIE, 'left_outer')\n",
    "                        .filter(col('NIF_CLIENTE').isNotNull())\n",
    "                        .filter('NIF_CLIENTE != \"\"')\n",
    "                        .filter('NIF_CLIENTE != \"7\"')\n",
    "                        #.select('DS_X_PHONE_CONSULTATION', 'CO_INTERACT_ID', 'FX_START_DATE')\n",
    "                )\n",
    "    reclamaciones_nif = reclamaciones.select('NIF_CLIENTE', 'CO_INTERACT_ID').groupby('NIF_CLIENTE').agg(countDistinct('CO_INTERACT_ID').alias('NUM_RECLAMACIONES_NIF'))\n",
    "     \n",
    "    return reclamaciones_nif\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Soporte Técnico\n",
    "def soporteTecnico(spark,starting_day,closing_day):\n",
    "    \n",
    "    from pyspark.sql.functions import year, month, dayofmonth\n",
    "    from pyspark.sql.functions import lpad\n",
    "    from pyspark.sql.functions import broadcast\n",
    "\n",
    "    interac = (spark.read.table('raw_es.callcentrecalls_interactionono')\n",
    "                .withColumn('year', year('FX_CREATE_DATE'))\n",
    "                .withColumn('month', month('FX_CREATE_DATE'))\n",
    "                .withColumn('day', dayofmonth('FX_CREATE_DATE'))\n",
    "                .where( (concat(col('year'),lpad(col('month'),2, '0'),lpad(col('day'),2, '0'))<=closing_day)\n",
    "                       &(concat(col('year'),lpad(col('month'),2, '0'),lpad(col('day'),2, '0'))>=starting_day) )\n",
    "          )\n",
    "    clientes = (spark.read.table('raw_es.customerprofilecar_customerow')\n",
    "                      .select('NUM_CLIENTE', 'NIF_CLIENTE').distinct()\n",
    "                      .withColumnRenamed('NUM_CLIENTE', 'NUM_CLIENTE_cli')\n",
    "           )\n",
    "    servicios = spark.read.table('raw_es.customerprofilecar_servicesow').select('NUM_CLIENTE', 'NUM_SERIE').distinct()\n",
    "    serv_cli = servicios.join(clientes, servicios.NUM_CLIENTE==clientes.NUM_CLIENTE_cli, 'full_outer').drop('NUM_CLIENTE_cli')\n",
    "    \n",
    "    serv_cli=serv_cli.where(serv_cli.NUM_SERIE.isNotNull())\n",
    "    serv_cli=serv_cli.where(serv_cli.NUM_SERIE != '')\n",
    "    \n",
    "    maestro_gt = spark.read.csv('/data/attributes/vf_es/trigger_analysis/maestro_gt/', header=True, sep='\\t')\n",
    " \n",
    "    soporte_tmp = (interac.filter(~upper(col(\"DS_X_GROUP_WORK\")).like('%IVR%'))\n",
    "                      .filter(~upper(col(\"DS_REASON_2\")).like('%Transf%'))\n",
    "                      .filter(~upper(col(\"DS_REASON_1\")).like('%Transf%'))\n",
    "                      .join(broadcast(maestro_gt), interac.DS_X_GROUP_WORK==maestro_gt. Grupo_de_Trabajo, 'left_outer')\n",
    "                      .filter(upper(col(\"SERVICIO_AGENTE\")).isin('CUSTOMER OP.')) \n",
    "                  )\n",
    "    soporte_tmp=soporte_tmp.where(soporte_tmp.DS_X_PHONE_CONSULTATION.isNotNull())\n",
    "    soporte_tmp=soporte_tmp.where(soporte_tmp.DS_X_PHONE_CONSULTATION != '')\n",
    "        \n",
    "    soporte=(soporte_tmp.join(serv_cli, soporte_tmp.DS_X_PHONE_CONSULTATION==serv_cli.NUM_SERIE, 'left_outer')\n",
    "                      .filter(col('NIF_CLIENTE').isNotNull())\n",
    "                      .filter('NIF_CLIENTE != \"\"')\n",
    "                      .filter('NIF_CLIENTE != \"7\"')\n",
    "             )\n",
    "    \n",
    "    soporte_nif = soporte.select('NIF_CLIENTE', 'CO_INTERACT_ID').groupby('NIF_CLIENTE').agg(countDistinct('CO_INTERACT_ID').alias('NUM_SOPORTE_TECNICO_NIF'))\n",
    "\n",
    "    return soporte_nif\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_partitions (df):\n",
    "    l = df.rdd.glom().map(len).collect()  # get length of each partition\n",
    "    print 'Smallest partition {}'.format(min(l))\n",
    "    print 'Largest partitions {}'.format(max(l))\n",
    "    print 'Avg. partition size {}'.format(sum(l)/len(l))\n",
    "    print 'Num partitions {}'.format(len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_customer_base(spark, date_):\n",
    "\n",
    "    # Customer base at the beginning of the period\n",
    "\n",
    "    day_ = date_[6:8]\n",
    "    month_ = date_[4:6]\n",
    "    year_ = date_[0:4]\n",
    "\n",
    "    customerDF = spark.read.option(\"mergeSchema\", True)\\\n",
    "    .parquet(\"/data/udf/vf_es/amdocs_ids/customer/year=\" + str(year_) + \"/month=\" + str(int(month_)) + \"/day=\" + str(int(day_)))\n",
    "\n",
    "    serviceDF = spark.read.option(\"mergeSchema\", True)\\\n",
    "    .parquet(\"/data/udf/vf_es/amdocs_ids/service/year=\" + str(year_) + \"/month=\" + str(int(month_)) + \"/day=\" + str(int(day_)))\\\n",
    "    \n",
    "    customerBase = customerDF\\\n",
    "    .join(serviceDF, \"NUM_CLIENTE\", \"inner\")\\\n",
    "    .select('NIF_CLIENTE')\\\n",
    "    .dropDuplicates()\n",
    "\n",
    "    print \"[Info Trigger Identification] Number of mobile service in the base of \" + date_ + \": \" + str(customerBase.count())\n",
    "\n",
    "    return customerBase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#starting_day='20190307'\n",
    "#closing_day='20190307'\n",
    "#horizon=[2,4,8]\n",
    "\n",
    "#starting_day_list=['20190430']#['20190314','20190321','20190331','20190407','20190414','20190421','20190430']\n",
    "closing_day_list=['20190621']#,'20190607']#'20190414','20190421','20190430','20190507','20190514','20190521','20190531']\n",
    "horizon=[2,4,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for closing_day in closing_day_list:\n",
    "       \n",
    "    cont=0\n",
    "    for hh in horizon:\n",
    "        # Calculamos n ciclos hacia atrás (donde n es el horizon)\n",
    "\n",
    "        cycle = 0\n",
    "        fini_tmp = closing_day\n",
    "        while cycle < hh:\n",
    "            yearmonthday_target = get_previous_cycle(fini_tmp, str_fmt=\"%Y%m%d\")\n",
    "            cycle = cycle + 1\n",
    "            fini_tmp = yearmonthday_target\n",
    "        starting_day=fini_tmp\n",
    "\n",
    "        cycle = 0\n",
    "        fini_tmp = starting_day\n",
    "        while cycle < hh:\n",
    "            yearmonthday_target = get_previous_cycle(fini_tmp, str_fmt=\"%Y%m%d\")\n",
    "            cycle = cycle + 1\n",
    "            fini_tmp = yearmonthday_target\n",
    "        ciclo_prev=fini_tmp\n",
    "\n",
    "        print(ciclo_prev,starting_day,closing_day)\n",
    "\n",
    "        customerBase=get_customer_base(spark,closing_day)\n",
    "\n",
    "        dfAverias_ini=averias(spark,starting_day,closing_day)\n",
    "        dfAverias_ini=dfAverias_ini.withColumnRenamed('NUM_AVERIAS_NIF','NUM_AVERIAS_NIF_ini_w'+str(hh))\n",
    "\n",
    "        dfAverias_prev=averias(spark,ciclo_prev,starting_day)\n",
    "        dfAverias_prev=dfAverias_prev.withColumnRenamed('NUM_AVERIAS_NIF','NUM_AVERIAS_NIF_prev_w'+str(hh))\n",
    "\n",
    "        dfAverias=customerBase.join(dfAverias_ini,on=['NIF_CLIENTE'],how='left_outer')\n",
    "        dfAverias=dfAverias.join(dfAverias_prev,on=['NIF_CLIENTE'],how='left_outer')\n",
    "\n",
    "        dfAverias=dfAverias.na.fill(0,subset=['NUM_AVERIAS_NIF_prev_w'+str(hh),'NUM_AVERIAS_NIF_ini_w'+str(hh)])\n",
    "\n",
    "        dfDifAverias_tmp=dfAverias.withColumn('NUM_AVERIAS_NIF_w'+str(hh)+'vsw'+str(hh),col('NUM_AVERIAS_NIF_ini_w'+str(hh))-col('NUM_AVERIAS_NIF_prev_w'+str(hh)))\n",
    "\n",
    "        if cont==0: dfDifAverias=dfDifAverias_tmp\n",
    "        else: dfDifAverias=dfDifAverias.join(dfDifAverias_tmp,on='NIF_CLIENTE',how='left')\n",
    "\n",
    "        cont=cont+1\n",
    "        \n",
    "    dfDifAveriasFin=(dfDifAverias.withColumn('year',lit(int(closing_day[0:4])))\n",
    "    .withColumn('month',lit(int(closing_day[4:6])))\n",
    "    .withColumn('day',lit(int(closing_day[6:8]))))\n",
    "    \n",
    "    (dfDifAveriasFin.write.partitionBy('year', 'month', 'day').mode(\"append\").format(\"parquet\")\n",
    "            .save('/data/attributes/vf_es/trigger_analysis/averias/'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reclamaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for closing_day in closing_day_list:\n",
    "    cont=0\n",
    "    for hh in horizon:\n",
    "        # Calculamos n ciclos hacia atrás (donde n es el horizon)\n",
    "        cycle = 0\n",
    "        fini_tmp = closing_day\n",
    "        while cycle < hh:\n",
    "            yearmonthday_target = get_previous_cycle(fini_tmp, str_fmt=\"%Y%m%d\")\n",
    "            cycle = cycle + 1\n",
    "            fini_tmp = yearmonthday_target\n",
    "        starting_day=fini_tmp\n",
    "\n",
    "        cycle = 0\n",
    "        fini_tmp = starting_day\n",
    "        while cycle < hh:\n",
    "            yearmonthday_target = get_previous_cycle(fini_tmp, str_fmt=\"%Y%m%d\")\n",
    "            cycle = cycle + 1\n",
    "            fini_tmp = yearmonthday_target\n",
    "        ciclo_prev=fini_tmp\n",
    "\n",
    "        print(ciclo_prev,starting_day,closing_day) \n",
    "\n",
    "        customerBase=get_customer_base(spark,closing_day)\n",
    "\n",
    "        dfReclamaciones_ini=reclamaciones(spark,starting_day,closing_day)\n",
    "        dfReclamaciones_ini=dfReclamaciones_ini.withColumnRenamed('NUM_RECLAMACIONES_NIF','NUM_RECLAMACIONES_NIF_ini_w'+str(hh))\n",
    "\n",
    "        dfReclamaciones_prev=reclamaciones(spark,ciclo_prev,starting_day)\n",
    "        dfReclamaciones_prev=dfReclamaciones_prev.withColumnRenamed('NUM_RECLAMACIONES_NIF','NUM_RECLAMACIONES_NIF_prev_w'+str(hh))\n",
    "\n",
    "        dfReclamaciones=customerBase.join(dfReclamaciones_ini,on=['NIF_CLIENTE'],how='left_outer')\n",
    "        dfReclamaciones=dfReclamaciones.join(dfReclamaciones_prev,on=['NIF_CLIENTE'],how='left_outer')\n",
    "\n",
    "        dfReclamaciones=dfReclamaciones.na.fill(0,subset=['NUM_RECLAMACIONES_NIF_prev_w'+str(hh),'NUM_RECLAMACIONES_NIF_ini_w'+str(hh)])\n",
    "\n",
    "        dfDifReclamaciones_tmp=dfReclamaciones.withColumn('NUM_RECLAMACIONES_NIF_w'+str(hh)+'vsw'+str(hh),col('NUM_RECLAMACIONES_NIF_ini_w'+str(hh))-col('NUM_RECLAMACIONES_NIF_prev_w'+str(hh)))\n",
    "\n",
    "        if cont==0: dfDifReclamaciones=dfDifReclamaciones_tmp\n",
    "        else: dfDifReclamaciones=dfDifReclamaciones.join(dfDifReclamaciones_tmp,on='NIF_CLIENTE',how='left')\n",
    "\n",
    "        cont=cont+1\n",
    "        \n",
    "    dfDifReclamacionesFin=(dfDifReclamaciones.withColumn('year',lit(int(closing_day[0:4])))\n",
    "    .withColumn('month',lit(int(closing_day[4:6])))\n",
    "    .withColumn('day',lit(int(closing_day[6:8]))))\n",
    "    \n",
    "    (dfDifReclamacionesFin.write.partitionBy('year', 'month', 'day').mode(\"append\").format(\"parquet\")\n",
    "            .save('/data/attributes/vf_es/trigger_analysis/reclamaciones/'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soporte Técnico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for closing_day in (closing_day_list):\n",
    "\n",
    "    cont=0\n",
    "    for hh in horizon:\n",
    "\n",
    "        # Calculamos n ciclos hacia atrás (donde n es el horizon)\n",
    "        cycle = 0\n",
    "        fini_tmp = closing_day\n",
    "        while cycle < hh:\n",
    "            yearmonthday_target = get_previous_cycle(fini_tmp, str_fmt=\"%Y%m%d\")\n",
    "            cycle = cycle + 1\n",
    "            fini_tmp = yearmonthday_target\n",
    "        starting_day=fini_tmp\n",
    "\n",
    "        cycle = 0\n",
    "        fini_tmp = starting_day\n",
    "        while cycle < hh:\n",
    "            yearmonthday_target = get_previous_cycle(fini_tmp, str_fmt=\"%Y%m%d\")\n",
    "            cycle = cycle + 1\n",
    "            fini_tmp = yearmonthday_target\n",
    "        ciclo_prev=fini_tmp\n",
    "\n",
    "        print(ciclo_prev,starting_day,closing_day)\n",
    "\n",
    "        customerBase=get_customer_base(spark,closing_day)\n",
    "\n",
    "        dfSoporteTec_ini=soporteTecnico(spark,starting_day,closing_day)\n",
    "        dfSoporteTec_ini=dfSoporteTec_ini.withColumnRenamed('NUM_SOPORTE_TECNICO_NIF','NUM_SOPORTE_TECNICO_NIF_ini_w'+str(hh))\n",
    "\n",
    "        dfSoporteTec_prev=soporteTecnico(spark,ciclo_prev,starting_day)\n",
    "        dfSoporteTec_prev=dfSoporteTec_prev.withColumnRenamed('NUM_SOPORTE_TECNICO_NIF','NUM_SOPORTE_TECNICO_NIF_prev_w'+str(hh))\n",
    "\n",
    "        dfSoporteTec=customerBase.join(dfSoporteTec_ini,on=['NIF_CLIENTE'],how='left_outer')\n",
    "        dfSoporteTec=dfSoporteTec.join(dfSoporteTec_prev,on=['NIF_CLIENTE'],how='left_outer')\n",
    "\n",
    "        dfSoporteTec=dfSoporteTec.na.fill(0,subset=['NUM_SOPORTE_TECNICO_NIF_prev_w'+str(hh),'NUM_SOPORTE_TECNICO_NIF_ini_w'+str(hh)])\n",
    "\n",
    "        dfDifSoporteTec_tmp=dfSoporteTec.withColumn('NUM_SOPORTE_TECNICO_NIF_w'+str(hh)+'vsw'+str(hh),col('NUM_SOPORTE_TECNICO_NIF_ini_w'+str(hh))-col('NUM_SOPORTE_TECNICO_NIF_prev_w'+str(hh)))\n",
    "\n",
    "        if cont==0: dfDifSoporteTec=dfDifSoporteTec_tmp\n",
    "        else: dfDifSoporteTec=dfDifSoporteTec.join(dfDifSoporteTec_tmp,on='NIF_CLIENTE',how='left')\n",
    "\n",
    "        cont=cont+1\n",
    "        \n",
    "    dfDifSoporteTecFin=(dfDifSoporteTec.withColumn('year',lit(int(closing_day[0:4])))\n",
    "    .withColumn('month',lit(int(closing_day[4:6])))\n",
    "    .withColumn('day',lit(int(closing_day[6:8]))))\n",
    "    \n",
    "    \n",
    "    (dfDifSoporteTecFin.write.partitionBy('year', 'month', 'day').mode(\"append\").format(\"parquet\")\n",
    "            .save('/data/attributes/vf_es/trigger_analysis/soporte_tecnico/'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAv=spark.read.parquet('/data/attributes/vf_es/trigger_analysis/averias/year=2019/month=4/day=14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAv.filter(col('NUM_AVERIAS_NIF_ini_w8') != col('NUM_AVERIAS_NIF_ini_w2')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfAv.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
