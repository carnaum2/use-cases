{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migrator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is intended for migrate scores in specific churn tables into output model tables\n",
    "Documentation on how to insert into these tables can be found here:\n",
    "https://confluence.sp.vodafone.com/pages/viewpage.action?spaceKey=VEBDA&title=Model+Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import datetime as dt\n",
    "DEVEL_SRC = os.path.join(os.environ.get('BDA_USER_HOME', ''), \"src\", \"devel\")\n",
    "if DEVEL_SRC not in sys.path:\n",
    "    sys.path.append(DEVEL_SRC)\n",
    "\n",
    "USECASES_SRC = os.path.join(DEVEL_SRC, \"use-cases\") # TODO when - is removed, remove also this line and adapt imports\n",
    "if USECASES_SRC not in sys.path: \n",
    "    sys.path.append(USECASES_SRC)\n",
    "    \n",
    "AMDOCS_SRC = os.path.join(DEVEL_SRC, \"amdocs_informational_dataset\") # TODO when - is removed, remove also this line and adapt imports\n",
    "if AMDOCS_SRC not in sys.path: \n",
    "    sys.path.append(AMDOCS_SRC)\n",
    "    \n",
    "import pykhaos.utils.custom_logger as clogger\n",
    "logging_file = os.path.join(os.environ.get('BDA_USER_HOME', ''), \"logging\",\n",
    "                                    \"out_\" + dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".log\")\n",
    "logger = clogger.configure_logger(log_filename=logging_file, std_channel=sys.stderr, logger_name=\"\")\n",
    "logger.info(\"Logging to file {}\".format(logging_file))    \n",
    "    \n",
    "#EXTERNAL_PATH = os.path.join(os.environ.get('BDA_USER_HOME', ''), \"src\", \"devel\", \"pykhaos\", \"external_lib\")\n",
    "EXTERNAL_PATH = \"/var/SP/data/bdpmdses/churn/lib\"\n",
    "if EXTERNAL_PATH not in sys.path:\n",
    "    sys.path.append(EXTERNAL_PATH)\n",
    "\n",
    "import logging\n",
    "logging.getLogger('matplotlib').setLevel(logging.WARNING)\n",
    "\n",
    "    \n",
    "import pykhaos.utils.notebooks as nb\n",
    "\n",
    "\n",
    "RUNNING_FROM_NOTEBOOK = nb.isnotebook()\n",
    "import matplotlib.pyplot as plt\n",
    "if RUNNING_FROM_NOTEBOOK:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    %matplotlib inline  \n",
    "    \n",
    "    \n",
    "    \n",
    "#logger = my_project.logger\n",
    "\n",
    "if not RUNNING_FROM_NOTEBOOK:\n",
    "    args = my_project.arg_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "app_name = \"migrator\"\n",
    "\n",
    "import pykhaos.utils.pyspark_configuration as pyspark_config\n",
    "sc, spark, sql_context = pyspark_config.get_spark_session(app_name=app_name, log_level=\"OFF\", min_n_executors = 1, max_n_executors = 10, n_cores = 4,\n",
    "                         executor_memory = \"16g\", driver_memory=\"4g\")\n",
    "print(\"Ended spark session: {} secs | default parallelism={}\".format(time.time() - start_time,\n",
    "                                                                     sc.defaultParallelism))\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amdocs_informational_dataset.engine.call_centre_calls import CallCentreCalls\n",
    "from pyspark.sql.functions import collect_set, concat, size, coalesce, col, lpad, struct, count as sql_count, lit, min as sql_min, max as sql_max, collect_list, udf, \\\n",
    "        desc, asc, to_date, create_map, sum as sql_sum, substring, sort_array, split, month, dayofmonth\n",
    "from pyspark.sql.types import StringType, ArrayType, MapType, StructType, StructField, IntegerType, DateType\n",
    "from pyspark.sql.functions import array, regexp_extract\n",
    "from itertools import chain\n",
    "import argparse\n",
    "import csv\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "from pyspark.sql.functions import concat_ws, date_format, from_unixtime, \\\n",
    "    length, lit, lower, lpad, month, regexp_replace, translate, udf, unix_timestamp, year, when, upper\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from engine.general_functions import format_date, compute_diff_days, sum_horizontal\n",
    "from collections import Counter\n",
    "from pyspark.sql.types import StringType\n",
    "from pykhaos.utils.date_functions import get_last_day_of_month, move_date_n_days, move_date_n_cycles, move_date_n_yearmonths\n",
    "from churn.analysis.ccc_churn.engine.data_loader import get_port, get_ccc_data, get_tgs, get_all_ports\n",
    "from churn.datapreparation.general.data_loader import get_active_services\n",
    "from churn.analysis.ccc_churn.engine.reporter import compute_results, SAVING_PATH, init_writer, print_sheet\n",
    "from churn.analysis.ccc_churn.app.run_ccc_churn_analysis import join_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select closing days to migrate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the first and latest cycle to migrate\n",
    "In case of more than one prediction are found for a closing day, the latest one is selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_cycle = \"20190314\"\n",
    "start_cycle = \"20181114\"\n",
    "\n",
    "FOR_MIGRATE = []\n",
    "ss = start_cycle\n",
    "while ss<=end_cycle:\n",
    "    FOR_MIGRATE.append(ss)\n",
    "    ss = move_date_n_cycles(date_=ss, n=1)\n",
    "FOR_MIGRATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Migrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getFirstElement = udf(lambda myarray: myarray[0])\n",
    "\n",
    "df_preds = (spark\n",
    "      .read\n",
    "      .table(\"tests_es.jvmm_amdocs_automated_churn_scores\"))\n",
    "\n",
    "\n",
    "df_model = (spark\n",
    "      .read\n",
    "      .table(\"tests_es.jvmm_amdocs_churn_model_results\").drop(*[\"horizon\", \"segment\"]))\n",
    "        \n",
    "df_complete = df_preds.join(df_model, on=[\"model\"], how=\"inner\")\n",
    "\n",
    "df_complete = df_complete.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples_list = df_preds.select(\"pred_name\", \"model\", \"segment\").distinct().rdd.map(lambda row: (row[0], row[1], row[2])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_model_list = pd.DataFrame(tuples_list, columns=[\"pred_name\", \"model\", \"segment\"])\n",
    "df_model_list[\"closing_day\"] = df_model_list[\"pred_name\"].apply(lambda mm: re.match(\"(churn_preds|preds)_(mobileandfbb|onlymob)_for(201[8|9][0-9]{4}).*\",mm).group(3))\n",
    "df_model_list.sort_values(by=\"model\", ascending=True, inplace=True)\n",
    "df_model_list[\"order\"] = df_model_list.groupby([\"closing_day\", \"segment\"]).cumcount()\n",
    "df_model_list.drop_duplicates([\"closing_day\", \"segment\"], inplace=True, keep=\"last\")\n",
    "df_model_list = df_model_list[df_model_list[\"closing_day\"].isin(FOR_MIGRATE)]\n",
    "df_model_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the information for the pred_name's and mode's stored in the df_model_list dataframe and format the info to insert into the new tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL = \"prediction_tr20181214to20181214_tt20190207_horizon8_on20190214_165745\"\n",
    "#XXXX_MODEL_LIST = [MODEL]\n",
    "\n",
    "model_param_cols = [\"model_name\", \n",
    "      \"executed_at\",\n",
    "      \"model_level\",\n",
    "      \"training_closing_date\",\n",
    "      \"target\",\n",
    "      \"model_path\",\n",
    "      \"metrics_path\",\n",
    "      \"metrics_train\",\n",
    "      \"metrics_test\",\n",
    "      \"varimp\",\n",
    "      \"algorithm\",\n",
    "      \"author_login\",\n",
    "      \"extra_info\",\n",
    "      \"scores_extra_info_headers\",\n",
    "      \"year\", \"month\", \"day\", \"time\"]\n",
    "\n",
    "model_scores_cols = [\"model_name\",\n",
    "      \"executed_at\",\n",
    "      \"model_executed_at\",\n",
    "      \"predict_closing_date\",\n",
    "      \"msisdn\",\n",
    "      \"client_id\", \n",
    "      \"nif\",\n",
    "      \"model_output\",\n",
    "      \"scoring\",\n",
    "      \"prediction\",\n",
    "      \"extra_info\"]\n",
    "\n",
    "for idx, df_model in df_model_list.iterrows():\n",
    "    \n",
    "    #print(df_model)\n",
    "    MODEL = df_model[\"model\"] # e.g. prediction_tr20181207to20181207_tt20190131_horizon8_on20190206_173333\n",
    "    PRED_NAME = df_model[\"pred_name\"] # e.g. preds_mobileandfbb_for20190221_on20190227_164405\n",
    "    #print(PRED_NAME)\n",
    "    EXECUTED_AT_DATE = re.match(\"(preds_|churn_preds_).*_on([0-9]{8})_([0-9]{6})\", PRED_NAME).group(2)\n",
    "    EXECUTED_AT_TIME = re.match(\"(preds_|churn_preds_).*_on([0-9]{8})_([0-9]{6})\", PRED_NAME).group(3)\n",
    "    SEGMENT = df_model[\"segment\"]\n",
    "    MODEL_NAME_PARTITION = \"churn_preds_\" + SEGMENT\n",
    "    \n",
    "    print(MODEL, PRED_NAME, SEGMENT)\n",
    "    \n",
    "    model_param_table = (df_complete\n",
    "          .where( ((col(\"model\") == MODEL) & (col(\"pred_name\") == PRED_NAME)))\n",
    "          .dropDuplicates()\n",
    "          .withColumn(\"model_name\", concat(lit(\"churn_preds_\"), regexp_extract(col(\"pred_name\"), \"(preds_|churn_preds_)(mobileandfbb|onlymob)_for.*\",2)))      \n",
    "          .withColumn(\"executed_at\", from_unixtime(unix_timestamp( substring(col(\"pred_name\"), -15, 15), \"yyyyMMdd_HHmmss\")))\n",
    "          .withColumn(\"model_level\", lit(\"service\"))\n",
    "          .withColumn(\"training_closing_date\", regexp_extract(col(\"model\"), \"^prediction_tr([0-9]{8}to[0-9]{8}).*\", 1))\n",
    "          #.withColumn(\"target\", array(col(\"target\")))\n",
    "          .withColumnRenamed(\"path\", \"model_path\")\n",
    "          .withColumn(\"metrics_path\", lit(\"-\"))\n",
    "          .withColumn(\"metrics_train\", concat_ws(\";\", concat(lit(\"roc=\"), col(\"roc_tr\")),\n",
    "                                                      concat(lit(\"avg_score=\"), col(\"avg_score_tr\")), \n",
    "                                                      concat(lit(\"sd_score=\"), col(\"sd_score_tr\")), \n",
    "                                                      concat(lit(\"skewness_score=\"), col(\"skewness_score_tr\")), \n",
    "                                                      concat(lit(\"kurtosis_score=\"), col(\"kurtosis_score_tr\")), \n",
    "                                                      concat(lit(\"min_score=\"), col(\"min_score_tr\")), \n",
    "                                                      concat(lit(\"max_score=\"), col(\"max_score_tr\"))))\n",
    "          .withColumn(\"metrics_test\", concat_ws(\";\", concat(lit(\"roc=\"), col(\"roc_tt\")),\n",
    "                                                      concat(lit(\"avg_score=\"), col(\"avg_score_tt\")), \n",
    "                                                      concat(lit(\"sd_score=\"), col(\"sd_score_tt\")), \n",
    "                                                      concat(lit(\"skewness_score=\"), col(\"skewness_score_tt\")), \n",
    "                                                      concat(lit(\"kurtosis_score=\"), col(\"kurtosis_score_tt\")), \n",
    "                                                      concat(lit(\"min_score=\"), col(\"min_score_tt\")), \n",
    "                                                      concat(lit(\"max_score=\"), col(\"max_score_tt\"))))\n",
    "          .withColumn(\"varimp\", lit(\"-\"))\n",
    "          .withColumnRenamed(\"alg\", \"algorithm\")\n",
    "          .withColumn(\"author_login\", lit(\"jmarcoso\"))\n",
    "          .withColumn(\"extra_info\", concat_ws(\";\", \n",
    "                                              concat(lit(\"horizon=\"), col(\"horizon\")), \n",
    "                                              concat(lit(\"input_dim=\"), col(\"input_dim\"))))\n",
    "          .withColumn(\"scores_extra_info_headers\", lit(\"-\"))\n",
    "          .withColumn(\"year\", year('executed_at'))\n",
    "          .withColumn(\"month\", month('executed_at'))\n",
    "          .withColumn(\"day\", dayofmonth('executed_at'))\n",
    "          .withColumn(\"time\", regexp_replace(split(col(\"executed_at\"), \" \")[1], \":\", \"\"))\n",
    "          .select(*model_param_cols)\n",
    "          .dropDuplicates())\n",
    "    \n",
    "\n",
    "    model_output_table = (df_complete\n",
    "          .filter(col(\"model\") == MODEL)\n",
    "          .withColumnRenamed(\"pred_name\", \"model_name\")\n",
    "          .withColumn(\"model_name\", when(col(\"model_name\").rlike(\"^churn_\"), col(\"model_name\")).otherwise(concat(lit(\"churn_\"), col(\"model_name\"))))\n",
    "          .withColumn(\"executed_at\", from_unixtime(unix_timestamp( substring(col(\"model_name\"), -15, 15), \"yyyyMMdd_HHmmss\")))\n",
    "          .withColumnRenamed(\"date\", \"model_executed_at\")\n",
    "          .withColumn(\"model_executed_at\",  from_unixtime(unix_timestamp( col(\"model_executed_at\"), \"yyyyMMdd_HHmmss\"))  )\n",
    "          .withColumnRenamed(\"test\", \"predict_closing_date\")\n",
    "          .withColumnRenamed(\"num_cliente\", \"client_id\")\n",
    "          .withColumnRenamed(\"nif_cliente\", \"nif\")\n",
    "          .withColumnRenamed(\"model_score\", \"scoring\")\n",
    "          .withColumn(\"model_output\", array(col(\"scoring\")))\n",
    "          .withColumn(\"prediction\", lit(\"-\"))\n",
    "          .withColumn(\"extra_info\", lit(\"-\"))\n",
    "          .withColumn(\"year\", year('executed_at'))\n",
    "          .withColumn(\"month\", month('executed_at'))\n",
    "          .withColumn(\"day\", dayofmonth('executed_at'))\n",
    "          .withColumn(\"time\", regexp_replace(split(col(\"executed_at\"), \" \")[1], \":\", \"\"))\n",
    "          .select(*model_scores_cols))\n",
    "    \n",
    "    (model_param_table\n",
    "          .write\n",
    "          .mode(\"append\")\n",
    "          .format(\"parquet\")\n",
    "          # /data/attributes/vf_es/model_outputs/model_parameters/\n",
    "          # /user/csanc109/projects/model_outputs/model_param/\n",
    "          .save(\"/data/attributes/vf_es/model_outputs/model_parameters/model_name={}/year={}/month={}/day={}/time={}\".format(MODEL_NAME_PARTITION, \n",
    "                                                                                                                         int(EXECUTED_AT_DATE[:4]), \n",
    "                                                                                                                         int(EXECUTED_AT_DATE[4:6]), \n",
    "                                                                                                                         int(EXECUTED_AT_DATE[6:]), \n",
    "                                                                                                                         int(EXECUTED_AT_TIME))))\n",
    "    (model_output_table\n",
    "          .write\n",
    "          .mode(\"append\")\n",
    "          .format(\"parquet\")\n",
    "          #/data/attributes/vf_es/model_outputs/model_scores/\n",
    "          #/user/csanc109/projects/model_outputs/model_scores/\n",
    "          .save(\"/data/attributes/vf_es/model_outputs/model_scores/model_name={}/year={}/month={}/day={}/time={}\".format(MODEL_NAME_PARTITION, \n",
    "                                                                                                                         int(EXECUTED_AT_DATE[:4]), \n",
    "                                                                                                                         int(EXECUTED_AT_DATE[4:6]), \n",
    "                                                                                                                         int(EXECUTED_AT_DATE[6:]), \n",
    "                                                                                                                         int(EXECUTED_AT_TIME))))\n",
    "print(\"[Info] Output model migration completed!!!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "df_onlymob = (spark.read.load(\"/data/attributes/vf_es/model_outputs/model_scores/model_name=churn_preds_onlymob\")\n",
    "              .select(\"executed_at\", \"predict_closing_date\").groupby(\"executed_at\", \"predict_closing_date\").agg(sql_count(\"*\").alias(\"onlymob\")))\n",
    "df_onlymob_params = spark.read.load(\"/data/attributes/vf_es/model_outputs/model_parameters/model_name=churn_preds_onlymob\").select(\"executed_at\", \"training_closing_date\")\n",
    "df_onlymob = df_onlymob.join(df_onlymob_params, on=[\"executed_at\"], how=\"outer\")\n",
    "df_onlymob = df_onlymob.select(\"training_closing_date\", \"predict_closing_date\")\n",
    "\n",
    "\n",
    "df_mobandfbb = (spark.read.load(\"/data/attributes/vf_es/model_outputs/model_scores/model_name=churn_preds_mobileandfbb\")\n",
    "                .select(\"executed_at\", \"predict_closing_date\").groupby(\"executed_at\", \"predict_closing_date\").agg(sql_count(\"*\").alias(\"mobileandfbb\")))\n",
    "\n",
    "df_mobandfbb_params = spark.read.load(\"/data/attributes/vf_es/model_outputs/model_parameters/model_name=churn_preds_mobileandfbb\").select(\"executed_at\", \"training_closing_date\")\n",
    "df_mobandfbb = df_mobandfbb.join(df_mobandfbb_params, on=[\"executed_at\"], how=\"outer\")\n",
    "df_mobandfbb = df_mobandfbb.select(\"training_closing_date\", \"predict_closing_date\")\n",
    "\n",
    "\n",
    "df_others = (spark.read.load(\"/data/attributes/vf_es/model_outputs/model_scores/model_name=churn_preds_others\")\n",
    "              .select(\"executed_at\", \"predict_closing_date\").groupby(\"executed_at\", \"predict_closing_date\").agg(sql_count(\"*\").alias(\"others\")))\n",
    "\n",
    "             \n",
    "df_others_params = spark.read.load(\"/data/attributes/vf_es/model_outputs/model_parameters/model_name=churn_preds_others\").select(\"executed_at\", \"training_closing_date\")\n",
    "df_others = df_others.join(df_others_params, on=[\"executed_at\"], how=\"outer\")\n",
    "df_others = df_others.select(\"training_closing_date\", \"predict_closing_date\")\n",
    "\n",
    "\n",
    "df_fbb = (spark.read.load(\"/data/attributes/vf_es/model_outputs/model_scores/model_name=churn_preds_fbb\")\n",
    "            .select(\"executed_at\", \"predict_closing_date\").groupby(\"executed_at\", \"predict_closing_date\").agg(sql_count(\"*\").alias(\"fbb\")))\n",
    "df_fbb_params = spark.read.load(\"/data/attributes/vf_es/model_outputs/model_parameters/model_name=churn_preds_fbb\").select(\"executed_at\", \"training_closing_date\")\n",
    "df_fbb = df_fbb.join(df_fbb_params, on=[\"executed_at\"], how=\"outer\")\n",
    "df_fbb = df_fbb.select(\"training_closing_date\", \"predict_closing_date\")\n",
    "\n",
    "\n",
    "df_all = df_onlymob.join(df_mobandfbb, on=[\"predict_closing_date\", \"training_closing_date\"], how=\"outer\")\n",
    "df_all = df_all.join(df_others, on=[\"predict_closing_date\", \"training_closing_date\"], how=\"outer\")\n",
    "df_all = df_all.join(df_fbb, on=[\"predict_closing_date\", \"training_closing_date\"], how=\"outer\")\n",
    "\n",
    "\n",
    "df_all.sort(desc(\"predict_closing_date\"), desc(\"training_closing_date\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closing_day = \"20191022\"\n",
    "churn_window=15\n",
    "\n",
    "from churn_nrt.src.data.sopos_dxs import FixPort\n",
    "from churn_nrt.src.data.customer_base import CustomerBase\n",
    "\n",
    "# Getting portout requests for fix and mobile services, and disconnections of fbb services\n",
    "print(\"******* Asking for FixPort...\")\n",
    "df_sopo_fix = FixPort(spark).get_module(closing_day, save=False, churn_window=churn_window)\n",
    "\n",
    "# The base of active services on closing_day\n",
    "from churn.analysis.triggers.base_utils.base_utils import get_mobile_portout_requests, get_customer_base\n",
    "base_df = get_customer_base(spark, closing_day).select('msisdn', \"nif_cliente\", \"rgu\")\n",
    "\n",
    "df_sopos = (base_df.join(df_sopo_fix, ['msisdn'], \"left\").na.fill({'label_srv': 0.0}))\n",
    "\n",
    "df_sopos.where(col(\"label_srv\")==1.0).groupby(\"rgu\").agg(sql_count(\"*\").alias(\"count\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sopos.where(col(\"label_srv\")==1.0).groupby(\"rgu\").agg(sql_count(\"*\").alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sopos.where(col(\"label_srv\")==1.0).select(\"msisdn\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.read.load(\"/data/udf/vf_es/churn/triggers/nav_comp_tests_all_labels/year=2019/month=10/day=30/time=1574941173\").groupby(\"rgu\").agg(sql_count(\"*\").alias(\"count\")).show()\n",
    "\n",
    "\n",
    "\n",
    "spark.read.load(\"/data/udf/vf_es/churn/triggers/navcomp_msisdn_data/year=2019/month=11/day=14/\").groupby(\"segment_nif\").agg(sql_count(\"*\").alias(\"count\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.load(\"/data/udf/vf_es/churn/triggers/navcomp_msisdn_data/year=2019/month=11/day=28/\").where(col(\"segment_nif\")==\"Standalone_FBB\").select(\"msisdn\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_customer_master(spark, \"20191128\", unlabeled=True)\n",
    "\n",
    "\n",
    "#nifs_standalone_fbb = df.where(col(\"segment_nif\")==\"Standalone_FBB\").sort(desc(\"nif_cliente\")).select(\"nif_cliente\", \"msisdn\", \"rgu\", \"segment_nif\", \"rgus_list\").select(\"nif_cliente\").rdd.map(lambda x: x[0]).collect()\n",
    "\n",
    "df.where(col(\"segment_nif\")==\"Standalone_FBB\").where(col(\"rgu\")==\"mobile\").select(\"nif_cliente\", \"msisdn\", \"rgu\", \"segment_nif\", \"rgus_list\").show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from churn.analysis.triggers.orders.customer_master import get_customer_master\n",
    "\n",
    "\n",
    "\n",
    "get_customer_master(spark, \"20191114\", unlabeled=True).where(col(\"msisdn\").isin([\"638079600\",\"649160250\", \"612735706\"])).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_tr_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from churn.analysis.triggers.orders.customer_master import get_segment_nif_anyday\n",
    "\n",
    "df_segment_nif_anyday = get_segment_nif_anyday(spark, date_).where(col(\"nif_cliente\").isin([nif_cliente_raros]))  \n",
    "df_segment_nif_anyday.select(\"nif_cliente\", \"segment_nif\", \"nb_rgus\", \"rgus_list\").show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from churn.analysis.triggers.orders.customer_master import get_customer_master\n",
    "df_customer_master_module = get_customer_master(spark, date_, unlabeled=True).where(col(\"nif_cliente\").isin(nif_error))  \n",
    "df_customer_master_module.select(\"nif_cliente\", \"segment_nif\", \"nb_rgus\", \"rgus_list\").show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from churn.analysis.triggers.orders.customer_master import get_customer_master_module\n",
    "df_customer_master_module = get_customer_master_module(spark, date_, unlabeled=True, save=False).where(col(\"nif_cliente\").isin(nif_error))  \n",
    "df_customer_master_module.select(\"nif_cliente\", \"segment_nif\", \"nb_rgus\", \"rgus_list\").show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASE COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from churn_nrt.src.data.customer_base import CustomerBase\n",
    "df_nrt_cust_base = CustomerBase(spark).get_module(\"20190930\", save=False).where(col(\"rgu\")==\"mobile\")\n",
    "df_nrt_cust_base.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_serv = spark.read.load(\"/data/udf/vf_es/amdocs_ids/service/year=2019/month=9/day=30\")\n",
    "[col_ for col_ in df_serv.columns if \"bas\" in col_.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_serv.where(col(\"rgu\")==\"mobile\").groupby(\"SRV_BASIC\").agg(sql_count(\"*\").alias(\"count\")).sort(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bd_anon = spark.read.load(\"/data/attributes/vf_es/model_outputs/model_scores/model_name=mobile_base/year=2019/month=9/day=30\") #\"/user/csanc109/data/mobile_base_20190930_parquet/\")\n",
    "df_bd_anon.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bd = spark.read.option(\"delimeter\", \"|\").option(\"header\",False).csv(\"/user/csanc109/data/mobile_base_20190930_desanonim/Mobile_Base-20190930.csv\")\n",
    "df_bd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "headers = ['executed_at',\n",
    " 'model_executed_at',\n",
    " 'predict_closing_date',\n",
    "        'msisdn',   \n",
    "        'client_id'  , \n",
    "           'nif',\n",
    " 'model_output',\n",
    " 'scoring',\n",
    " 'prediction',\n",
    " 'extra_info',\n",
    " 'time',\n",
    " \n",
    " \n",
    " ]\n",
    "df_bd=df_bd.toDF(*headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring_index, posexplode, split\n",
    "EXTRA_INFO_COLS = ['srv_basic',\n",
    " 'rgu',\n",
    " 'CLASE_CLI_COD_CLASE_CLIENTE',\n",
    " 'COD_ESTADO_GENERAL',\n",
    " 'TARIFF']\n",
    "\n",
    "for ii, col_ in enumerate(EXTRA_INFO_COLS):\n",
    "    df_bd = df_bd.withColumn(col_, split(\"extra_info\", \";\")[ii])\n",
    "\n",
    "\n",
    "#df_bd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insi.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insi.coalesce(1).write.mode('overwrite').format('csv').option('sep', '|').option('header', 'true').save(\"/user/csanc109/data/mobile_base_20190930_insigths_campos/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insi.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insi = spark.read.option(\"delimiter\", \"|\").option(\"header\",True).csv(\"/user/csanc109/data/mobile_base_20190930_insigths/mobile_base_20190930_insights.csv\")\n",
    "# for col_ in df_insi.columns:\n",
    "#     if col_ == \"MSISDN\":\n",
    "#         df_insi = df_insi.withColumnRenamed(col_, col_.lower())\n",
    "#     else:\n",
    "#         df_insi = df_insi.withColumnRenamed(col_, col_.lower()+\"_insi\")\n",
    "        \n",
    "# df_insi.count()\n",
    "df_insi = df_insi.withColumn(\"campo1\", col(\"msisdn\"))\n",
    "df_insi = df_insi.withColumn(\"campo2\", col(\"num_cliente\"))\n",
    "df_insi = df_insi.withColumn(\"campo3\", col(\"nif_cliente\"))\n",
    "df_insi.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insi.count() - df_bd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En BD y no en Insi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_cross = df_insi.join(df_bd, ['msisdn'], 'right').where(df_insi['msisdn'].isNull())\n",
    "df_labels_cross = df_labels_cross.cache()\n",
    "print(\"volumen incremental\", df_labels_cross.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_cross.select(\"msisdn\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En Insi y no en BD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_cross2 = df_bd.join(df_insi, ['msisdn'], 'right').where(df_bd['msisdn'].isNull())\n",
    "df_labels_cross2 = df_labels_cross2.cache()\n",
    "print(\"volumen incremental\", df_labels_cross2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_cross3 = df_insi.join(df_bd, ['msisdn'], 'inner')\n",
    "df_labels_cross3 = df_labels_cross3.cache()\n",
    "print(\"volumen comun\", df_labels_cross3.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_cross.groupby(\"CLASE_CLI_COD_CLASE_CLIENTE\", 'COD_ESTADO_GENERAL').agg(sql_count(\"*\").alias(\"count\")).sort(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_cross2.groupby('serv_basico_insi','tarifa_insi',).agg(sql_count(\"*\").alias(\"count\")).sort(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# navcomp exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pasoprevio = spark.read.load(\"/data/udf/vf_es/netscout/dailyMSISDNApplicationName/year=2019/month=4/day=7\")\n",
    "df_pasoprevio.columns\n",
    "df_pasoprevio2 = spark.read.load(\"/data/udf/vf_es/netscout/dailyMSISDNApplicationName/year=2019/month=10/day=7\")\n",
    "df_pasoprevio2.columns\n",
    "\n",
    "set(df_pasoprevio.columns) ^ set(df_pasoprevio2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_limit200 = spark.read.load(\"/data/attributes/vf_es/return_feed/data_navigation/year=2019/month=10/day=7\").limit(200)\n",
    "msisdns = list(set(df_data_limit200.select(\"subscriber_msisdn\").rdd.map(lambda x: x['subscriber_msisdn']).collect()))\n",
    "#msisdns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apps_names = list(set(spark.read.load(\"/data/attributes/vf_es/return_feed/data_navigation/year=2019/month=10/day=7\").select(\"application_name\").rdd.map(lambda x: x['application_name']).collect()))\n",
    "apps_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(spark.read.load(\"/data/udf/vf_es/netscout/dailyMSISDNApplicationName/year=2019/month=10/day=7\")\n",
    " .where(col(\"subscriber_msisdn\").isin([\"34683771231\", \"34695709335\"]))\n",
    " .where( (col(\"SUM_userplane_upload_bytes_count\") + col(\"SUM_userplane_download_bytes_count\"))> 524288)\n",
    " .where(col(\"application_name\").isin(apps_names)).sort(desc(\"subscriber_msisdn\")).show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(spark.read.load(\"/data/attributes/vf_es/return_feed/data_navigation/year=2019/month=10/day=7\")\n",
    " .where(col(\"subscriber_msisdn\").isin([\"34683771231\", \"34695709335\"]))\n",
    " .where(col(\"application_name\").isin(apps_names)).sort(desc(\"subscriber_msisdn\")).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.load(\"/data/attributes/vf_es/return_feed/data_navigation/year=2019/month=10/day=7\").where(col(\"subscriber_msisdn\").isin([\"34683771231\", \"34695709335\"])).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operators = [\"JAZZTEL\", \"LOWI\", \"MASMOVIL\", \"MOVISTAR\", \"O2\", \"ORANGE\", \"PEPEPHONE\", \"VODAFONE\", \"YOIGO\"]\n",
    "apps_names = [\"WEB_\"+op+\"_\"+p for op in operators for p in [\"HTTP\", \"HTTPS\"]]\n",
    "apps_names\n",
    "\n",
    "\n",
    "# apps_names = [u'WEB_O2_HTTPS',\n",
    "#  u'WEB_MOVISTAR_HTTPS',\n",
    "#  u'WEB_VODAFONE_HTTPS',\n",
    "#  u'WEB_YOIGO_HTTP',\n",
    "#  u'WEB_LOWI_HTTPS',\n",
    "#  u'WEB_ORANGE_HTTP',\n",
    "#  u'WEB_JAZZTEL_HTTP',\n",
    "#  u'WEB_MASMOVIL_HTTP',\n",
    "#  u'WEB_VODAFONE_HTTP',\n",
    "#  u'WEB_MOVISTAR_HTTP',\n",
    "#  u'WEB_ORANGE_HTTPS',\n",
    "#  u'WEB_PEPEPHONE_HTTPS',\n",
    "#  u'WEB_PEPEPHONE_HTTP',\n",
    "#  u'WEB_JAZZTEL_HTTPS',\n",
    "#  u'WEB_LOWI_HTTP',\n",
    "#  u'WEB_MASMOVIL_HTTPS',\n",
    "#  u'WEB_YOIGO_HTTPS',\n",
    "#  u'WEB_O2_HTTP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(spark.read.load(\"/data/udf/vf_es/netscout/dailyMSISDNApplicationName/year=2019/month=10/day=17\").where(col(\"application_name\").isin(apps_names)).where(col(\"subscriber_msisdn\").isNotNull())\n",
    "           .withColumn(\"data\",col(\"SUM_userplane_upload_bytes_count\")+col(\"SUM_userplane_download_bytes_count\"))\n",
    "           .where(col(\"data\")>lit(524288))\n",
    "           .count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(spark.read.load(\"/data/udf/vf_es/netscout/dailyMSISDNApplicationName/year=2019/month=10/day=17\").where(col(\"application_name\").isin(apps_names))\n",
    "           .withColumn(\"data\",col(\"SUM_userplane_upload_bytes_count\")+col(\"SUM_userplane_download_bytes_count\"))\n",
    "           .where(col(\"data\")>lit(524288)).where(col(\"subscriber_msisdn\").isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(spark.read.load(\"/data/attributes/vf_es/return_feed/data_navigation/year=2019/month=10/day=17\")\n",
    "           .where(col(\"application_name\").isin(apps_names)).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(msisdns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.load(\"/data/udf/vf_es/netscout/dailyMSISDNApplicationName/year=2019/month=10/day=7\").where( (col(\"application_name\").isin(apps_names)) & (col(\"subscriber_msisdn\").isNotNull())).withColumn(\"data\",col(\"SUM_userplane_upload_bytes_count\")+col(\"SUM_userplane_download_bytes_count\")).where(col(\"data\")>lit(524288)).agg(sql_sum(\"data\")).show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.load(\"/data/attributes/vf_es/return_feed/data_navigation/year=2019/month=10/day=7\").withColumn(\"data\",col(\"SUM_userplane_upload_bytes_count\")+col(\"SUM_userplane_download_bytes_count\")).agg(sql_sum(\"data\")).show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from churn.analysis.triggers.navcomp.navcomp_utils import get_navcomp_attributes\n",
    "\n",
    "starting_date = \"20191001\"\n",
    "process_date = \"20191015\"\n",
    "\n",
    "df_orig = get_navcomp_attributes(spark, starting_date, process_date, level=\"msisdn\", suffix=\"\", orig_path=True)\n",
    "df_new = get_navcomp_attributes(spark, starting_date, process_date, level=\"msisdn\", suffix=\"\", orig_path=False)\n",
    "\n",
    "\n",
    "msisdns = [u'34602579459',\n",
    " u'34699265865',\n",
    " u'34637118046',\n",
    " u'34643541635',\n",
    " u'34667016219',\n",
    " u'34603522398',\n",
    " u'34612028463',\n",
    " u'34602172915',\n",
    " u'34638122635',\n",
    " u'34612661085',\n",
    " u'34621389630',\n",
    " u'34626735798',\n",
    " u'34683771231',\n",
    " u'34664804027',\n",
    " u'34612610069',\n",
    " u'34639483976',\n",
    " u'34652601172',\n",
    " u'34609921178',\n",
    " u'34661956702',\n",
    " u'882393230430821',\n",
    " u'34639513800',\n",
    " u'34603357360',\n",
    " u'34652976638',\n",
    " u'34653975428',\n",
    " u'34612139344',\n",
    " u'34657565248',\n",
    " u'34656504103',\n",
    " u'34615956180',\n",
    " u'34608183564',\n",
    " u'34658555004',\n",
    " u'34650548809',\n",
    " u'34603997436',\n",
    " u'34694917249',\n",
    " u'34655457739',\n",
    " u'34679067766',\n",
    " u'34655551506',\n",
    " u'4534296369',\n",
    " u'34688510057',\n",
    " u'34692843710',\n",
    " u'34615977864',\n",
    " u'34694252365',\n",
    " u'34750363217',\n",
    " u'34621848425',\n",
    " u'34647631129',\n",
    " u'34661980383',\n",
    " u'34698409737',\n",
    " u'34655950914',\n",
    " u'34670819047',\n",
    " u'34602342056',\n",
    " u'34672405851',\n",
    " u'34697093354',\n",
    " u'34676843005',\n",
    " u'34607755410',\n",
    " u'34643746223',\n",
    " u'34624075045',\n",
    " u'34664164957',\n",
    " u'34640055967',\n",
    " u'34699436991',\n",
    " u'34615040780',\n",
    " u'34649505208',\n",
    " u'34630228529',\n",
    " u'34641787568',\n",
    " u'34695261294',\n",
    " u'34650852676',\n",
    " u'34651786844',\n",
    " u'34615406433',\n",
    " u'34661414103',\n",
    " u'34635496504',\n",
    " u'34650886571',\n",
    " u'34684878681',\n",
    " u'34679830577',\n",
    " u'34624664265',\n",
    " u'34686333247',\n",
    " u'34619574709',\n",
    " u'34661626345',\n",
    " u'34698930485',\n",
    " u'34643290334',\n",
    " u'34607224467',\n",
    " u'34611273565',\n",
    " u'34659755182',\n",
    " u'34626409295',\n",
    " u'34625087446',\n",
    " u'34697793869',\n",
    " u'34620566839',\n",
    " u'34647541941',\n",
    " u'34605096525',\n",
    " u'34650504179',\n",
    " u'34625289088',\n",
    " u'34683210998',\n",
    " u'34656319159',\n",
    " u'34630720603',\n",
    " u'447741382752',\n",
    " u'34683152899',\n",
    " u'34602929199',\n",
    " u'34692822948',\n",
    " u'34622813172',\n",
    " u'34670572823',\n",
    " u'34789958445',\n",
    " u'34688965869',\n",
    " u'34680347447',\n",
    " u'34664508482',\n",
    " u'491789031376',\n",
    " u'34609848759',\n",
    " u'34627904079',\n",
    " u'34655266152',\n",
    " u'34610455798',\n",
    " u'34695261579',\n",
    " u'34682884563',\n",
    " u'34666763747',\n",
    " u'34604753014',\n",
    " u'34682473201',\n",
    " u'34631727443',\n",
    " u'34610441363',\n",
    " u'882396873366857',\n",
    " u'34636544644',\n",
    " u'34679518709',\n",
    " u'34649438836',\n",
    " u'34654761792',\n",
    " u'34640763250',\n",
    " u'34693958858',\n",
    " u'34634959019',\n",
    " u'34642901478',\n",
    " u'34606813261',\n",
    " u'34635773177',\n",
    " u'34667060855',\n",
    " u'34628330469',\n",
    " u'34627906966',\n",
    " u'34609348547',\n",
    " u'34699593370',\n",
    " u'34601186557',\n",
    " u'34684669501',\n",
    " u'34693123558',\n",
    " u'34653037531',\n",
    " u'34679250802',\n",
    " u'34688200353',\n",
    " u'34611425181',\n",
    " u'34696064244',\n",
    " u'34676455684',\n",
    " u'34693045687',\n",
    " u'34683627907',\n",
    " u'34645462158',\n",
    " u'34601143234',\n",
    " u'34648188487',\n",
    " u'34629999175',\n",
    " u'34625299080',\n",
    " u'34693494584',\n",
    " u'491795499178',\n",
    " u'34692124140',\n",
    " u'34683818952',\n",
    " u'34600503065',\n",
    " u'34611822026',\n",
    " u'34613046218',\n",
    " u'34699581645',\n",
    " u'34664571843',\n",
    " u'34662072310',\n",
    " u'34656348015',\n",
    " u'34616934022',\n",
    " u'34636099471',\n",
    " u'34666271584',\n",
    " u'34670942078',\n",
    " u'34670472204',\n",
    " u'34663565093',\n",
    " u'34667039830',\n",
    " u'34688385443',\n",
    " u'34640854236',\n",
    " u'34609934152',\n",
    " u'34628964902',\n",
    " u'34620534832',\n",
    " u'34619282211',\n",
    " u'34640265051',\n",
    " u'34632300147',\n",
    " u'34691150155',\n",
    " u'34681535231',\n",
    " u'34693190094',\n",
    " u'34644285982',\n",
    " u'34620270626',\n",
    " u'34654243256',\n",
    " u'34662901998',\n",
    " u'34600732856',\n",
    " u'34656334691',\n",
    " u'34658894953',\n",
    " u'34610703900',\n",
    " u'34601426823',\n",
    " u'34601761332',\n",
    " u'34675069520',\n",
    " u'34654207810',\n",
    " u'34694845333',\n",
    " u'34613742893',\n",
    " u'34600038285',\n",
    " u'34606950109',\n",
    " u'34624679794',\n",
    " u'34637879188',\n",
    " u'34689218659',\n",
    " u'34643243869',\n",
    " u'34601168324',\n",
    " u'34669574220',\n",
    " u'34648207731',\n",
    " u'34613234875',\n",
    " u'34683590410',\n",
    " u'34634399909']\n",
    "\n",
    "\n",
    "msisdns = [re.sub(r'^34', '', mm) for mm in msisdns]\n",
    "\n",
    "df_orig = df_orig.where(col(\"msisdn\").isin(msisdns))\n",
    "df_new = df_new.where(col(\"msisdn\").isin(msisdns))\n",
    "\n",
    "df_orig = df_orig.show(200, truncate=False)\n",
    "df_new = df_new.show(200, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_service_param.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_table.count(), df_table.distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_service_param.count(), data_service_param.distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.load(\"/data/raw/vf_es/customerprofilecar/WEBSERVICES/1.0/parquet\").where(col(\"COD_SERVICIO\")==\"MRPD1\").show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOC_RT_PATH = '/data/udf/vf_es/ref_tables/amdocs_ids/'\n",
    "LOC_RT_PARAM_OW_SERVICES = LOC_RT_PATH + 'PARAM_OW_SERVICES.TXT'\n",
    "data_service_param =  spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true',delimiter='\\t').load(LOC_RT_PARAM_OW_SERVICES).select(\"RGU\", \"COD_SERVICIO\", \"TIPO\").distinct()\n",
    "df_table = spark.read.load(\"/data/raw/vf_es/customerprofilecar/WEBSERVICES/1.0/parquet\").select(\"COD_SERVICIO\", \"RGU\", \"AMBITO\").withColumnRenamed(\"RGU\", \"RGU_table\").distinct()\n",
    "df_table = df_table.withColumn(\"RGU_table\", when(col(\"RGU_table\")==\"F - Internet (BA)\", \"fbb\").when(col(\"RGU_table\")==\"F - Televisión (TV)\", \"tv\").when(col(\"RGU_table\")==\"M - Móvil (MV)\", \"mobile\").otherwise(col(\"RGU_table\")))\n",
    "\n",
    "df_all = df_table.join(data_service_param, on=[\"COD_SERVICIO\"], how=\"outer\")\n",
    "\n",
    "df_all.show(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from churn.analysis.triggers.navcomp.navcomp_utils import get_customer_base_navcomp\n",
    "date_  = \"20191015\"\n",
    "df_navcomp = get_customer_base_navcomp(spark, date_, verbose=False)\n",
    "\n",
    "from churn_nrt.src.data.customer_base import CustomerBase\n",
    "df_base_msisdn = CustomerBase(spark).get_module(date_).filter(col('rgu') == 'mobile')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_navcomp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_msisdn.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_msisdn.filter((col('segment_nif').isin(\"Other\", \"Convergent\", \"Mobile_only\")) & (col(\"segment_nif\").isNotNull())).drop_duplicates([\"nif_cliente\"]).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_msisdn = df_base_msisdn.drop_duplicates([\"msisdn\"])\n",
    "\n",
    "df_base_msisdn = df_base_msisdn.drop_duplicates(['msisdn', 'nif_cliente', 'num_cliente'])\n",
    "df_base_msisdn.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_navcomp = df_navcomp.drop_duplicates([\"msisdn\"])\n",
    "\n",
    "df_navcomp = df_navcomp.drop_duplicates(['msisdn', 'nif_cliente'])\n",
    "df_navcomp.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
