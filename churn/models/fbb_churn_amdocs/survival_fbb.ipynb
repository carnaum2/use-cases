{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added '/var/SP/data/home/asaezco/src/devel2/use-cases' to path\n",
      "Added '/var/SP/data/home/asaezco/src/devel2' to path\n"
     ]
    }
   ],
   "source": [
    "def set_paths():\n",
    "    '''\n",
    "    Deployment should be something like \"dirs/dir1/use-cases\"\n",
    "    This function adds to the path \"dirs/dir1/use-cases\" and \"dirs/dir1/\"\n",
    "    :return:\n",
    "    '''\n",
    "    import imp\n",
    "    from os.path import dirname\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    USE_CASES = \"/var/SP/data/home/asaezco/src/devel2/use-cases\"#dirname(os.path.abspath(imp.find_module('churn')[1]))\n",
    "    sys.path.append(\"/var/SP/data/home/asaezco/src/devel2/pykhaos\")\n",
    "    if USE_CASES not in sys.path:\n",
    "        sys.path.append(USE_CASES)\n",
    "        print(\"Added '{}' to path\".format(USE_CASES))\n",
    "\n",
    "    # if deployment is correct, this path should be the one that contains \"use-cases\", \"pykhaos\", ...\n",
    "    # FIXME another way of doing it more general?\n",
    "    DEVEL_SRC = os.path.dirname(USE_CASES)  # dir before use-cases dir\n",
    "    if DEVEL_SRC not in sys.path:\n",
    "        sys.path.append(DEVEL_SRC)\n",
    "        print(\"Added '{}' to path\".format(DEVEL_SRC))\n",
    "set_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "\n",
    "import sys\n",
    "\n",
    "from common.src.main.python.utils.hdfs_generic import *\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pyspark.sql.functions import (udf,\n",
    "                                    col,\n",
    "                                    decode,\n",
    "                                    when,\n",
    "                                    lit,\n",
    "                                    lower,\n",
    "                                    concat,\n",
    "                                    translate,\n",
    "                                    count,\n",
    "                                    sum as sql_sum,\n",
    "                                    max as sql_max,\n",
    "                                    min as sql_min,\n",
    "                                    avg as sql_avg,\n",
    "                                    greatest,\n",
    "                                    least,\n",
    "                                    isnull,\n",
    "                                    isnan,\n",
    "                                    struct, \n",
    "                                    substring,\n",
    "                                    size,\n",
    "                                    length,\n",
    "                                    year,\n",
    "                                    month,\n",
    "                                    dayofmonth,\n",
    "                                    unix_timestamp,\n",
    "                                    date_format,\n",
    "                                    from_unixtime,\n",
    "                                    datediff,\n",
    "                                    to_date, \n",
    "                                    desc,\n",
    "                                    asc,\n",
    "                                    countDistinct,\n",
    "                                    row_number,\n",
    "                                    skewness,\n",
    "                                    kurtosis,\n",
    "                                    concat_ws)\n",
    "\n",
    "from pyspark.sql import Row, DataFrame, Column, Window\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType, DateType, ArrayType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler, SQLTransformer, OneHotEncoder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from datetime import datetime\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from utils_general import *\n",
    "from pykhaos.utils.date_functions import convert_to_date\n",
    "from utils_model import *\n",
    "from utils_fbb_churn import *\n",
    "from metadata_fbb_churn import *\n",
    "from feature_selection_utils import *\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para que no haga falta recargar el notebook si hacemos un cambio en alguna de las funciones que estamos importando\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set BDP parameters\n",
    "def setting_bdp(min_n_executors = 3, max_n_executors = 15, n_cores = 8, executor_memory = \"12g\", driver_memory=\"8g\",\n",
    "                   app_name = \"Python app\", driver_overhead=\"1g\", executor_overhead='3g'):\n",
    "\n",
    "    MAX_N_EXECUTORS = max_n_executors\n",
    "    MIN_N_EXECUTORS = min_n_executors\n",
    "    N_CORES_EXECUTOR = n_cores\n",
    "    EXECUTOR_IDLE_MAX_TIME = 120\n",
    "    EXECUTOR_MEMORY = executor_memory\n",
    "    DRIVER_MEMORY = driver_memory\n",
    "    N_CORES_DRIVER = 1\n",
    "    MEMORY_OVERHEAD = N_CORES_EXECUTOR * 2048\n",
    "    QUEUE = \"root.BDPtenants.es.medium\"\n",
    "    BDA_CORE_VERSION = \"1.0.0\"\n",
    "\n",
    "    SPARK_COMMON_OPTS = os.environ.get('SPARK_COMMON_OPTS', '')\n",
    "    SPARK_COMMON_OPTS += \" --executor-memory %s --driver-memory %s\" % (EXECUTOR_MEMORY, DRIVER_MEMORY)\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.shuffle.manager=tungsten-sort\"\n",
    "    SPARK_COMMON_OPTS += \"  --queue %s\" % QUEUE\n",
    "\n",
    "    # Dynamic allocation configuration\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.dynamicAllocation.enabled=true\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.shuffle.service.enabled=true\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.dynamicAllocation.maxExecutors=%s\" % (MAX_N_EXECUTORS)\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.dynamicAllocation.minExecutors=%s\" % (MIN_N_EXECUTORS)\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.executor.cores=%s\" % (N_CORES_EXECUTOR)\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.dynamicAllocation.executorIdleTimeout=%s\" % (EXECUTOR_IDLE_MAX_TIME)\n",
    "    # SPARK_COMMON_OPTS += \" --conf spark.ui.port=58235\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.port.maxRetries=100\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.app.name='%s'\" % (app_name)\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.submit.deployMode=client\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.ui.showConsoleProgress=true\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.sql.broadcastTimeout=1200\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.yarn.executor.memoryOverhead={}\".format(executor_overhead)\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.yarn.executor.driverOverhead={}\".format(driver_overhead)\n",
    "\n",
    "    BDA_ENV = os.environ.get('BDA_USER_HOME', '')\n",
    "\n",
    "    # Attach bda-core-ra codebase\n",
    "    SPARK_COMMON_OPTS+=\" --files {}/scripts/properties/red_agent/nodes.properties,{}/scripts/properties/red_agent/nodes-de.properties,{}/scripts/properties/red_agent/nodes-es.properties,{}/scripts/properties/red_agent/nodes-ie.properties,{}/scripts/properties/red_agent/nodes-it.properties,{}/scripts/properties/red_agent/nodes-pt.properties,{}/scripts/properties/red_agent/nodes-uk.properties\".format(*[BDA_ENV]*7)\n",
    "\n",
    "    os.environ[\"SPARK_COMMON_OPTS\"] = SPARK_COMMON_OPTS\n",
    "    os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"%s pyspark-shell \" % SPARK_COMMON_OPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_session(app_name=\"default name\", log_level='INFO', min_n_executors = 3, max_n_executors = 15, n_cores = 4, executor_memory = \"12g\", driver_memory=\"8g\"):\n",
    "    HOME_SRC = os.path.join(os.environ.get('BDA_USER_HOME', ''), \"src\")\n",
    "    if HOME_SRC not in sys.path:\n",
    "        sys.path.append(HOME_SRC)\n",
    "\n",
    "\n",
    "    setting_bdp(app_name=app_name, min_n_executors = min_n_executors, max_n_executors = max_n_executors, n_cores = n_cores, executor_memory = executor_memory, driver_memory=driver_memory)\n",
    "    from common.src.main.python.utils.hdfs_generic import run_sc\n",
    "    sc, spark, sql_context = run_sc(log_level=log_level)\n",
    "\n",
    "\n",
    "    return sc, spark, sql_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(app_name, min_n_executors = 3, max_n_executors = 15, n_cores = 4, executor_memory = \"12g\", driver_memory=\"8g\"):\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"_initialize spark\")\n",
    "    #import pykhaos.utils.pyspark_configuration as pyspark_config\n",
    "    sc, spark, sql_context = get_spark_session(app_name=app_name, log_level=\"OFF\", min_n_executors = min_n_executors, max_n_executors = max_n_executors, n_cores = n_cores,\n",
    "                             executor_memory = executor_memory, driver_memory=driver_memory)\n",
    "    print(\"Ended spark session: {} secs | default parallelism={}\".format(time.time() - start_time,\n",
    "                                                                         sc.defaultParallelism))\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_initialize spark\n",
      "Ended spark session: 1264.03534007 secs | default parallelism=2\n"
     ]
    }
   ],
   "source": [
    "spark = initialize(\"Survival FBB \",executor_memory = \"32g\",min_n_executors = 6,max_n_executors = 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_paths():\n",
    "    '''\n",
    "    Deployment should be something like \"dirs/dir1/use-cases\"\n",
    "    This function adds to the path \"dirs/dir1/use-cases\" and \"dirs/dir1/\"\n",
    "    :return:\n",
    "    '''\n",
    "    import imp\n",
    "    from os.path import dirname\n",
    "    import os\n",
    "\n",
    "    USE_CASES = \"/var/SP/data/home/asaezco/src/devel2/use-cases\"#dirname(os.path.abspath(imp.find_module('churn')[1]))\n",
    "\n",
    "    if USE_CASES not in sys.path:\n",
    "        sys.path.append(USE_CASES)\n",
    "        print(\"Added '{}' to path\".format(USE_CASES))\n",
    "\n",
    "    # if deployment is correct, this path should be the one that contains \"use-cases\", \"pykhaos\", ...\n",
    "    # FIXME another way of doing it more general?\n",
    "    DEVEL_SRC = os.path.dirname(USE_CASES)  # dir before use-cases dir\n",
    "    if DEVEL_SRC not in sys.path:\n",
    "        sys.path.append(DEVEL_SRC)\n",
    "        print(\"Added '{}' to path\".format(DEVEL_SRC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from common.src.main.python.utils.hdfs_generic import *\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pyspark.sql.functions import (udf,\n",
    "                                    col,\n",
    "                                    decode,\n",
    "                                    when,\n",
    "                                    lit,\n",
    "                                    lower,\n",
    "                                    concat,\n",
    "                                    translate,\n",
    "                                    count,\n",
    "                                    sum as sql_sum,\n",
    "                                    max as sql_max,\n",
    "                                    min as sql_min,\n",
    "                                    avg as sql_avg,\n",
    "                                    greatest,\n",
    "                                    least,\n",
    "                                    isnull,\n",
    "                                    isnan,\n",
    "                                    struct, \n",
    "                                    substring,\n",
    "                                    size,\n",
    "                                    length,\n",
    "                                    year,\n",
    "                                    month,\n",
    "                                    dayofmonth,\n",
    "                                    unix_timestamp,\n",
    "                                    date_format,\n",
    "                                    from_unixtime,\n",
    "                                    datediff,\n",
    "                                    to_date, \n",
    "                                    desc,\n",
    "                                    asc,\n",
    "                                    countDistinct,\n",
    "                                    row_number,\n",
    "                                    skewness,\n",
    "                                    kurtosis,\n",
    "                                    concat_ws)\n",
    "\n",
    "from pyspark.sql import Row, DataFrame, Column, Window\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType, DateType, ArrayType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler, SQLTransformer, OneHotEncoder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from datetime import datetime\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from utils_general import *\n",
    "from pykhaos.utils.date_functions import convert_to_date\n",
    "from utils_model import *\n",
    "from utils_fbb_churn import *\n",
    "from metadata_fbb_churn import *\n",
    "from feature_selection_utils import *\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config from file model_under_eval.yaml\n",
      "Reading internal config from file /var/SP/data/home/asaezco/src/devel2/use-cases/churn/config_manager/config/internal_config_churn.yaml\n",
      "----- CHECKING INPUT PARAMETERS ------\n",
      "('check_args', 20190521, <type 'int'>)\n",
      "('20190521', 1, '20190531')\n",
      "----- INPUT PARAMETERS OK! ------\n",
      "{'closing_day': 20190521,\n",
      " 'cycles_horizon': 1,\n",
      " 'discarded_cycles': 0,\n",
      " 'internal_config_file': '/var/SP/data/home/asaezco/src/devel2/use-cases/churn/config_manager/config/internal_config_churn.yaml',\n",
      " 'labeled': False,\n",
      " 'level': 'service',\n",
      " 'mode': 'test/validation/predict',\n",
      " 'model_target': 'port',\n",
      " 'save_car': False,\n",
      " 'save_results': True,\n",
      " 'segment_filter': 'mobileandfbb',\n",
      " 'service_set': 'mobile',\n",
      " 'services': True,\n",
      " 'sources': {'ids': {'address': False,\n",
      "                     'billing': True,\n",
      "                     'call_centre_calls': False,\n",
      "                     'campaigns': True,\n",
      "                     'customer': True,\n",
      "                     'customer_aggregations': True,\n",
      "                     'customer_penalties': True,\n",
      "                     'device_catalogue': True,\n",
      "                     'geneva': True,\n",
      "                     'netscout': False,\n",
      "                     'orders': True,\n",
      "                     'tnps': True}},\n",
      " 'user_config_file': 'model_under_eval.yaml'}\n",
      "############ Config Object Created ############\n"
     ]
    }
   ],
   "source": [
    "import imp\n",
    "config_obj = Config('model_under_eval.yaml', internal_config_filename=os.path.join(imp.find_module('churn')[1], \"config_manager\", \"config\", \"internal_config_churn.yaml\"))\n",
    "print(\"############ Config Object Created ############\")\n",
    "    \n",
    "######################## TRAINING DF ########################\n",
    "    \n",
    "selcols = getIdFeats() + getCrmFeats() + getBillingFeats() + getMobSopoFeats() + getOrdersFeats()\n",
    "now = datetime.now()\n",
    "date_name = str(now.year) + str(now.month).rjust(2, '0') + str(now.day).rjust(2, '0')\n",
    "origin = '/user/hive/warehouse/tests_es.db/jvmm_amdocs_ids_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "selcols = getIdFeats() + getCrmFeats() + getBillingFeats() + getMobSopoFeats() + getOrdersFeats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info getCarNumClienteDf] Thu Jun 13 10:37:36 2019 Size of the original df for 20181130: 13248445\n",
      "[Info FbbChurn] Thu Jun 13 10:50:23 2019 Saving origindf_tmp to HDFS\n",
      "[Info getCarNumClienteDf] Thu Jun 13 10:50:23 2019 Size of the origindf after load from HDFS: 13248445\n",
      "[Info getCarNumClienteDf] Thu Jun 13 10:51:40 2019 Size of the numclidf for 20181130 - Num rows: 1978186 - Num columns: 101\n",
      "[Info FbbChurn] Thu Jun 13 10:58:10 2019 Saving numclidf_tmp to HDFS\n",
      "[Info get_billing_df] Thu Jun 13 10:58:11 2019 Starting the preparation of billing feats\n",
      "[Info get_billing_df] Thu Jun 13 10:58:25 2019 Size of the df with billing feats: 5714359\n",
      "[Info getCarNumClienteDf] Thu Jun 13 10:58:44 2019 Size of the tmpdf for 20181130 - Num rows: 1978186 - Num columns: 134\n",
      "[Info get_mobile_spinners_df] Thu Jun 13 10:59:10 2019 Port-out data loaded for 20181130 with a total of 27468495 rows and 11448545 distinct NIFs\n",
      "[Info get_mobile_spinners_df] Thu Jun 13 10:59:25 2019 Timing feats computed for a total of 11448545 distinct NIFs\n",
      "[Info get_mobile_spinners_df] Thu Jun 13 10:59:56 2019 Destinatination and state feats computed for a total of 11448545 distinct NIFs\n",
      "[Info getCarNumClienteDf] Thu Jun 13 11:00:42 2019 Size of the featdf for 20181130 - Num rows: 1978186 - Num columns: 236\n",
      "[Info get_orders_df] Thu Jun 13 11:01:47 2019 Orders feats up to 20181031 computed - Number of rows is 8150066 for a total of 8150066 distinct num_cliente\n",
      "[Info getCarNumClienteDf] Thu Jun 13 11:02:02 2019 Size of the featdf for 20181130 - Num rows: 1978186 - Num columns: 238\n",
      "[Info getFbbChurnLabeledCar] Thu Jun 13 11:03:27 2019 Samples for month 20181130: 1978186\n",
      "20181130\n",
      "20181231\n",
      "[Info getFixPortRequestsForMonth] Thu Jun 13 11:05:16 2019 Port-out requests for fixed services during period 20181130-20181231: 47422\n",
      "[Info getFbbDxsForMonth] Thu Jun 13 11:05:31 2019 DXs for FBB services during the period: 20181130-20181231: 63345\n",
      "[Info getFbbChurnLabeledCar] Thu Jun 13 11:06:22 2019 Labeled samples for month 201811: 1978186\n",
      "[Info Main FbbChurn] Thu Jun 13 11:08:40 2019 Count of the ExtraFeats:  8335417\n"
     ]
    }
   ],
   "source": [
    "trcycle_ini = '20181130'\n",
    "horizon = 4\n",
    "    # Cycle used for CAR and Extra Feats in the test set\n",
    "ttcycle_ini = '20181231'  # Test data\n",
    "    \n",
    "inittrdf_ini = getFbbChurnLabeledCarCycles(spark, origin, trcycle_ini, selcols, horizon)\n",
    "\n",
    "dfExtraFeat = spark.read.parquet('/data/udf/vf_es/churn/extra_feats_mod/extra_feats/year={}/month={}/day={}'\n",
    "                                         .format(int(trcycle_ini[0:4]), int(trcycle_ini[4:6]), int(trcycle_ini[6:8])))\n",
    "\n",
    "        # Taking only the clients with a fbb service\n",
    "dfExtraFeatfbb = dfExtraFeat.join(inittrdf_ini, [\"num_cliente\"], \"leftsemi\")\n",
    "\n",
    "dfExtraFeatfbb = dfExtraFeatfbb.cache()\n",
    "print \"[Info Main FbbChurn] \" + time.ctime() + \" Count of the ExtraFeats: \", dfExtraFeatfbb.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info FbbChurn] Thu Jun 13 11:18:14 2019 Inside CCC module of the Extra Feats \n",
      "[Info FbbChurn] Thu Jun 13 11:18:15 2019 Number of columns processed: 205\n",
      "[Info FbbChurn] Number of rows: 8335417\n",
      "[Info FbbChurn] Thu Jun 13 11:19:19 2019 The module for CCC has been run. Number of clients  1978186\n",
      "[Info FbbChurn] Thu Jun 13 11:19:49 2019 Inside NS module of the Extra Feats \n",
      "[Info FbbChurn] Thu Jun 13 11:19:51 2019 Number of columns processed: 608\n",
      "[Info FbbChurn] Thu Jun 13 11:19:58 2019 The module for NS has been run. Number of clients  1978186\n",
      "[Info FbbChurn] Thu Jun 13 11:27:01 2019 Inside TGS module of the Extra Feats \n",
      "[Info FbbChurn] Thu Jun 13 11:27:02 2019 Number of columns processed: 21\n",
      "[Info FbbChurn] Thu Jun 13 11:27:02 2019 The module for TGS has been run. Number of clientes  1978186\n",
      "[Info FbbChurn] Thu Jun 13 11:27:25 2019 Inside Orders module of the Extra Feats \n",
      "[Info FbbChurn] Thu Jun 13 11:27:25 2019 Number of columns processed: 8\n",
      "[Info FbbChurn] Thu Jun 13 11:27:26 2019 The module for Orders has been run. Number of clientes  1978186\n",
      "[Info FbbChurn] Thu Jun 13 11:27:30 2019 Inside Competitors module of the Extra Feats \n",
      "[Info FbbChurn] Thu Jun 13 11:27:31 2019 Number of columns processed: 275\n",
      "[Info FbbChurn] Thu Jun 13 11:27:35 2019 The module for Competitors has been run. Number of clients  1978186\n",
      "[Info FbbChurn] Thu Jun 13 11:28:41 2019 Inside Device module of the Extra Feats \n",
      "[Info FbbChurn] Thu Jun 13 11:28:41 2019 Number of columns processed: 35\n",
      "[Info FbbChurn] Number of rows: 8335417\n",
      "[Info FbbChurn] Thu Jun 13 11:28:51 2019 The module for Device has been run. Number of clients  1978186\n",
      "[Info FbbChurn] Thu Jun 13 11:28:56 2019 Inside Pbms module of the Extra Feats \n",
      "[Info FbbChurn] Thu Jun 13 11:28:56 2019 Number of columns processed: 10\n",
      "[Info FbbChurn] Thu Jun 13 11:28:57 2019 The module for Pbms has been run. Number of clientes  1978186\n",
      "[Info FbbChurn] Thu Jun 13 11:29:00 2019 Inside Additional module of the Extra Feats \n",
      "[Info FbbChurn] Thu Jun 13 11:29:00 2019 Number of columns processed: 42\n",
      "[Info FbbChurn] Thu Jun 13 11:29:01 2019 The module for Additional has been run. Number of clientes  1978186\n",
      "[Info FbbChurn] Thu Jun 13 11:29:08 2019 Inside Campaigns module of the Extra Feats \n",
      "[Info FbbChurn] Thu Jun 13 11:29:08 2019 Number of columns processed: 33\n",
      "[Info FbbChurn] Thu Jun 13 11:29:09 2019 The module for Campaigns has been run. Number of clientes  1978186\n",
      "[Info FbbChurn] Thu Jun 13 11:29:17 2019 Inside Incremental module of the Extra Feats \n",
      "[Info FbbChurn] Thu Jun 13 11:29:17 2019 Number of columns processed: 155\n",
      "[Info FbbChurn] Number of rows: 8335417\n",
      "[Info FbbChurn] Thu Jun 13 11:29:54 2019 The module for Incremental has been run. Number of clients  1978186\n",
      "('[Info FbbChurn] Thu Jun 13 11:31:24 2019 The extra feats have been joined. Number of clientes ', 1978186)\n",
      "[Info Main FbbChurn] Thu Jun 13 11:38:15 2019 Calculating the total value of the extra feats for each number client\n",
      "############ Finished loading Training Dataset ############\n"
     ]
    }
   ],
   "source": [
    "dfExtraFeatSel, selColumnas = addExtraFeatsEvol(dfExtraFeatfbb)\n",
    "\n",
    "print \"[Info Main FbbChurn] \" + time.ctime() + \" Calculating the total value of the extra feats for each number client\"\n",
    "\n",
    "dfillNa = fillNa(spark)\n",
    "for kkey in dfillNa.keys():\n",
    "    if kkey not in dfExtraFeatSel.columns:\n",
    "        dfillNa.pop(kkey, None)\n",
    "\n",
    "inittrdf = inittrdf_ini.join(dfExtraFeatSel, [\"msisdn\", \"num_cliente\", 'rgu'], how=\"left\").na.fill(dfillNa)\n",
    "print(\"############ Finished loading Training Dataset ############\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info getCarNumClienteDf] Thu Jun 13 11:53:22 2019 Size of the original df for 20181231: 13683905\n",
      "[Info FbbChurn] Thu Jun 13 12:07:42 2019 Saving origindf_tmp to HDFS\n",
      "[Info getCarNumClienteDf] Thu Jun 13 12:07:42 2019 Size of the origindf after load from HDFS: 13683905\n",
      "[Info getCarNumClienteDf] Thu Jun 13 12:09:00 2019 Size of the numclidf for 20181231 - Num rows: 2050838 - Num columns: 101\n",
      "[Info FbbChurn] Thu Jun 13 12:15:57 2019 Saving numclidf_tmp to HDFS\n",
      "[Info get_billing_df] Thu Jun 13 12:15:58 2019 Starting the preparation of billing feats\n",
      "[Info get_billing_df] Thu Jun 13 12:16:51 2019 Size of the df with billing feats: 5825804\n",
      "[Info getCarNumClienteDf] Thu Jun 13 12:17:04 2019 Size of the tmpdf for 20181231 - Num rows: 2050838 - Num columns: 134\n",
      "[Info get_mobile_spinners_df] Thu Jun 13 12:17:19 2019 Port-out data loaded for 20181231 with a total of 27844643 rows and 11499038 distinct NIFs\n",
      "[Info get_mobile_spinners_df] Thu Jun 13 12:18:10 2019 Timing feats computed for a total of 11499038 distinct NIFs\n",
      "[Info get_mobile_spinners_df] Thu Jun 13 12:18:28 2019 Destinatination and state feats computed for a total of 11499038 distinct NIFs\n",
      "[Info getCarNumClienteDf] Thu Jun 13 12:18:54 2019 Size of the featdf for 20181231 - Num rows: 2050838 - Num columns: 236\n",
      "[Info get_orders_df] Thu Jun 13 12:20:47 2019 Orders feats up to 20181130 computed - Number of rows is 8725478 for a total of 8725478 distinct num_cliente\n",
      "[Info getCarNumClienteDf] Thu Jun 13 12:21:08 2019 Size of the featdf for 20181231 - Num rows: 2050838 - Num columns: 238\n",
      "[Info getFbbChurnLabeledCar] Thu Jun 13 12:23:03 2019 Samples for month 20181231: 2050838\n",
      "20181231\n",
      "20190131\n",
      "[Info getFixPortRequestsForMonth] Thu Jun 13 12:24:49 2019 Port-out requests for fixed services during period 20181231-20190131: 57686\n",
      "[Info getFbbDxsForMonth] Thu Jun 13 12:25:04 2019 DXs for FBB services during the period: 20181231-20190131: 71798\n",
      "[Info getFbbChurnLabeledCar] Thu Jun 13 12:26:03 2019 Labeled samples for month 201812: 2050838\n",
      "[Info Main FbbChurn] Thu Jun 13 12:28:41 2019 Saving ttdf_ini to HDFS \n",
      "(2050764, 2050764)\n",
      "('[Info Main FbbChurn] Thu Jun 13 12:34:20 2019 Count of the ExtraFeats ', 8701397)\n",
      "[Info FbbChurn] Thu Jun 13 12:50:47 2019 Inside CCC module of the Extra Feats \n",
      "[Info FbbChurn] Thu Jun 13 12:50:48 2019 Number of columns processed: 205\n",
      " 2050838\n",
      "[Info FbbChurn] Thu Jun 13 12:53:30 2019 Inside NS module of the Extra Feats \n",
      "[Info FbbChurn] Thu Jun 13 12:53:32 2019 Number of columns processed: 608\n",
      "[Info FbbChurn] Thu Jun 13 12:53:40 2019 The module for NS has been run. Number of clients  2050838\n",
      "[Info FbbChurn] Thu Jun 13 13:02:29 2019 Inside TGS module of the Extra Feats \n",
      "[Info FbbChurn] Thu Jun 13 13:02:29 2019 Number of columns processed: 21\n",
      "[Info FbbChurn] Thu Jun 13 13:02:30 2019 The module for TGS has been run. Number of clientes  2050838\n",
      "[Info FbbChurn] Thu Jun 13 13:03:03 2019 Inside Orders module of the Extra Feats \n",
      "[Info FbbChurn] Thu Jun 13 13:03:03 2019 Number of columns processed: 8\n",
      "[Info FbbChurn] Thu Jun 13 13:03:04 2019 The module for Orders has been run. Number of clientes  2050838\n",
      "[Info FbbChurn] Thu Jun 13 13:03:17 2019 Inside Competitors module of the Extra Feats \n",
      "[Info FbbChurn] Thu Jun 13 13:03:18 2019 Number of columns processed: 275\n",
      "[Info FbbChurn] Thu Jun 13 13:03:23 2019 The module for Competitors has been run. Number of clients  2050838\n",
      "[Info FbbChurn] Thu Jun 13 13:05:39 2019 Inside Device module of the Extra Feats \n",
      "[Info FbbChurn] Thu Jun 13 13:05:39 2019 Number of columns processed: 35\n",
      "[Info FbbChurn] Number of rows: 8701397\n",
      "[Info FbbChurn] Thu Jun 13 13:06:02 2019 The module for Device has been run. Number of clients  2050838\n",
      "[Info FbbChurn] Thu Jun 13 13:06:07 2019 Inside Pbms module of the Extra Feats \n",
      "[Info FbbChurn] Thu Jun 13 13:06:07 2019 Number of columns processed: 10\n",
      "[Info FbbChurn] Thu Jun 13 13:06:08 2019 The module for Pbms has been run. Number of clientes  2050838\n",
      "[Info FbbChurn] Thu Jun 13 13:06:22 2019 Inside Additional module of the Extra Feats \n",
      "[Info FbbChurn] Thu Jun 13 13:06:22 2019 Number of columns processed: 42\n",
      "[Info FbbChurn] Thu Jun 13 13:06:23 2019 The module for Additional has been run. Number of clientes  2050838\n",
      "[Info FbbChurn] Thu Jun 13 13:06:49 2019 Inside Campaigns module of the Extra Feats \n",
      "[Info FbbChurn] Thu Jun 13 13:06:49 2019 Number of columns processed: 33\n",
      "[Info FbbChurn] Thu Jun 13 13:06:50 2019 The module for Campaigns has been run. Number of clientes  2050838\n",
      "[Info FbbChurn] Thu Jun 13 13:07:11 2019 Inside Incremental module of the Extra Feats \n",
      "[Info FbbChurn] Thu Jun 13 13:07:12 2019 Number of columns processed: 155\n",
      "[Info FbbChurn] Number of rows: 8701397\n",
      "[Info FbbChurn] Thu Jun 13 13:08:18 2019 The module for Incremental has been run. Number of clients  2050838\n",
      "('[Info FbbChurn] Thu Jun 13 13:10:16 2019 The extra feats have been joined. Number of clientes ', 2050838)\n",
      "[Info Main FbbChurn] Thu Jun 13 13:14:03 2019 Calculating the total value of the extra feats for each number client in tt\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o26089.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1611.0 failed 4 times, most recent failure: Lost task 1.3 in stage 1611.0 (TID 153312, vgddp367hr.dc.sedc.internal.vodafone.com, executor 29): java.io.FileNotFoundException: File does not exist: hdfs://nameservice1/data/udf/vf_es/churn/fbb_tmp/numclidf_tmp_20181231/part-00100-752d3347-024b-4510-b91d-6a15c1650d4b-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:182)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:381)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2775)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2774)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2774)\n\tat sun.reflect.GeneratedMethodAccessor144.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: File does not exist: hdfs://nameservice1/data/udf/vf_es/churn/fbb_tmp/numclidf_tmp_20181231/part-00100-752d3347-024b-4510-b91d-6a15c1650d4b-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:182)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:381)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-180f12e8e117>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mttdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mttdf_ini\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfExtraFeat_ttSel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"msisdn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"num_cliente\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rgu'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfillNa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"[Info Main FbbChurn] \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" Number of clients after joining the Extra Feats to the test set \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mttdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"############ Finished loading Test Dataset ############\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \"\"\"\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/Anaconda-2.5.0/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/Anaconda-2.5.0/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o26089.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 1611.0 failed 4 times, most recent failure: Lost task 1.3 in stage 1611.0 (TID 153312, vgddp367hr.dc.sedc.internal.vodafone.com, executor 29): java.io.FileNotFoundException: File does not exist: hdfs://nameservice1/data/udf/vf_es/churn/fbb_tmp/numclidf_tmp_20181231/part-00100-752d3347-024b-4510-b91d-6a15c1650d4b-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:182)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:381)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2775)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2774)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2774)\n\tat sun.reflect.GeneratedMethodAccessor144.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: File does not exist: hdfs://nameservice1/data/udf/vf_es/churn/fbb_tmp/numclidf_tmp_20181231/part-00100-752d3347-024b-4510-b91d-6a15c1650d4b-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:182)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:381)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "ttdf_ini = getFbbChurnLabeledCarCycles(spark, origin, ttcycle_ini, selcols,horizon)\n",
    "\n",
    "print \"[Info Main FbbChurn] \" + time.ctime() + \" Saving ttdf_ini to HDFS \"\n",
    "\n",
    "dfExtraFeat_tt = spark.read.parquet('/data/udf/vf_es/churn/extra_feats_mod/extra_feats/year={}/month={}/day={}'\n",
    "                                            .format(int(ttcycle_ini[0:4]), int(ttcycle_ini[4:6]), int(ttcycle_ini[6:8])))\n",
    "\n",
    "dfExtraFeatfbb_tt = dfExtraFeat_tt.join(ttdf_ini.select('num_cliente'), on='num_cliente', how='leftsemi')\n",
    "print(dfExtraFeatfbb_tt.select('num_cliente').distinct().count(), ttdf_ini.select('num_cliente').distinct().count())\n",
    "\n",
    "dfExtraFeatfbb_tt = dfExtraFeatfbb_tt.cache()\n",
    "print(\"[Info Main FbbChurn] \" + time.ctime() + \" Count of the ExtraFeats \", dfExtraFeatfbb_tt.count())\n",
    "\n",
    "dfExtraFeat_ttSel, selColumnas = addExtraFeatsEvol(dfExtraFeatfbb_tt)\n",
    "\n",
    "print \"[Info Main FbbChurn] \" + time.ctime() + \" Calculating the total value of the extra feats for each number client in tt\"\n",
    "\n",
    "dfillNa = fillNa(spark)\n",
    "for kkey in dfillNa.keys():\n",
    "    if kkey not in dfExtraFeat_ttSel.columns:\n",
    "        dfillNa.pop(kkey, None)\n",
    "\n",
    "ttdf = ttdf_ini.join(dfExtraFeat_ttSel, [\"msisdn\", \"num_cliente\", 'rgu'], how=\"left\").na.fill(dfillNa)\n",
    "print \"[Info Main FbbChurn] \" + time.ctime() + \" Number of clients after joining the Extra Feats to the test set \" + str(ttdf.count())\n",
    "print(\"############ Finished loading Test Dataset ############\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closing_day_f = '20181231'\n",
    "key = \"censor\"\n",
    "label = 'label'\n",
    "now = dt.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "quantileProbabilities = [0.5, 0.75, 0.8, 0.9, 0.95, 0.99]\n",
    "    \n",
    "cycles_horizon = 4\n",
    "closing_day = '20181231'\n",
    "discarded_cycles = 0\n",
    "start_date = '20181130'\n",
    "start_date = move_date_n_cycles(start_date, n=discarded_cycles) if discarded_cycles > 0 else move_date_n_days(start_date, n=1)\n",
    "end_date = move_date_n_cycles(start_date, n=cycles_horizon)\n",
    "print('End Date')\n",
    "print(end_date)\n",
    "df_sol_por_discard = get_port_requests_table(spark, config_obj, start_date, end_date, closing_day, select_cols=['msisdn_a','portout_date', 'label'])\n",
    "    \n",
    "df_ids = getIds(spark, closing_day_f)\n",
    "    \n",
    "portas = df_sol_por_discard.select('msisdn_a','portout_date', 'label').withColumnRenamed('msisdn_a','msisdn')\n",
    "    \n",
    "labeled = inittrdf.join(portas, ['msisdn'], how=\"left\").na.fill({'label': 0.0})\n",
    "    \n",
    "labeled2 = labeled.withColumn('endDate', labeled['portout_date'].cast(DateType()))\n",
    "    \n",
    "start_ = 'fx_fbb_fx_first'\n",
    "    \n",
    "labeled3 = labeled2.join( df_ids.select('msisdn', start_ +'_nif', start_ +'_nc'), ['msisdn'], 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import least, col\n",
    "labeled4 = labeled3.withColumn('startTime', least(labeled3[start_ +'_nif'],labeled3[start_ +'_nc']))\n",
    "prepared = labeled4.withColumn('startDate', labeled4['startTime'].cast(DateType()))\n",
    "    \n",
    "from pyspark.sql import functions as F\n",
    "timeFmt = \"yyyy-MM-dd\"\n",
    "timeDiff = (F.unix_timestamp('endDate', format=timeFmt) - F.unix_timestamp('startDate', format=timeFmt))\n",
    "final_df = prepared.withColumn(\"Duration\", timeDiff)\n",
    "\n",
    "final_censor = final_df.withColumnRenamed('label', 'censor')\n",
    "timeDiff2 = (F.unix_timestamp('closing_day', format=timeFmt) - F.unix_timestamp('startTime', format=timeFmt))\n",
    "import datetime as datetime\n",
    "closing_date=datetime.date(int(closing_day_f[:4]),int(closing_day_f[4:6]),int(closing_day_f[6:8]))\n",
    "    \n",
    "final = final_censor.withColumn('closing_day', lit(closing_date))\n",
    "    \n",
    "sec_2_day = 3600*24\n",
    "    \n",
    "final_2 = final.withColumn('label', when(final[key] == 1, final_censor['Duration']/(sec_2_day)).otherwise(timeDiff2/(sec_2_day)))\n",
    "    \n",
    "final_d = final_2\n",
    "(Train_, Test_) = final_d.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Getting only the numeric variables\n",
    "allFeats = inittrdf.columns\n",
    "catCols = [item[0] for item in inittrdf.dtypes if item[1].startswith('string')]\n",
    "numerical_feats = list(set(allFeats) - set(list(set().union(getIdFeats(), getIdFeats_tr(), getNoInputFeats(), catCols, [c + \"_enc\" for c in getCatFeatsCrm()],\n",
    "                    [\"label\"]))))\n",
    "    \n",
    "trdf = balance_df2(Train_.where(col(label).isNotNull()).filter(Train_[label] > 0), 'censor')\n",
    "print(\"############ Balanced Dataset ############\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
