{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added '/var/SP/data/home/asaezco/src/devel2/use-cases' to path\n",
      "Added '/var/SP/data/home/asaezco/src/devel2' to path\n"
     ]
    }
   ],
   "source": [
    "def set_paths():\n",
    "    '''\n",
    "    Deployment should be something like \"dirs/dir1/use-cases\"\n",
    "    This function adds to the path \"dirs/dir1/use-cases\" and \"dirs/dir1/\"\n",
    "    :return:\n",
    "    '''\n",
    "    import imp\n",
    "    from os.path import dirname\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    USE_CASES = \"/var/SP/data/home/asaezco/src/devel2/use-cases\"#dirname(os.path.abspath(imp.find_module('churn')[1]))\n",
    "    sys.path.append(\"/var/SP/data/home/asaezco/src/devel2/pykhaos\")\n",
    "    if USE_CASES not in sys.path:\n",
    "        sys.path.append(USE_CASES)\n",
    "        print(\"Added '{}' to path\".format(USE_CASES))\n",
    "\n",
    "    # if deployment is correct, this path should be the one that contains \"use-cases\", \"pykhaos\", ...\n",
    "    # FIXME another way of doing it more general?\n",
    "    DEVEL_SRC = os.path.dirname(USE_CASES)  # dir before use-cases dir\n",
    "    if DEVEL_SRC not in sys.path:\n",
    "        sys.path.append(DEVEL_SRC)\n",
    "        print(\"Added '{}' to path\".format(DEVEL_SRC))\n",
    "set_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "\n",
    "import sys\n",
    "\n",
    "from common.src.main.python.utils.hdfs_generic import *\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pyspark.sql.functions import (udf,\n",
    "                                    col,\n",
    "                                    decode,\n",
    "                                    when,\n",
    "                                    lit,\n",
    "                                    lower,\n",
    "                                    concat,\n",
    "                                    translate,\n",
    "                                    count,\n",
    "                                    sum as sql_sum,\n",
    "                                    max as sql_max,\n",
    "                                    min as sql_min,\n",
    "                                    avg as sql_avg,\n",
    "                                    greatest,\n",
    "                                    least,\n",
    "                                    isnull,\n",
    "                                    isnan,\n",
    "                                    struct, \n",
    "                                    substring,\n",
    "                                    size,\n",
    "                                    length,\n",
    "                                    year,\n",
    "                                    month,\n",
    "                                    dayofmonth,\n",
    "                                    unix_timestamp,\n",
    "                                    date_format,\n",
    "                                    from_unixtime,\n",
    "                                    datediff,\n",
    "                                    to_date, \n",
    "                                    desc,\n",
    "                                    asc,\n",
    "                                    countDistinct,\n",
    "                                    row_number,\n",
    "                                    skewness,\n",
    "                                    kurtosis,\n",
    "                                    concat_ws)\n",
    "\n",
    "from pyspark.sql import Row, DataFrame, Column, Window\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType, DateType, ArrayType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler, SQLTransformer, OneHotEncoder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from datetime import datetime\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from utils_general import *\n",
    "from pykhaos.utils.date_functions import convert_to_date\n",
    "from utils_model import *\n",
    "from utils_fbb_churn import *\n",
    "from metadata_fbb_churn import *\n",
    "from feature_selection_utils import *\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para que no haga falta recargar el notebook si hacemos un cambio en alguna de las funciones que estamos importando\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set BDP parameters\n",
    "def setting_bdp(min_n_executors = 3, max_n_executors = 15, n_cores = 8, executor_memory = \"12g\", driver_memory=\"8g\",\n",
    "                   app_name = \"Python app\", driver_overhead=\"1g\", executor_overhead='3g'):\n",
    "\n",
    "    MAX_N_EXECUTORS = max_n_executors\n",
    "    MIN_N_EXECUTORS = min_n_executors\n",
    "    N_CORES_EXECUTOR = n_cores\n",
    "    EXECUTOR_IDLE_MAX_TIME = 120\n",
    "    EXECUTOR_MEMORY = executor_memory\n",
    "    DRIVER_MEMORY = driver_memory\n",
    "    N_CORES_DRIVER = 1\n",
    "    MEMORY_OVERHEAD = N_CORES_EXECUTOR * 2048\n",
    "    QUEUE = \"root.BDPtenants.es.low\"\n",
    "    BDA_CORE_VERSION = \"1.0.0\"\n",
    "\n",
    "    SPARK_COMMON_OPTS = os.environ.get('SPARK_COMMON_OPTS', '')\n",
    "    SPARK_COMMON_OPTS += \" --executor-memory %s --driver-memory %s\" % (EXECUTOR_MEMORY, DRIVER_MEMORY)\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.shuffle.manager=tungsten-sort\"\n",
    "    SPARK_COMMON_OPTS += \"  --queue %s\" % QUEUE\n",
    "\n",
    "    # Dynamic allocation configuration\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.dynamicAllocation.enabled=true\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.shuffle.service.enabled=true\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.dynamicAllocation.maxExecutors=%s\" % (MAX_N_EXECUTORS)\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.dynamicAllocation.minExecutors=%s\" % (MIN_N_EXECUTORS)\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.executor.cores=%s\" % (N_CORES_EXECUTOR)\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.dynamicAllocation.executorIdleTimeout=%s\" % (EXECUTOR_IDLE_MAX_TIME)\n",
    "    # SPARK_COMMON_OPTS += \" --conf spark.ui.port=58235\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.port.maxRetries=100\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.app.name='%s'\" % (app_name)\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.submit.deployMode=client\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.ui.showConsoleProgress=true\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.sql.broadcastTimeout=1200\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.yarn.executor.memoryOverhead={}\".format(executor_overhead)\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.yarn.executor.driverOverhead={}\".format(driver_overhead)\n",
    "\n",
    "    BDA_ENV = os.environ.get('BDA_USER_HOME', '')\n",
    "\n",
    "    # Attach bda-core-ra codebase\n",
    "    SPARK_COMMON_OPTS+=\" --files {}/scripts/properties/red_agent/nodes.properties,{}/scripts/properties/red_agent/nodes-de.properties,{}/scripts/properties/red_agent/nodes-es.properties,{}/scripts/properties/red_agent/nodes-ie.properties,{}/scripts/properties/red_agent/nodes-it.properties,{}/scripts/properties/red_agent/nodes-pt.properties,{}/scripts/properties/red_agent/nodes-uk.properties\".format(*[BDA_ENV]*7)\n",
    "\n",
    "    os.environ[\"SPARK_COMMON_OPTS\"] = SPARK_COMMON_OPTS\n",
    "    os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"%s pyspark-shell \" % SPARK_COMMON_OPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_session(app_name=\"default name\", log_level='INFO', min_n_executors = 3, max_n_executors = 15, n_cores = 4, executor_memory = \"12g\", driver_memory=\"8g\"):\n",
    "    HOME_SRC = os.path.join(os.environ.get('BDA_USER_HOME', ''), \"src\")\n",
    "    if HOME_SRC not in sys.path:\n",
    "        sys.path.append(HOME_SRC)\n",
    "\n",
    "\n",
    "    setting_bdp(app_name=app_name, min_n_executors = min_n_executors, max_n_executors = max_n_executors, n_cores = n_cores, executor_memory = executor_memory, driver_memory=driver_memory)\n",
    "    from common.src.main.python.utils.hdfs_generic import run_sc\n",
    "    sc, spark, sql_context = run_sc(log_level=log_level)\n",
    "\n",
    "\n",
    "    return sc, spark, sql_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(app_name, min_n_executors = 3, max_n_executors = 15, n_cores = 4, executor_memory = \"12g\", driver_memory=\"8g\"):\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"_initialize spark\")\n",
    "    #import pykhaos.utils.pyspark_configuration as pyspark_config\n",
    "    sc, spark, sql_context = get_spark_session(app_name=app_name, log_level=\"OFF\", min_n_executors = min_n_executors, max_n_executors = max_n_executors, n_cores = n_cores,\n",
    "                             executor_memory = executor_memory, driver_memory=driver_memory)\n",
    "    print(\"Ended spark session: {} secs | default parallelism={}\".format(time.time() - start_time,\n",
    "                                                                         sc.defaultParallelism))\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_initialize spark\n",
      "Ended spark session: 234.74130702 secs | default parallelism=2\n"
     ]
    }
   ],
   "source": [
    "spark = initialize(\"Model Outputs Test \",executor_memory = \"32g\",min_n_executors = 6,max_n_executors = 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_paths()\n",
    "\n",
    "from pykhaos.utils.date_functions import *\n",
    "from utils_fbb_churn import *\n",
    "global sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('/data/attributes/vf_es/model_outputs/model_scores/model_name=churn_preds_fbb/year=2019/month=7/day=18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- client_id: string (nullable = true)\n",
      " |-- scoring: float (nullable = true)\n",
      " |-- nif: string (nullable = true)\n",
      " |-- msisdn: string (nullable = true)\n",
      " |-- executed_at: timestamp (nullable = true)\n",
      " |-- model_output: string (nullable = true)\n",
      " |-- model_executed_at: timestamp (nullable = true)\n",
      " |-- predict_closing_date: string (nullable = true)\n",
      " |-- time: integer (nullable = true)\n",
      " |-- prediction: string (nullable = true)\n",
      " |-- extra_info: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|msisdn|\n",
      "+------+\n",
      "|     -|\n",
      "|     -|\n",
      "|     -|\n",
      "|     -|\n",
      "|     -|\n",
      "|     -|\n",
      "|     -|\n",
      "|     -|\n",
      "|     -|\n",
      "|     -|\n",
      "|     -|\n",
      "|     -|\n",
      "|     -|\n",
      "|     -|\n",
      "|     -|\n",
      "|     -|\n",
      "|     -|\n",
      "|     -|\n",
      "|     -|\n",
      "|     -|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('msisdn').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['client_id',\n",
       " 'scoring',\n",
       " 'nif',\n",
       " 'msisdn',\n",
       " 'executed_at',\n",
       " 'model_output',\n",
       " 'model_executed_at',\n",
       " 'predict_closing_date',\n",
       " 'time',\n",
       " 'prediction',\n",
       " 'extra_info']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columnss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.select('client_id',\n",
    " 'scoring',\n",
    " 'nif',\n",
    " 'executed_at',\n",
    " 'model_output',\n",
    " 'model_executed_at',\n",
    " 'predict_closing_date',\n",
    " 'time',\n",
    " 'prediction',\n",
    " 'extra_info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_save = df2.withColumn('msisdn', lit(''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- client_id: string (nullable = true)\n",
      " |-- scoring: float (nullable = true)\n",
      " |-- nif: string (nullable = true)\n",
      " |-- executed_at: timestamp (nullable = true)\n",
      " |-- model_output: string (nullable = true)\n",
      " |-- model_executed_at: timestamp (nullable = true)\n",
      " |-- predict_closing_date: string (nullable = true)\n",
      " |-- time: integer (nullable = true)\n",
      " |-- prediction: string (nullable = true)\n",
      " |-- extra_info: string (nullable = true)\n",
      " |-- msisdn: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_save.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_name = '20190720'\n",
    "\n",
    "df_save2 = df_save.withColumn('year', lit(datetime.strptime(date_name, '%Y%m%d').year))\\\n",
    "                   .withColumn('month', lit(datetime.strptime(date_name, '%Y%m%d').month))\\\n",
    "                   .withColumn('day', lit(datetime.strptime(date_name, '%Y%m%d').day))\\\n",
    "        .withColumn('model_name', lit('churn_preds_fbb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|     model_name|\n",
      "+---------------+\n",
      "|churn_preds_fbb|\n",
      "|churn_preds_fbb|\n",
      "|churn_preds_fbb|\n",
      "|churn_preds_fbb|\n",
      "|churn_preds_fbb|\n",
      "|churn_preds_fbb|\n",
      "|churn_preds_fbb|\n",
      "|churn_preds_fbb|\n",
      "|churn_preds_fbb|\n",
      "|churn_preds_fbb|\n",
      "+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_save2.select('model_name').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_save2.write.partitionBy('model_name', 'year', 'month', 'day').mode(\"append\").format(\"parquet\")\\\n",
    ".save('/data/attributes/vf_es/model_outputs/model_scores/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'/data/attributes/vf_es/model_outputs/model_scores/model_name=churn_preds_fbb/year=2019/month=7/day=18'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
