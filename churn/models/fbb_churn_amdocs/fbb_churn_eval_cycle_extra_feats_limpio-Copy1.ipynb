{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added '/var/SP/data/home/asaezco/src/devel2/use-cases' to path\n",
      "Added '/var/SP/data/home/asaezco/src/devel2' to path\n"
     ]
    }
   ],
   "source": [
    "def set_paths():\n",
    "    '''\n",
    "    Deployment should be something like \"dirs/dir1/use-cases\"\n",
    "    This function adds to the path \"dirs/dir1/use-cases\" and \"dirs/dir1/\"\n",
    "    :return:\n",
    "    '''\n",
    "    import imp\n",
    "    from os.path import dirname\n",
    "    import os\n",
    "    import sys\n",
    "\n",
    "    USE_CASES = \"/var/SP/data/home/asaezco/src/devel2/use-cases\"#dirname(os.path.abspath(imp.find_module('churn')[1]))\n",
    "    sys.path.append(\"/var/SP/data/home/asaezco/src/devel2/pykhaos\")\n",
    "    if USE_CASES not in sys.path:\n",
    "        sys.path.append(USE_CASES)\n",
    "        print(\"Added '{}' to path\".format(USE_CASES))\n",
    "\n",
    "    # if deployment is correct, this path should be the one that contains \"use-cases\", \"pykhaos\", ...\n",
    "    # FIXME another way of doing it more general?\n",
    "    DEVEL_SRC = os.path.dirname(USE_CASES)  # dir before use-cases dir\n",
    "    if DEVEL_SRC not in sys.path:\n",
    "        sys.path.append(DEVEL_SRC)\n",
    "        print(\"Added '{}' to path\".format(DEVEL_SRC))\n",
    "set_paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "\n",
    "import sys\n",
    "\n",
    "from common.src.main.python.utils.hdfs_generic import *\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pyspark.sql.functions import (udf,\n",
    "                                    col,\n",
    "                                    decode,\n",
    "                                    when,\n",
    "                                    lit,\n",
    "                                    lower,\n",
    "                                    concat,\n",
    "                                    translate,\n",
    "                                    count,\n",
    "                                    sum as sql_sum,\n",
    "                                    max as sql_max,\n",
    "                                    min as sql_min,\n",
    "                                    avg as sql_avg,\n",
    "                                    greatest,\n",
    "                                    least,\n",
    "                                    isnull,\n",
    "                                    isnan,\n",
    "                                    struct, \n",
    "                                    substring,\n",
    "                                    size,\n",
    "                                    length,\n",
    "                                    year,\n",
    "                                    month,\n",
    "                                    dayofmonth,\n",
    "                                    unix_timestamp,\n",
    "                                    date_format,\n",
    "                                    from_unixtime,\n",
    "                                    datediff,\n",
    "                                    to_date, \n",
    "                                    desc,\n",
    "                                    asc,\n",
    "                                    countDistinct,\n",
    "                                    row_number,\n",
    "                                    skewness,\n",
    "                                    kurtosis,\n",
    "                                    concat_ws)\n",
    "\n",
    "from pyspark.sql import Row, DataFrame, Column, Window\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType, DateType, ArrayType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler, SQLTransformer, OneHotEncoder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from datetime import datetime\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from utils_general import *\n",
    "from pykhaos.utils.date_functions import convert_to_date\n",
    "from utils_model import *\n",
    "from utils_fbb_churn import *\n",
    "from metadata_fbb_churn import *\n",
    "from feature_selection_utils import *\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para que no haga falta recargar el notebook si hacemos un cambio en alguna de las funciones que estamos importando\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set BDP parameters\n",
    "def setting_bdp(min_n_executors = 3, max_n_executors = 15, n_cores = 8, executor_memory = \"12g\", driver_memory=\"8g\",\n",
    "                   app_name = \"Python app\", driver_overhead=\"1g\", executor_overhead='3g'):\n",
    "\n",
    "    MAX_N_EXECUTORS = max_n_executors\n",
    "    MIN_N_EXECUTORS = min_n_executors\n",
    "    N_CORES_EXECUTOR = n_cores\n",
    "    EXECUTOR_IDLE_MAX_TIME = 120\n",
    "    EXECUTOR_MEMORY = executor_memory\n",
    "    DRIVER_MEMORY = driver_memory\n",
    "    N_CORES_DRIVER = 1\n",
    "    MEMORY_OVERHEAD = N_CORES_EXECUTOR * 2048\n",
    "    QUEUE = \"root.BDPtenants.es.medium\"\n",
    "    BDA_CORE_VERSION = \"1.0.0\"\n",
    "\n",
    "    SPARK_COMMON_OPTS = os.environ.get('SPARK_COMMON_OPTS', '')\n",
    "    SPARK_COMMON_OPTS += \" --executor-memory %s --driver-memory %s\" % (EXECUTOR_MEMORY, DRIVER_MEMORY)\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.shuffle.manager=tungsten-sort\"\n",
    "    SPARK_COMMON_OPTS += \"  --queue %s\" % QUEUE\n",
    "\n",
    "    # Dynamic allocation configuration\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.dynamicAllocation.enabled=true\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.shuffle.service.enabled=true\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.dynamicAllocation.maxExecutors=%s\" % (MAX_N_EXECUTORS)\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.dynamicAllocation.minExecutors=%s\" % (MIN_N_EXECUTORS)\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.executor.cores=%s\" % (N_CORES_EXECUTOR)\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.dynamicAllocation.executorIdleTimeout=%s\" % (EXECUTOR_IDLE_MAX_TIME)\n",
    "    # SPARK_COMMON_OPTS += \" --conf spark.ui.port=58235\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.port.maxRetries=100\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.app.name='%s'\" % (app_name)\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.submit.deployMode=client\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.ui.showConsoleProgress=true\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.sql.broadcastTimeout=1200\"\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.yarn.executor.memoryOverhead={}\".format(executor_overhead)\n",
    "    SPARK_COMMON_OPTS += \" --conf spark.yarn.executor.driverOverhead={}\".format(driver_overhead)\n",
    "\n",
    "    BDA_ENV = os.environ.get('BDA_USER_HOME', '')\n",
    "\n",
    "    # Attach bda-core-ra codebase\n",
    "    SPARK_COMMON_OPTS+=\" --files {}/scripts/properties/red_agent/nodes.properties,{}/scripts/properties/red_agent/nodes-de.properties,{}/scripts/properties/red_agent/nodes-es.properties,{}/scripts/properties/red_agent/nodes-ie.properties,{}/scripts/properties/red_agent/nodes-it.properties,{}/scripts/properties/red_agent/nodes-pt.properties,{}/scripts/properties/red_agent/nodes-uk.properties\".format(*[BDA_ENV]*7)\n",
    "\n",
    "    os.environ[\"SPARK_COMMON_OPTS\"] = SPARK_COMMON_OPTS\n",
    "    os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"%s pyspark-shell \" % SPARK_COMMON_OPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_session(app_name=\"default name\", log_level='INFO', min_n_executors = 3, max_n_executors = 15, n_cores = 4, executor_memory = \"12g\", driver_memory=\"8g\"):\n",
    "    HOME_SRC = os.path.join(os.environ.get('BDA_USER_HOME', ''), \"src\")\n",
    "    if HOME_SRC not in sys.path:\n",
    "        sys.path.append(HOME_SRC)\n",
    "\n",
    "\n",
    "    setting_bdp(app_name=app_name, min_n_executors = min_n_executors, max_n_executors = max_n_executors, n_cores = n_cores, executor_memory = executor_memory, driver_memory=driver_memory)\n",
    "    from common.src.main.python.utils.hdfs_generic import run_sc\n",
    "    sc, spark, sql_context = run_sc(log_level=log_level)\n",
    "\n",
    "\n",
    "    return sc, spark, sql_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(app_name, min_n_executors = 3, max_n_executors = 15, n_cores = 4, executor_memory = \"12g\", driver_memory=\"8g\"):\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"_initialize spark\")\n",
    "    #import pykhaos.utils.pyspark_configuration as pyspark_config\n",
    "    sc, spark, sql_context = get_spark_session(app_name=app_name, log_level=\"OFF\", min_n_executors = min_n_executors, max_n_executors = max_n_executors, n_cores = n_cores,\n",
    "                             executor_memory = executor_memory, driver_memory=driver_memory)\n",
    "    print(\"Ended spark session: {} secs | default parallelism={}\".format(time.time() - start_time,\n",
    "                                                                         sc.defaultParallelism))\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_initialize spark\n",
      "Ended spark session: 35.352255106 secs | default parallelism=2\n"
     ]
    }
   ],
   "source": [
    "spark = initialize(\"FBB Improvement \",executor_memory = \"32g\",min_n_executors = 6,max_n_executors = 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "selcols = getIdFeats() + getCrmFeats() + getBillingFeats() + getMobSopoFeats() + getOrdersFeats()\n",
    "now = datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_name = str('20190328')#str(now.year) + str(now.month).rjust(2, '0') + str(now.day).rjust(2, '0')\n",
    "origin = '/user/hive/warehouse/tests_es.db/jvmm_amdocs_ids_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fecha inicial va a ser un ciclo\n",
    "# Fecha final es la fecha inicial+4 ciclos\n",
    "trcycle_ini = '20181130'  #Training data\n",
    "horizon = 4 #En ciclos\n",
    "ttcycle_ini='20181231' #Test data\n",
    "tr_ttdates = trcycle_ini + '_' + ttcycle_ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. TRAINING DATA\n",
    "# 1.1. Loading training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info getCarNumClienteDf] Tue Jul 16 12:10:43 2019 Size of the original df for 20181130: 13248445\n",
      "[Info FbbChurn] Tue Jul 16 12:15:31 2019 Saving origindf_tmp to HDFS\n",
      "[Info getCarNumClienteDf] Tue Jul 16 12:15:32 2019 Size of the origindf after load from HDFS: 13248445\n",
      "[Info getCarNumClienteDf] Tue Jul 16 12:16:44 2019 Size of the numclidf for 20181130 - Num rows: 13248445 - Num columns: 101\n",
      "[Info FbbChurn] Tue Jul 16 12:20:02 2019 Saving numclidf_tmp to HDFS\n",
      "[Info get_billing_df] Tue Jul 16 12:20:03 2019 Starting the preparation of billing feats\n",
      "[Info get_billing_df] Tue Jul 16 12:20:12 2019 Size of the df with billing feats: 5714359\n",
      "[Info getCarNumClienteDf] Tue Jul 16 12:20:16 2019 Size of the tmpdf for 20181130 - Num rows: 13248445 - Num columns: 134\n",
      "[Info get_mobile_spinners_df] Tue Jul 16 12:20:27 2019 Port-out data loaded for 20181130 with a total of 27468495 rows and 11448545 distinct NIFs\n",
      "[Info get_mobile_spinners_df] Tue Jul 16 12:20:33 2019 Timing feats computed for a total of 11448545 distinct NIFs\n",
      "[Info get_mobile_spinners_df] Tue Jul 16 12:20:46 2019 Destinatination and state feats computed for a total of 11448545 distinct NIFs\n",
      "[Info getCarNumClienteDf] Tue Jul 16 12:21:05 2019 Size of the featdf for 20181130 - Num rows: 13248445 - Num columns: 236\n",
      "[Info get_orders_df] Tue Jul 16 12:21:38 2019 Orders feats up to 20181031 computed - Number of rows is 8150066 for a total of 8150066 distinct num_cliente\n",
      "[Info getCarNumClienteDf] Tue Jul 16 12:21:50 2019 Size of the featdf for 20181130 - Num rows: 13248445 - Num columns: 238\n",
      "[Info getFbbChurnLabeledCar] Tue Jul 16 12:22:50 2019 Samples for month 20181130: 13248445\n",
      "20181130\n",
      "20181231\n",
      "[Info getFixPortRequestsForMonth] Tue Jul 16 12:23:20 2019 Port-out requests for fixed services during period 20181130-20181231: 47422\n",
      "[Info getFbbDxsForMonth] Tue Jul 16 12:23:33 2019 DXs for FBB services during the period: 20181130-20181231: 63345\n",
      "[Info getFbbChurnLabeledCar] Tue Jul 16 12:24:13 2019 Labeled samples for month 201811: 1978186\n"
     ]
    }
   ],
   "source": [
    "#inittrdf_ini = getFbbChurnUnlabeledCar(spark, origin,trcycle_ini ,selcols)\n",
    "\n",
    "df = getFbbChurnLabeledCarCycles2(spark, origin, trcycle_ini, selcols, horizon = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- num_cliente: string (nullable = true)\n",
      " |-- nif_cliente: string (nullable = true)\n",
      " |-- rgu: string (nullable = true)\n",
      " |-- campo1: string (nullable = true)\n",
      " |-- msisdn: string (nullable = true)\n",
      " |-- msisdn_d: string (nullable = true)\n",
      " |-- mean_dias_desde_fx_srv_basic: double (nullable = false)\n",
      " |-- movil_services: long (nullable = true)\n",
      " |-- tv_services: long (nullable = true)\n",
      " |-- total_penal_cust_pending_n3_penal_amount: double (nullable = false)\n",
      " |-- x_formato_factura: string (nullable = true)\n",
      " |-- x_user_twitter: string (nullable = true)\n",
      " |-- total_penal_cust_pending_n4_penal_amount: double (nullable = false)\n",
      " |-- dias_desde_bam-movil_fx_first: double (nullable = false)\n",
      " |-- max_dias_desde_fx_trybuy_autom_tv: double (nullable = false)\n",
      " |-- total_price_tariff: double (nullable = false)\n",
      " |-- flg_robinson: string (nullable = true)\n",
      " |-- pvr_services: double (nullable = false)\n",
      " |-- num_tariff_redl: double (nullable = false)\n",
      " |-- dias_desde_fx_fbb_upgrade: double (nullable = false)\n",
      " |-- max_dias_desde_fx_srv_basic: double (nullable = false)\n",
      " |-- total_price_trybuy: double (nullable = false)\n",
      " |-- total_penal_srv_pending_n4_penal_amount: double (nullable = false)\n",
      " |-- max_dias_desde_fx_pvr_tv: double (nullable = false)\n",
      " |-- max_dias_desde_fx_football_tv: double (nullable = false)\n",
      " |-- total_max_dias_hasta_penal_cust_pending_end_date: double (nullable = false)\n",
      " |-- trybuy_services: double (nullable = false)\n",
      " |-- min_price_srv_basic: double (nullable = false)\n",
      " |-- total_penal_cust_pending_n2_penal_amount: double (nullable = false)\n",
      " |-- x_datos_trafico: string (nullable = true)\n",
      " |-- total_min_dias_hasta_penal_srv_pending_end_date: double (nullable = false)\n",
      " |-- cta_correo: string (nullable = true)\n",
      " |-- gender2hgbst_elm: string (nullable = true)\n",
      " |-- dias_desde_fixed_fx_first: double (nullable = false)\n",
      " |-- max_price_srv_basic: double (nullable = false)\n",
      " |-- total_penal_srv_pending_n5_penal_amount: double (nullable = false)\n",
      " |-- prepaid_services: long (nullable = true)\n",
      " |-- hz_services: double (nullable = false)\n",
      " |-- dias_desde_tv_fx_first: double (nullable = false)\n",
      " |-- dias_desde_movil_fx_first: double (nullable = false)\n",
      " |-- total_penal_srv_pending_n1_penal_amount: double (nullable = false)\n",
      " |-- total_max_dias_hasta_penal_srv_pending_end_date: double (nullable = false)\n",
      " |-- num_tariff_xs: double (nullable = false)\n",
      " |-- total_price_motor: double (nullable = false)\n",
      " |-- x_user_facebook: string (nullable = true)\n",
      " |-- fixed_services: long (nullable = true)\n",
      " |-- fbb_upgrade: string (nullable = true)\n",
      " |-- max_dias_desde_fx_zapper_tv: double (nullable = false)\n",
      " |-- total_penal_srv_pending_n2_penal_amount: double (nullable = false)\n",
      " |-- factura_electronica: string (nullable = true)\n",
      " |-- num_tariff_plana200min: double (nullable = false)\n",
      " |-- max_dias_desde_fx_trybuy_tv: double (nullable = false)\n",
      " |-- min_price_tariff: double (nullable = false)\n",
      " |-- bam-movil_services: long (nullable = true)\n",
      " |-- total_penal_srv_pending_n3_penal_amount: double (nullable = false)\n",
      " |-- num_tariff_unknown: double (nullable = false)\n",
      " |-- total_min_dias_hasta_penal_cust_pending_end_date: double (nullable = false)\n",
      " |-- x_cesion_datos: string (nullable = true)\n",
      " |-- football_services: double (nullable = false)\n",
      " |-- trybuy_autom_services: double (nullable = false)\n",
      " |-- total_price_zapper: double (nullable = false)\n",
      " |-- marriage2hgbst_elm: string (nullable = true)\n",
      " |-- total_price_pvr: double (nullable = false)\n",
      " |-- num_2lins: double (nullable = false)\n",
      " |-- fbb_services: long (nullable = true)\n",
      " |-- num_tariff_smart: double (nullable = false)\n",
      " |-- x_publicidad_email: string (nullable = true)\n",
      " |-- cliente_migrado: string (nullable = true)\n",
      " |-- x_idioma_factura: string (nullable = true)\n",
      " |-- num_tariff_otros: double (nullable = false)\n",
      " |-- metodo_pago: string (nullable = true)\n",
      " |-- max_price_tariff: double (nullable = false)\n",
      " |-- bam_services: long (nullable = true)\n",
      " |-- total_price_dto_lev2: double (nullable = false)\n",
      " |-- superoferta: string (nullable = true)\n",
      " |-- total_price_dto_lev1: double (nullable = false)\n",
      " |-- total_penal_cust_pending_n5_penal_amount: double (nullable = false)\n",
      " |-- total_price_football: double (nullable = false)\n",
      " |-- x_antiguedad_cuenta: string (nullable = true)\n",
      " |-- dias_desde_prepaid_fx_first: double (nullable = false)\n",
      " |-- total_tv_total_charges: double (nullable = false)\n",
      " |-- num_tariff_minim: double (nullable = false)\n",
      " |-- dias_desde_fbb_fx_first: double (nullable = false)\n",
      " |-- mean_price_srv_basic: double (nullable = false)\n",
      " |-- dias_desde_bam_fx_first: double (nullable = false)\n",
      " |-- zapper_services: double (nullable = false)\n",
      " |-- motor_services: double (nullable = false)\n",
      " |-- num_tariff_maslineasmini: double (nullable = false)\n",
      " |-- num_tariff_redm: double (nullable = false)\n",
      " |-- total_price_srv_basic: double (nullable = false)\n",
      " |-- nacionalidad: string (nullable = true)\n",
      " |-- total_price_trybuy_autom: double (nullable = false)\n",
      " |-- max_dias_desde_fx_motor_tv: double (nullable = false)\n",
      " |-- min_dias_desde_fx_dto_lev1: double (nullable = false)\n",
      " |-- min_dias_desde_fx_dto_lev2: double (nullable = false)\n",
      " |-- num_tariff_planaminilim: double (nullable = false)\n",
      " |-- total_penal_cust_pending_n1_penal_amount: double (nullable = false)\n",
      " |-- tipo_documento: string (nullable = true)\n",
      " |-- x_datos_navegacion: string (nullable = true)\n",
      " |-- min_dias_desde_fx_srv_basic: double (nullable = false)\n",
      " |-- num_tariff_megayuser: double (nullable = false)\n",
      " |-- Bill_N1_InvoiceCharges: double (nullable = false)\n",
      " |-- Bill_N1_Amount_To_Pay: double (nullable = false)\n",
      " |-- Bill_N1_Tax_Amount: double (nullable = false)\n",
      " |-- Bill_N1_Debt_Amount: double (nullable = false)\n",
      " |-- Bill_N2_InvoiceCharges: double (nullable = false)\n",
      " |-- Bill_N2_Amount_To_Pay: double (nullable = false)\n",
      " |-- Bill_N2_Tax_Amount: double (nullable = false)\n",
      " |-- Bill_N2_Debt_Amount: double (nullable = false)\n",
      " |-- Bill_N3_InvoiceCharges: double (nullable = false)\n",
      " |-- Bill_N3_Amount_To_Pay: double (nullable = false)\n",
      " |-- Bill_N3_Tax_Amount: double (nullable = false)\n",
      " |-- Bill_N3_Debt_Amount: double (nullable = false)\n",
      " |-- Bill_N4_InvoiceCharges: double (nullable = false)\n",
      " |-- Bill_N4_Amount_To_Pay: double (nullable = false)\n",
      " |-- Bill_N4_Tax_Amount: double (nullable = false)\n",
      " |-- Bill_N4_Debt_Amount: double (nullable = false)\n",
      " |-- Bill_N5_InvoiceCharges: double (nullable = false)\n",
      " |-- Bill_N5_Amount_To_Pay: double (nullable = false)\n",
      " |-- Bill_N5_Tax_Amount: double (nullable = false)\n",
      " |-- Bill_N5_Debt_Amount: double (nullable = false)\n",
      " |-- bill_n1_net: double (nullable = false)\n",
      " |-- bill_n2_net: double (nullable = false)\n",
      " |-- bill_n3_net: double (nullable = false)\n",
      " |-- bill_n4_net: double (nullable = false)\n",
      " |-- bill_n5_net: double (nullable = false)\n",
      " |-- inc_bill_n1_n2_net: double (nullable = false)\n",
      " |-- inc_bill_n1_n3_net: double (nullable = false)\n",
      " |-- inc_bill_n1_n4_net: double (nullable = false)\n",
      " |-- inc_bill_n1_n5_net: double (nullable = false)\n",
      " |-- inc_Bill_N1_N2_Amount_To_Pay: double (nullable = false)\n",
      " |-- inc_Bill_N1_N3_Amount_To_Pay: double (nullable = false)\n",
      " |-- inc_Bill_N1_N4_Amount_To_Pay: double (nullable = false)\n",
      " |-- inc_Bill_N1_N5_Amount_To_Pay: double (nullable = false)\n",
      " |-- nif_port_number: long (nullable = true)\n",
      " |-- nif_min_days_since_port: double (nullable = false)\n",
      " |-- nif_max_days_since_port: double (nullable = false)\n",
      " |-- nif_avg_days_since_port: double (nullable = false)\n",
      " |-- nif_var_days_since_port: double (nullable = false)\n",
      " |-- nif_distinct_msisdn: long (nullable = true)\n",
      " |-- nif_port_freq_per_day: double (nullable = false)\n",
      " |-- nif_port_freq_per_msisdn: double (nullable = false)\n",
      " |-- movistar_ACON: long (nullable = true)\n",
      " |-- movistar_ASOL: long (nullable = true)\n",
      " |-- movistar_PCAN: long (nullable = true)\n",
      " |-- movistar_ACAN: long (nullable = true)\n",
      " |-- movistar_AACE: long (nullable = true)\n",
      " |-- movistar_AENV: long (nullable = true)\n",
      " |-- movistar_APOR: long (nullable = true)\n",
      " |-- movistar_AREC: long (nullable = true)\n",
      " |-- simyo_ACON: long (nullable = true)\n",
      " |-- simyo_ASOL: long (nullable = true)\n",
      " |-- simyo_PCAN: long (nullable = true)\n",
      " |-- simyo_ACAN: long (nullable = true)\n",
      " |-- simyo_AACE: long (nullable = true)\n",
      " |-- simyo_AENV: long (nullable = true)\n",
      " |-- simyo_APOR: long (nullable = true)\n",
      " |-- simyo_AREC: long (nullable = true)\n",
      " |-- orange_ACON: long (nullable = true)\n",
      " |-- orange_ASOL: long (nullable = true)\n",
      " |-- orange_PCAN: long (nullable = true)\n",
      " |-- orange_ACAN: long (nullable = true)\n",
      " |-- orange_AACE: long (nullable = true)\n",
      " |-- orange_AENV: long (nullable = true)\n",
      " |-- orange_APOR: long (nullable = true)\n",
      " |-- orange_AREC: long (nullable = true)\n",
      " |-- jazztel_ACON: long (nullable = true)\n",
      " |-- jazztel_ASOL: long (nullable = true)\n",
      " |-- jazztel_PCAN: long (nullable = true)\n",
      " |-- jazztel_ACAN: long (nullable = true)\n",
      " |-- jazztel_AACE: long (nullable = true)\n",
      " |-- jazztel_AENV: long (nullable = true)\n",
      " |-- jazztel_APOR: long (nullable = true)\n",
      " |-- jazztel_AREC: long (nullable = true)\n",
      " |-- yoigo_ACON: long (nullable = true)\n",
      " |-- yoigo_ASOL: long (nullable = true)\n",
      " |-- yoigo_PCAN: long (nullable = true)\n",
      " |-- yoigo_ACAN: long (nullable = true)\n",
      " |-- yoigo_AACE: long (nullable = true)\n",
      " |-- yoigo_AENV: long (nullable = true)\n",
      " |-- yoigo_APOR: long (nullable = true)\n",
      " |-- yoigo_AREC: long (nullable = true)\n",
      " |-- masmovil_ACON: long (nullable = true)\n",
      " |-- masmovil_ASOL: long (nullable = true)\n",
      " |-- masmovil_PCAN: long (nullable = true)\n",
      " |-- masmovil_ACAN: long (nullable = true)\n",
      " |-- masmovil_AACE: long (nullable = true)\n",
      " |-- masmovil_AENV: long (nullable = true)\n",
      " |-- masmovil_APOR: long (nullable = true)\n",
      " |-- masmovil_AREC: long (nullable = true)\n",
      " |-- pepephone_ACON: long (nullable = true)\n",
      " |-- pepephone_ASOL: long (nullable = true)\n",
      " |-- pepephone_PCAN: long (nullable = true)\n",
      " |-- pepephone_ACAN: long (nullable = true)\n",
      " |-- pepephone_AACE: long (nullable = true)\n",
      " |-- pepephone_AENV: long (nullable = true)\n",
      " |-- pepephone_APOR: long (nullable = true)\n",
      " |-- pepephone_AREC: long (nullable = true)\n",
      " |-- reuskal_ACON: long (nullable = true)\n",
      " |-- reuskal_ASOL: long (nullable = true)\n",
      " |-- reuskal_PCAN: long (nullable = true)\n",
      " |-- reuskal_ACAN: long (nullable = true)\n",
      " |-- reuskal_AACE: long (nullable = true)\n",
      " |-- reuskal_AENV: long (nullable = true)\n",
      " |-- reuskal_APOR: long (nullable = true)\n",
      " |-- reuskal_AREC: long (nullable = true)\n",
      " |-- unknown_ACON: long (nullable = true)\n",
      " |-- unknown_ASOL: long (nullable = true)\n",
      " |-- unknown_PCAN: long (nullable = true)\n",
      " |-- unknown_ACAN: long (nullable = true)\n",
      " |-- unknown_AACE: long (nullable = true)\n",
      " |-- unknown_AENV: long (nullable = true)\n",
      " |-- unknown_APOR: long (nullable = true)\n",
      " |-- unknown_AREC: long (nullable = true)\n",
      " |-- otros_ACON: long (nullable = true)\n",
      " |-- otros_ASOL: long (nullable = true)\n",
      " |-- otros_PCAN: long (nullable = true)\n",
      " |-- otros_ACAN: long (nullable = true)\n",
      " |-- otros_AACE: long (nullable = true)\n",
      " |-- otros_AENV: long (nullable = true)\n",
      " |-- otros_APOR: long (nullable = true)\n",
      " |-- otros_AREC: long (nullable = true)\n",
      " |-- total_acan: long (nullable = true)\n",
      " |-- total_apor: long (nullable = true)\n",
      " |-- total_arec: long (nullable = true)\n",
      " |-- total_movistar: long (nullable = true)\n",
      " |-- total_simyo: long (nullable = true)\n",
      " |-- total_orange: long (nullable = true)\n",
      " |-- total_jazztel: long (nullable = true)\n",
      " |-- total_yoigo: long (nullable = true)\n",
      " |-- total_masmovil: long (nullable = true)\n",
      " |-- total_pepephone: long (nullable = true)\n",
      " |-- total_reuskal: long (nullable = true)\n",
      " |-- total_unknown: long (nullable = true)\n",
      " |-- total_otros: long (nullable = true)\n",
      " |-- num_distinct_operators: double (nullable = false)\n",
      " |-- days_since_last_order: integer (nullable = false)\n",
      " |-- days_since_first_order: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inittrdf_ini.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Añadimos extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Añadimos las extra features (lo hacemos aquí fuera porque ya están filtradas por rgu)\n",
    "dfExtraFeat = spark.read.parquet('/data/udf/vf_es/churn/extra_feats_mod/extra_feats/year={}/month={}/day={}'\n",
    "                                            .format(int(trcycle_ini[0:4]), int(trcycle_ini[4:6]), int(trcycle_ini[6:8])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfExtraFeatfbb = dfExtraFeat.join(df, [\"num_cliente\"], \"leftsemi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[Info FbbChurn] Tue Jul 16 12:25:25 2019 The module for the Extra Feats has been run. Count for the table', 1978186)\n"
     ]
    }
   ],
   "source": [
    "dfExtraFeatSel,selColumnas=addExtraFeats(dfExtraFeatfbb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfillNa = fillNa(spark)\n",
    "for kkey in dfillNa.keys():\n",
    "    if kkey not in dfExtraFeatSel.columns:\n",
    "        dfillNa.pop(kkey, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "inittrdf = df.join(dfExtraFeatSel, [\"msisdn\",\"num_cliente\",'rgu'], how=\"left\").na.fill(dfillNa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1978186"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inittrdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92, 238, 327)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfExtraFeatSel.columns),len(inittrdf_ini.columns),len(inittrdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 4\n",
    "yearmonthday = '20181130'\n",
    "cycle = 0\n",
    "fini_tmp = yearmonthday\n",
    "while cycle < horizon:\n",
    "    yearmonthday_target = get_next_cycle(fini_tmp, str_fmt=\"%Y%m%d\")\n",
    "    cycle = cycle + 1\n",
    "    fini_tmp = yearmonthday_target\n",
    "\n",
    "yearmonth = yearmonthday[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20181130\n",
      "20181231\n",
      "[Info getFixPortRequestsForMonth] Tue Jul 16 11:06:30 2019 Port-out requests for fixed services during period 20181130-20181231: 47422\n",
      "[Info getFbbDxsForMonth] Tue Jul 16 11:06:43 2019 DXs for FBB services during the period: 20181130-20181231: 63345\n"
     ]
    }
   ],
   "source": [
    "fixporttr = getFixPortRequestsForCycleList(spark, yearmonthday, yearmonthday_target)\n",
    "fixdxtr = getFbbDxsForCycleList(spark,yearmonthday, yearmonthday_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFbbChurnLabeledCarCycles2(spark, origin, yearmonthday, selcols, horizon = 4):\n",
    "\n",
    "    cycle = 0\n",
    "    fini_tmp = yearmonthday\n",
    "    while cycle < horizon:\n",
    "        yearmonthday_target = get_next_cycle(fini_tmp, str_fmt=\"%Y%m%d\")\n",
    "        cycle = cycle + 1\n",
    "        fini_tmp = yearmonthday_target\n",
    "\n",
    "    yearmonth = yearmonthday[0:6]\n",
    "\n",
    "    trfeatdf = getCarNumClienteDf(spark, origin, yearmonthday)\n",
    "\n",
    "    print(\"[Info getFbbChurnLabeledCar] \" + time.ctime() + \" Samples for month \" + yearmonthday + \": \" + str(trfeatdf.count()))\n",
    "\n",
    "    # Loading port-out requests and DXs\n",
    "    # # labmonthlisttr = getMonthSeq(initportmonthtr, lastportmonthtr)\n",
    "\n",
    "    # Las bajas de fibra pueden venir por:\n",
    "    #- Solicitudes de baja de fijo\n",
    "    fixporttr = getFixPortRequestsForCycleList(spark, yearmonthday, yearmonthday_target)\n",
    "    #- Porque dejen de estar en la lista de clientes\n",
    "    fixdxtr = getFbbDxsForCycleList(spark,yearmonthday, yearmonthday_target)\n",
    "\n",
    "    # Labeling: FBB service is labeled as 1 if, during the next time window specified by the horizon, either the associated fixed service requested to be ported out or the FBB was disconnected\n",
    "    window = Window.partitionBy(\"num_cliente\")\n",
    "\n",
    "    unbaltrdf = trfeatdf\\\n",
    "    .join(fixporttr, ['msisdn_d'], \"left_outer\")\\\n",
    "    .na.fill({'label_srv': 0.0})\\\n",
    "    .join(fixdxtr, ['msisdn'], \"left_outer\")\\\n",
    "    .na.fill({'label_dx': 0.0})\\\n",
    "    .withColumn('tmp', when((col('label_srv')==1.0) | (col('label_dx')==1.0), 1.0).otherwise(0.0))\\\n",
    "    .withColumn('label', sql_max('tmp').over(window))\\\n",
    "    .filter(col(\"rgu\")==\"fbb\")\\\n",
    "    .select(selcols + ['label'])\n",
    "\n",
    "    print(\"[Info getFbbChurnLabeledCar] \" + time.ctime() + \" Labeled samples for month \" + yearmonth + \": \" + str(unbaltrdf.count()))\n",
    "\n",
    "    return unbaltrdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFbbChurnLabeledCarCycles3(spark, origin, yearmonthday, selcols, horizon = 4):\n",
    "\n",
    "    cycle = 0\n",
    "    fini_tmp = yearmonthday\n",
    "    while cycle < horizon:\n",
    "        yearmonthday_target = get_next_cycle(fini_tmp, str_fmt=\"%Y%m%d\")\n",
    "        cycle = cycle + 1\n",
    "        fini_tmp = yearmonthday_target\n",
    "\n",
    "    yearmonth = yearmonthday[0:6]\n",
    "\n",
    "    trfeatdf = getCarNumClienteDf(spark, origin, yearmonthday)\n",
    "\n",
    "    print(\"[Info getFbbChurnLabeledCar] \" + time.ctime() + \" Samples for month \" + yearmonthday + \": \" + str(trfeatdf.count()))\n",
    "\n",
    "    # Loading port-out requests and DXs\n",
    "    # # labmonthlisttr = getMonthSeq(initportmonthtr, lastportmonthtr)\n",
    "\n",
    "    # Las bajas de fibra pueden venir por:\n",
    "    #- Solicitudes de baja de fijo\n",
    "    fixporttr = getFixPortRequestsForCycleList(spark, yearmonthday, yearmonthday_target)\n",
    "    #- Porque dejen de estar en la lista de clientes\n",
    "    fixdxtr = getFbbDxsForCycleList(spark,yearmonthday, yearmonthday_target)\n",
    "\n",
    "    # Labeling: FBB service is labeled as 1 if, during the next time window specified by the horizon, either the associated fixed service requested to be ported out or the FBB was disconnected\n",
    "    window = Window.partitionBy(\"num_cliente\")\n",
    "\n",
    "    unbaltrdf = trfeatdf\\\n",
    "    .join(fixporttr, ['msisdn_d'], \"left_outer\")\\\n",
    "    .na.fill({'label_srv': 0.0})\\\n",
    "    .join(fixdxtr, ['msisdn'], \"left_outer\")\\\n",
    "    .na.fill({'label_dx': 0.0})\\\n",
    "    .withColumn('tmp', when((col('label_dx')==1.0), 1.0).otherwise(0.0))\\\n",
    "    .withColumn('label', sql_max('tmp').over(window))\\\n",
    "    .filter(col(\"rgu\")==\"fbb\")\\\n",
    "    .select(selcols + ['label'])\n",
    "\n",
    "    print(\"[Info getFbbChurnLabeledCar] \" + time.ctime() + \" Labeled samples for month \" + yearmonth + \": \" + str(unbaltrdf.count()))\n",
    "\n",
    "    return unbaltrdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13248445"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inittrdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|label|count(1)|\n",
      "+-----+--------+\n",
      "|  0.0| 1956543|\n",
      "|  1.0|   21643|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inittrdf.groupBy('label').agg(count('*')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "[unbaltrdf, valdf] = inittrdf.randomSplit([0.7, 0.3], 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(327, 238)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inittrdf.columns), len(trfeatdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1385997"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unbaltrdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2. Balanced df for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info balance_df2] Dataframe has been balanced - Total number of rows is 30459\n"
     ]
    }
   ],
   "source": [
    "trdf = balance_df2(unbaltrdf, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|label|count(1)|\n",
      "+-----+--------+\n",
      "|  0.0|   15306|\n",
      "|  1.0|   15153|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trdf.groupBy('label').agg(count('*')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info FbbChurn] 1/2 Selecting the features that add information to the model\n",
      "[Info FbbChurn] 2/2 Selecting the features that add information to the model\n"
     ]
    }
   ],
   "source": [
    "allFeats = trdf.columns\n",
    "\n",
    "# Getting only the numeric variables\n",
    "catCols = [item[0] for item in trdf.dtypes if item[1].startswith('string')]\n",
    "numerical_feats = list(set(allFeats) - set(list(\n",
    "        set().union(getIdFeats(), getIdFeats_tr(), getNoInputFeats(), catCols, [c + \"_enc\" for c in getCatFeatsCrm()],\n",
    "                    [\"label\"]))))\n",
    "\n",
    "noninf_feats = getNonInfFeats(trdf, numerical_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "featCols = list(set(numerical_feats) - set(noninf_feats))\n",
    "\n",
    "assembler = VectorAssembler(inputCols=featCols, outputCol=\"features\")\n",
    "\n",
    "classifier = RandomForestClassifier(featuresCol=\"features\", \\\n",
    "                                        labelCol=\"label\", \\\n",
    "                                        maxDepth=20, \\\n",
    "                                        maxBins=32, \\\n",
    "                                        minInstancesPerNode=100, \\\n",
    "                                        impurity=\"entropy\", \\\n",
    "                                        featureSubsetStrategy=\"sqrt\", \\\n",
    "                                        subsamplingRate=0.85, minInfoGain = 0.001, \\\n",
    "                                        numTrees=800, \\\n",
    "                                        seed=1234)  \n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, classifier])\n",
    "model = pipeline.fit(trdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info Calibration] Training - Num samples in the target class 6490.0\n",
      "[Info Calibration] Training 0.140616770625, 0.00169491525424\n",
      "[Info Calibration] Training 0.223732049807, 0.00832049306626\n",
      "[Info Calibration] Training 0.306847328989, 0.0281972265023\n",
      "[Info Calibration] Training 0.389962608171, 0.0790446841294\n",
      "[Info Calibration] Training 0.473077887353, 0.217257318952\n",
      "[Info Calibration] Training 0.556193166535, 0.420647149461\n",
      "[Info Calibration] Training 0.639308445718, 0.645762711864\n",
      "[Info Calibration] Training 0.7224237249, 0.862095531587\n",
      "[Info Calibration] Training 0.805539004082, 0.985824345146\n",
      "+-------------------+--------------------+\n",
      "|        model_score|         target_prob|\n",
      "+-------------------+--------------------+\n",
      "| 0.1406167706248943|0.001694915254237288|\n",
      "| 0.2237320498070065|0.008320493066255779|\n",
      "|0.30684732898911865|0.028197226502311247|\n",
      "|0.38996260817123085| 0.07904468412942989|\n",
      "|0.47307788735334305|  0.2172573189522342|\n",
      "| 0.5561931665354553|  0.4206471494607088|\n",
      "| 0.6393084457175675|  0.6457627118644068|\n",
      "| 0.7224237248996797|   0.862095531587057|\n",
      "| 0.8055390040817918|  0.9858243451463791|\n",
      "+-------------------+--------------------+\n",
      "\n",
      "[Info Calibration] Samples to implement the calibration model showed above\n"
     ]
    }
   ],
   "source": [
    "calibmodel = get_calibration_function2(spark, model, valdf, 'label', 10)\n",
    "\n",
    "getScore = udf(lambda prob: float(prob[1]), DoubleType())\n",
    "\n",
    "# Train\n",
    "tr_preds_df = model.transform(trdf).withColumn(\"model_score\", getScore(col(\"probability\")).cast(DoubleType()))\n",
    "tr_calib_preds_df = calibmodel[0].transform(tr_preds_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "trPredictionAndLabels = tr_calib_preds_df.select(['calib_model_score', 'label']).rdd.map(lambda r: (r['calib_model_score'], r['label']))\n",
    "trmetrics = BinaryClassificationMetrics(trPredictionAndLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8081188067089619"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trmetrics.areaUnderROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info getCarNumClienteDf] Tue Jul 16 15:01:39 2019 Size of the original df for 20181130: 13248445\n",
      "[Info FbbChurn] Tue Jul 16 15:05:26 2019 Saving origindf_tmp to HDFS\n",
      "[Info getCarNumClienteDf] Tue Jul 16 15:05:27 2019 Size of the origindf after load from HDFS: 13248445\n",
      "[Info getCarNumClienteDf] Tue Jul 16 15:06:39 2019 Size of the numclidf for 20181130 - Num rows: 13248445 - Num columns: 101\n",
      "[Info FbbChurn] Tue Jul 16 15:09:26 2019 Saving numclidf_tmp to HDFS\n",
      "[Info get_billing_df] Tue Jul 16 15:09:26 2019 Starting the preparation of billing feats\n"
     ]
    }
   ],
   "source": [
    "df2 = getFbbChurnLabeledCarCycles3(spark, origin, trcycle_ini, selcols, horizon = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfExtraFeatfbb = dfExtraFeat.join(df2, [\"num_cliente\"], \"leftsemi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[Info FbbChurn] Wed Jul 17 07:49:23 2019 The module for the Extra Feats has been run. Count for the table', 1978186)\n"
     ]
    }
   ],
   "source": [
    "dfExtraFeatSel,selColumnas=addExtraFeats(dfExtraFeatfbb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfillNa = fillNa(spark)\n",
    "for kkey in dfillNa.keys():\n",
    "    if kkey not in dfExtraFeatSel.columns:\n",
    "        dfillNa.pop(kkey, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "inittrdf = df2.join(dfExtraFeatSel, [\"msisdn\",\"num_cliente\",'rgu'], how=\"left\").na.fill(dfillNa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1978186"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inittrdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|label|count(1)|\n",
      "+-----+--------+\n",
      "|  0.0| 1914837|\n",
      "|  1.0|   63349|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inittrdf.groupBy('label').agg(count('*')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "[unbaltrdf, valdf] = inittrdf.randomSplit([0.7, 0.3], 1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1385871"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unbaltrdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info balance_df2] Dataframe has been balanced - Total number of rows is 88454\n"
     ]
    }
   ],
   "source": [
    "trdf = balance_df2(unbaltrdf, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|label|count(1)|\n",
      "+-----+--------+\n",
      "|  0.0|   44175|\n",
      "|  1.0|   44279|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trdf.groupBy('label').agg(count('*')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info FbbChurn] 1/2 Selecting the features that add information to the model\n",
      "[Info FbbChurn] 2/2 Selecting the features that add information to the model\n"
     ]
    }
   ],
   "source": [
    "allFeats = trdf.columns\n",
    "\n",
    "# Getting only the numeric variables\n",
    "catCols = [item[0] for item in trdf.dtypes if item[1].startswith('string')]\n",
    "numerical_feats = list(set(allFeats) - set(list(\n",
    "        set().union(getIdFeats(), getIdFeats_tr(), getNoInputFeats(), catCols, [c + \"_enc\" for c in getCatFeatsCrm()],\n",
    "                    [\"label\"]))))\n",
    "\n",
    "noninf_feats = getNonInfFeats(trdf, numerical_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "featCols = list(set(numerical_feats) - set(noninf_feats))\n",
    "\n",
    "assembler_dx = VectorAssembler(inputCols=featCols, outputCol=\"features\")\n",
    "\n",
    "classifier_dx = RandomForestClassifier(featuresCol=\"features\", \\\n",
    "                                        labelCol=\"label\", \\\n",
    "                                        maxDepth=20, \\\n",
    "                                        maxBins=32, \\\n",
    "                                        minInstancesPerNode=100, \\\n",
    "                                        impurity=\"entropy\", \\\n",
    "                                        featureSubsetStrategy=\"sqrt\", \\\n",
    "                                        subsamplingRate=0.85, minInfoGain = 0.001, \\\n",
    "                                        numTrees=800, \\\n",
    "                                        seed=1234)  \n",
    "\n",
    "pipeline_dx = Pipeline(stages=[assembler_dx, classifier_dx])\n",
    "model_dx = pipeline_dx.fit(trdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info Calibration] Training - Num samples in the target class 19070.0\n",
      "[Info Calibration] Training 0.154809694588, 0.00209753539591\n",
      "[Info Calibration] Training 0.245802397268, 0.0149973780808\n",
      "[Info Calibration] Training 0.336795099947, 0.085631882538\n",
      "[Info Calibration] Training 0.427787802627, 0.218668065024\n",
      "[Info Calibration] Training 0.518780505306, 0.3843733613\n",
      "[Info Calibration] Training 0.609773207986, 0.577608809649\n",
      "[Info Calibration] Training 0.700765910665, 0.737021499738\n",
      "[Info Calibration] Training 0.791758613345, 0.8642370215\n",
      "[Info Calibration] Training 0.882751316024, 0.940010487677\n",
      "+-------------------+--------------------+\n",
      "|        model_score|         target_prob|\n",
      "+-------------------+--------------------+\n",
      "|0.15480969458839328|0.002097535395909806|\n",
      "|0.24580239726784703|0.014997378080755113|\n",
      "| 0.3367950999473008| 0.08563188253801783|\n",
      "|0.42778780262675453| 0.21866806502359729|\n",
      "| 0.5187805053062082| 0.38437336130047195|\n",
      "|  0.609773207985662|  0.5776088096486628|\n",
      "| 0.7007659106651158|  0.7370214997378081|\n",
      "| 0.7917586133445695|  0.8642370214997378|\n",
      "| 0.8827513160240232|  0.9400104876769796|\n",
      "+-------------------+--------------------+\n",
      "\n",
      "[Info Calibration] Samples to implement the calibration model showed above\n"
     ]
    }
   ],
   "source": [
    "calibmodel_dx = get_calibration_function2(spark, model_dx, valdf, 'label', 10)\n",
    "\n",
    "getScore = udf(lambda prob: float(prob[1]), DoubleType())\n",
    "\n",
    "# Train\n",
    "tr_preds_df_dx = model_dx.transform(trdf).withColumn(\"model_score\", getScore(col(\"probability\")).cast(DoubleType()))\n",
    "tr_calib_preds_df_dx = calibmodel_dx[0].transform(tr_preds_df_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "trPredictionAndLabels_dx = tr_calib_preds_df_dx.select(['calib_model_score', 'label']).rdd.map(lambda r: (r['calib_model_score'], r['label']))\n",
    "trmetrics_dx = BinaryClassificationMetrics(trPredictionAndLabels_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7830164412662808"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trmetrics_dx.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info getCarNumClienteDf] Wed Jul 17 11:15:36 2019 Size of the original df for 20181231: 13683905\n",
      "[Info FbbChurn] Wed Jul 17 11:20:08 2019 Saving origindf_tmp to HDFS\n",
      "[Info getCarNumClienteDf] Wed Jul 17 11:20:08 2019 Size of the origindf after load from HDFS: 13683905\n",
      "[Info getCarNumClienteDf] Wed Jul 17 11:21:28 2019 Size of the numclidf for 20181231 - Num rows: 13683905 - Num columns: 101\n",
      "[Info FbbChurn] Wed Jul 17 11:24:59 2019 Saving numclidf_tmp to HDFS\n",
      "[Info get_billing_df] Wed Jul 17 11:24:59 2019 Starting the preparation of billing feats\n",
      "[Info get_billing_df] Wed Jul 17 11:25:35 2019 Size of the df with billing feats: 5825804\n",
      "[Info getCarNumClienteDf] Wed Jul 17 11:25:42 2019 Size of the tmpdf for 20181231 - Num rows: 13683905 - Num columns: 134\n",
      "[Info get_mobile_spinners_df] Wed Jul 17 11:25:54 2019 Port-out data loaded for 20181231 with a total of 27844643 rows and 11499038 distinct NIFs\n",
      "[Info get_mobile_spinners_df] Wed Jul 17 11:26:17 2019 Timing feats computed for a total of 11499038 distinct NIFs\n",
      "[Info get_mobile_spinners_df] Wed Jul 17 11:26:23 2019 Destinatination and state feats computed for a total of 11499038 distinct NIFs\n",
      "[Info getCarNumClienteDf] Wed Jul 17 11:26:32 2019 Size of the featdf for 20181231 - Num rows: 13683905 - Num columns: 236\n",
      "[Info get_orders_df] Wed Jul 17 11:28:08 2019 Orders feats up to 20181130 computed - Number of rows is 8725478 for a total of 8725478 distinct num_cliente\n",
      "[Info getCarNumClienteDf] Wed Jul 17 11:28:22 2019 Size of the featdf for 20181231 - Num rows: 13683905 - Num columns: 238\n",
      "[Info getFbbChurnLabeledCar] Wed Jul 17 11:30:04 2019 Samples for month 20181231: 13683905\n",
      "20181231\n",
      "20190131\n",
      "[Info getFixPortRequestsForMonth] Wed Jul 17 11:30:48 2019 Port-out requests for fixed services during period 20181231-20190131: 57686\n",
      "[Info getFbbDxsForMonth] Wed Jul 17 11:31:04 2019 DXs for FBB services during the period: 20181231-20190131: 71798\n",
      "[Info getFbbChurnLabeledCar] Wed Jul 17 11:31:40 2019 Labeled samples for month 201812: 2050838\n"
     ]
    }
   ],
   "source": [
    "df_test_dx = getFbbChurnLabeledCarCycles3(spark, origin, ttcycle_ini, selcols, horizon = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info getCarNumClienteDf] Wed Jul 17 11:35:30 2019 Size of the original df for 20181231: 13683905\n",
      "[Info FbbChurn] Wed Jul 17 11:41:47 2019 Saving origindf_tmp to HDFS\n",
      "[Info getCarNumClienteDf] Wed Jul 17 11:41:47 2019 Size of the origindf after load from HDFS: 13683905\n",
      "[Info getCarNumClienteDf] Wed Jul 17 11:43:02 2019 Size of the numclidf for 20181231 - Num rows: 13683905 - Num columns: 101\n",
      "[Info FbbChurn] Wed Jul 17 11:46:44 2019 Saving numclidf_tmp to HDFS\n",
      "[Info get_billing_df] Wed Jul 17 11:46:44 2019 Starting the preparation of billing feats\n",
      "[Info get_billing_df] Wed Jul 17 11:46:57 2019 Size of the df with billing feats: 5825804\n",
      "[Info getCarNumClienteDf] Wed Jul 17 11:47:07 2019 Size of the tmpdf for 20181231 - Num rows: 13683905 - Num columns: 134\n",
      "[Info get_mobile_spinners_df] Wed Jul 17 11:47:23 2019 Port-out data loaded for 20181231 with a total of 27844643 rows and 11499038 distinct NIFs\n",
      "[Info get_mobile_spinners_df] Wed Jul 17 11:47:31 2019 Timing feats computed for a total of 11499038 distinct NIFs\n",
      "[Info get_mobile_spinners_df] Wed Jul 17 11:47:44 2019 Destinatination and state feats computed for a total of 11499038 distinct NIFs\n",
      "[Info getCarNumClienteDf] Wed Jul 17 11:48:03 2019 Size of the featdf for 20181231 - Num rows: 13683905 - Num columns: 236\n",
      "[Info get_orders_df] Wed Jul 17 11:49:46 2019 Orders feats up to 20181130 computed - Number of rows is 8725478 for a total of 8725478 distinct num_cliente\n",
      "[Info getCarNumClienteDf] Wed Jul 17 11:50:00 2019 Size of the featdf for 20181231 - Num rows: 13683905 - Num columns: 238\n",
      "[Info getFbbChurnLabeledCar] Wed Jul 17 11:50:54 2019 Samples for month 20181231: 13683905\n",
      "20181231\n",
      "20190131\n",
      "[Info getFixPortRequestsForMonth] Wed Jul 17 11:52:31 2019 Port-out requests for fixed services during period 20181231-20190131: 57686\n",
      "[Info getFbbDxsForMonth] Wed Jul 17 11:52:41 2019 DXs for FBB services during the period: 20181231-20190131: 71798\n",
      "[Info getFbbChurnLabeledCar] Wed Jul 17 11:53:08 2019 Labeled samples for month 201812: 2050838\n"
     ]
    }
   ],
   "source": [
    "df_test_port = getFbbChurnLabeledCarCycles2(spark, origin, ttcycle_ini, selcols, horizon = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfExtraFeat_tt = spark.read.parquet('/data/udf/vf_es/churn/extra_feats_mod/extra_feats/year={}/month={}/day={}'\n",
    "                                            .format(int(ttcycle_ini[0:4]), int(ttcycle_ini[4:6]), int(ttcycle_ini[6:8])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfExtraFeatfbb_tt_dx = dfExtraFeat_tt.join(df_test_dx, [\"num_cliente\"], \"leftsemi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfExtraFeatfbb_tt_port = dfExtraFeat_tt.join(df_test_port, [\"num_cliente\"], \"leftsemi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o20097.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 8674.0 failed 4 times, most recent failure: Lost task 1.3 in stage 8674.0 (TID 984770, vgddp647hr.dc.sedc.internal.vodafone.com, executor 209): java.io.FileNotFoundException: File does not exist: hdfs://nameservice1/data/udf/vf_es/churn/fbb_tmp/numclidf_tmp_20181231/part-00154-00b1bfeb-7325-4f98-ac8a-dcc79a92cf52-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:182)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:381)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2775)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2774)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2774)\n\tat sun.reflect.GeneratedMethodAccessor275.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: File does not exist: hdfs://nameservice1/data/udf/vf_es/churn/fbb_tmp/numclidf_tmp_20181231/part-00154-00b1bfeb-7325-4f98-ac8a-dcc79a92cf52-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:182)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:381)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-5e912145672b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfExtraFeatSel_tt_dx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mselColumnas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maddExtraFeats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfExtraFeatfbb_tt_dx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/SP/data/home/asaezco/src/devel2/use-cases/churn/models/fbb_churn_amdocs/utils_fbb_churn.pyc\u001b[0m in \u001b[0;36maddExtraFeats\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0mdfSelGrouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfSelGrouped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolFinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'num_cliente'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0mdfSelGrouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfSelGrouped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rgu'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'fbb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"[Info FbbChurn] \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" The module for the Extra Feats has been run. Count for the table\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfSelGrouped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdfSelGrouped\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumnSel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \"\"\"\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/Anaconda-2.5.0/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/Anaconda-2.5.0/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o20097.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 8674.0 failed 4 times, most recent failure: Lost task 1.3 in stage 8674.0 (TID 984770, vgddp647hr.dc.sedc.internal.vodafone.com, executor 209): java.io.FileNotFoundException: File does not exist: hdfs://nameservice1/data/udf/vf_es/churn/fbb_tmp/numclidf_tmp_20181231/part-00154-00b1bfeb-7325-4f98-ac8a-dcc79a92cf52-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:182)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:381)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2775)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2774)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2774)\n\tat sun.reflect.GeneratedMethodAccessor275.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: File does not exist: hdfs://nameservice1/data/udf/vf_es/churn/fbb_tmp/numclidf_tmp_20181231/part-00154-00b1bfeb-7325-4f98-ac8a-dcc79a92cf52-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:182)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:381)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "dfExtraFeatSel_tt_dx,selColumnas=addExtraFeats(dfExtraFeatfbb_tt_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfExtraFeatSel_tt_port,selColumnas=addExtraFeats(dfExtraFeatfbb_tt_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initttdf_dx = df_test_dx.join(dfExtraFeatSel_tt_dx, [\"msisdn\",\"num_cliente\",'rgu'], how=\"left\").na.fill(dfillNa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initttdf_port = df_test_port.join(dfExtraFeatSel_tt_port, [\"msisdn\",\"num_cliente\",'rgu'], how=\"left\").na.fill(dfillNa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info getCarNumClienteDf] Wed May 29 14:55:23 2019 Size of the original df for 20181231: 13683905\n",
      "[Info FbbChurn] Wed May 29 15:03:03 2019 Saving origindf_tmp to HDFS\n",
      "[Info getCarNumClienteDf] Wed May 29 15:03:04 2019 Size of the origindf after load from HDFS: 13683905\n",
      "[Info getCarNumClienteDf] Wed May 29 15:04:31 2019 Size of the numclidf for 20181231 - Num rows: 2050838 - Num columns: 101\n",
      "[Info FbbChurn] Wed May 29 15:07:33 2019 Saving numclidf_tmp to HDFS\n",
      "[Info get_billing_df] Wed May 29 15:07:34 2019 Starting the preparation of billing feats\n",
      "[Info get_billing_df] Wed May 29 15:08:08 2019 Size of the df with billing feats: 5825804\n",
      "[Info getCarNumClienteDf] Wed May 29 15:08:29 2019 Size of the tmpdf for 20181231 - Num rows: 2050838 - Num columns: 134\n",
      "[Info get_mobile_spinners_df] Wed May 29 15:08:57 2019 Port-out data loaded for 20181231 with a total of 27844643 rows and 11499038 distinct NIFs\n",
      "[Info get_mobile_spinners_df] Wed May 29 15:09:34 2019 Timing feats computed for a total of 11499038 distinct NIFs\n",
      "[Info get_mobile_spinners_df] Wed May 29 15:09:52 2019 Destinatination and state feats computed for a total of 11499038 distinct NIFs\n",
      "[Info getCarNumClienteDf] Wed May 29 15:10:19 2019 Size of the featdf for 20181231 - Num rows: 2050838 - Num columns: 236\n",
      "[Info get_orders_df] Wed May 29 15:11:01 2019 Orders feats up to 20181130 computed - Number of rows is 8725478 for a total of 8725478 distinct num_cliente\n",
      "[Info getCarNumClienteDf] Wed May 29 15:11:15 2019 Size of the featdf for 20181231 - Num rows: 2050838 - Num columns: 238\n",
      "[Info getFbbChurnLabeledCar] Wed May 29 15:12:47 2019 Samples for month 20181231: 2050838\n",
      "20181231\n",
      "20190131\n",
      "[Info getFixPortRequestsForMonth] Wed May 29 15:14:43 2019 Port-out requests for fixed services during period 20181231-20190131: 57686\n",
      "[Info getFbbDxsForMonth] Wed May 29 15:14:57 2019 DXs for FBB services during the period: 20181231-20190131: 71798\n",
      "[Info getFbbChurnLabeledCar] Wed May 29 15:16:16 2019 Labeled samples for month 201812: 2050838\n"
     ]
    }
   ],
   "source": [
    "ttdf_ini = getFbbChurnLabeledCarCycles(spark, origin,ttcycle_ini,selcols, horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Añadimos extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfExtraFeat_tt = spark.read.parquet('/data/udf/vf_es/churn/extra_feats_mod/extra_feats/year={}/month={}/day={}'\n",
    "                                            .format(int(ttcycle_ini[0:4]), int(ttcycle_ini[4:6]), int(ttcycle_ini[6:8])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2050764, 2050764)\n"
     ]
    }
   ],
   "source": [
    "dfExtraFeatfbb_tt=dfExtraFeat_tt.join(ttdf_ini.select('num_cliente'),on='num_cliente',how='right')\n",
    "print(dfExtraFeatfbb_tt.select('num_cliente').distinct().count(),ttdf_ini.select('num_cliente').distinct().count()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[Info FbbChurn] Wed May 29 15:22:21 2019 The module for the Extra Feats has been run. Count for the table', 2050986)\n"
     ]
    }
   ],
   "source": [
    "dfExtraFeat_ttSel, selColumnas = addExtraFeats(dfExtraFeatfbb_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfillNa = fillNa(spark)\n",
    "for kkey in dfillNa.keys():\n",
    "    if kkey not in dfExtraFeat_ttSel.columns:\n",
    "        dfillNa.pop(kkey, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttdf = ttdf_ini.join(dfExtraFeat_ttSel, [\"msisdn\", \"num_cliente\", 'rgu'], how=\"left\").na.fill(dfillNa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************************************************\n",
    "# ***************************************************************************************\n",
    "#path = \"/data/udf/vf_es/churn/fbb_tmp/ttdf_\" + tr_ttdates\n",
    "#ttdf = spark.read.parquet(path)\n",
    "# ***************************************************************************************\n",
    "# ***************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_feats = []\n",
    "for i in range(0,num_tr_dfs):\n",
    "    featCols = list(set(numerical_feats) - set(noninf_feats_vec[i]))\n",
    "    vector_feats.append(featCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler_vec = []\n",
    "for i in range(0,num_tr_dfs):\n",
    "    assembler = VectorAssembler(inputCols = vector_feats[i], outputCol = \"features\")\n",
    "    assembler_vec.append(assembler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(featuresCol=\"features\",\\\n",
    "        labelCol=\"label\",\\\n",
    "        maxDepth=12,\\\n",
    "        maxBins=32,\\\n",
    "        minInstancesPerNode=200,\\\n",
    "        impurity=\"gini\",\\\n",
    "        featureSubsetStrategy=\"sqrt\",\\\n",
    "        subsamplingRate=0.7,\\\n",
    "        numTrees=800,\\\n",
    "        seed = 1234,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_vec = []\n",
    "for i in range(0,num_tr_dfs):\n",
    "    pipeline = Pipeline(stages= [assembler_vec[i], classifier])\n",
    "    pipeline_vec.append(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "models_vec = []\n",
    "for i in range(0,num_tr_dfs):\n",
    "    model = pipeline_vec[i].fit(vector_train[i])\n",
    "    models_vec.append(model)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calibration_function_vec(spark, model, valdf, labelcol, numpoints = 10, num = 0):\n",
    "    import numpy as np\n",
    "    score_name = \"model_score_\" + str(num)\n",
    "    score_calib_name ='calib_model_score_' + str(num)\n",
    "    getScore = udf(lambda prob: float(prob[1]), DoubleType())\n",
    "    valpredsdf = model.transform(valdf).withColumn(score_name, getScore(col(\"probability\")).cast(DoubleType())).select([score_name, labelcol])\n",
    "    minprob = valpredsdf.select(sql_min(score_name).alias('min')).rdd.collect()[0]['min']\n",
    "    maxprob = valpredsdf.select(sql_max(score_name).alias('max')).rdd.collect()[0]['max']\n",
    "    numlabels = float(valpredsdf.filter(col(labelcol) == 1.0).count())\n",
    "    print \"[Info Calibration] Training - Num samples in the target class \" + str(numlabels)\n",
    "    delta = float(maxprob - minprob)/float(numpoints)\n",
    "    ths = list([(minprob + i*delta) for i in list(np.arange(1,numpoints,step=1))])\n",
    "    samplepoints = [(float(i), float(valpredsdf.filter((col(score_name) <= i) & (col(labelcol) == 1.0)).count())/numlabels) for i in ths]\n",
    "    # samplepoints = [(float(i), valpredsdf.filter(col('model_score') <= i &).select(sql_avg(labelcol).alias('rate')).rdd.collect()[0]['rate']) for i in ths]\n",
    "    for pair in samplepoints:\n",
    "        print \"[Info Calibration] Training \" + str(pair[0]) + \", \" + str(pair[1])\n",
    "    sampledf = spark.createDataFrame(samplepoints, [score_name, 'target_prob'])\n",
    "    sampledf.show()\n",
    "    print \"[Info Calibration] Samples to implement the calibration model showed above\"\n",
    "    ir = IsotonicRegression(featuresCol=score_name, labelCol='target_prob', predictionCol=score_calib_name)\n",
    "    irmodel = ir.fit(sampledf)\n",
    "    return (irmodel, samplepoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***************************************************************************************\n",
    "# ***************************************************************************************\n",
    "# path2 = '/data/udf/vf_es/churn/fbb_tmp/valdf_' + tr_ttdates\n",
    "# valdf = spark.read.parquet(path2)\n",
    "# ***************************************************************************************\n",
    "# ***************************************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info Calibration] Training - Num samples in the target class 19070.0\n",
      "[Info Calibration] Training 0.174235204208, 0.00272679601468\n",
      "[Info Calibration] Training 0.260449978173, 0.0162034609334\n",
      "[Info Calibration] Training 0.346664752138, 0.0958049292082\n",
      "[Info Calibration] Training 0.432879526103, 0.224908232826\n",
      "[Info Calibration] Training 0.519094300069, 0.392711064499\n",
      "[Info Calibration] Training 0.605309074034, 0.578605138962\n",
      "[Info Calibration] Training 0.691523847999, 0.737703198741\n",
      "[Info Calibration] Training 0.777738621964, 0.868274777137\n",
      "[Info Calibration] Training 0.86395339593, 0.950340849502\n",
      "+-------------------+--------------------+\n",
      "|      model_score_0|         target_prob|\n",
      "+-------------------+--------------------+\n",
      "|0.17423520420768662|0.002726796014682...|\n",
      "| 0.2604499781729195| 0.01620346093340325|\n",
      "| 0.3466647521381523| 0.09580492920818039|\n",
      "| 0.4328795261033851| 0.22490823282642894|\n",
      "| 0.5190943000686179| 0.39271106449921345|\n",
      "| 0.6053090740338508|    0.57860513896172|\n",
      "| 0.6915238479990836|  0.7377031987414787|\n",
      "| 0.7777386219643164|  0.8682747771368642|\n",
      "| 0.8639533959295492|  0.9503408495018354|\n",
      "+-------------------+--------------------+\n",
      "\n",
      "[Info Calibration] Samples to implement the calibration model showed above\n",
      "0\n",
      "[Info Calibration] Training - Num samples in the target class 19070.0\n",
      "[Info Calibration] Training 0.176910063015, 0.00272679601468\n",
      "[Info Calibration] Training 0.263475763405, 0.0171473518616\n",
      "[Info Calibration] Training 0.350041463795, 0.100576822234\n",
      "[Info Calibration] Training 0.436607164186, 0.229103303618\n",
      "[Info Calibration] Training 0.523172864576, 0.400367068694\n",
      "[Info Calibration] Training 0.609738564966, 0.589984268485\n",
      "[Info Calibration] Training 0.696304265356, 0.754955427373\n",
      "[Info Calibration] Training 0.782869965747, 0.879863660199\n",
      "[Info Calibration] Training 0.869435666137, 0.952857891977\n",
      "+-------------------+--------------------+\n",
      "|      model_score_1|         target_prob|\n",
      "+-------------------+--------------------+\n",
      "|0.17691006301481257|0.002726796014682...|\n",
      "|0.26347576340508766|0.017147351861562663|\n",
      "| 0.3500414637953628|  0.1005768222338752|\n",
      "| 0.4366071641856379| 0.22910330361824857|\n",
      "| 0.5231728645759129| 0.40036706869428423|\n",
      "| 0.6097385649661881|  0.5899842684845307|\n",
      "| 0.6963042653564632|  0.7549554273728369|\n",
      "| 0.7828699657467383|  0.8798636601992659|\n",
      "| 0.8694356661370134|  0.9528578919769272|\n",
      "+-------------------+--------------------+\n",
      "\n",
      "[Info Calibration] Samples to implement the calibration model showed above\n",
      "1\n",
      "[Info Calibration] Training - Num samples in the target class 19070.0\n",
      "[Info Calibration] Training 0.171285681692, 0.0023597273204\n",
      "[Info Calibration] Training 0.258380400941, 0.0152071316203\n",
      "[Info Calibration] Training 0.34547512019, 0.0889879391715\n",
      "[Info Calibration] Training 0.43256983944, 0.22055584688\n",
      "[Info Calibration] Training 0.519664558689, 0.396171997902\n",
      "[Info Calibration] Training 0.606759277938, 0.586837965391\n",
      "[Info Calibration] Training 0.693853997187, 0.754535920294\n",
      "[Info Calibration] Training 0.780948716437, 0.874829575249\n",
      "[Info Calibration] Training 0.868043435686, 0.951546932354\n",
      "+-------------------+--------------------+\n",
      "|      model_score_2|         target_prob|\n",
      "+-------------------+--------------------+\n",
      "|0.17128568169177316|0.002359727320398...|\n",
      "|0.25838040094103476|0.015207131620346093|\n",
      "|0.34547512019029625| 0.08898793917147352|\n",
      "|0.43256983943955785|  0.2205558468799161|\n",
      "| 0.5196645586888194|  0.3961719979024646|\n",
      "| 0.6067592779380809|   0.586837965390666|\n",
      "| 0.6938539971873425|  0.7545359202936549|\n",
      "| 0.7809487164366041|  0.8748295752490823|\n",
      "| 0.8680434356858657|  0.9515469323544835|\n",
      "+-------------------+--------------------+\n",
      "\n",
      "[Info Calibration] Samples to implement the calibration model showed above\n",
      "2\n",
      "[Info Calibration] Training - Num samples in the target class 19070.0\n",
      "[Info Calibration] Training 0.167047919893, 0.0022548505506\n",
      "[Info Calibration] Training 0.253958591076, 0.0135291033036\n",
      "[Info Calibration] Training 0.340869262258, 0.0839538542213\n",
      "[Info Calibration] Training 0.42777993344, 0.211012060829\n",
      "[Info Calibration] Training 0.514690604622, 0.386837965391\n",
      "[Info Calibration] Training 0.601601275804, 0.575616151023\n",
      "[Info Calibration] Training 0.688511946987, 0.742737283692\n",
      "[Info Calibration] Training 0.775422618169, 0.870424750918\n",
      "[Info Calibration] Training 0.862333289351, 0.950183534347\n",
      "+-------------------+--------------------+\n",
      "|      model_score_3|         target_prob|\n",
      "+-------------------+--------------------+\n",
      "|0.16704791989345086|0.002254850550603...|\n",
      "| 0.2539585910756349| 0.01352910330361825|\n",
      "|0.34086926225781894| 0.08395385422128998|\n",
      "|0.42777993344000304| 0.21101206082852647|\n",
      "| 0.5146906046221872| 0.38683796539066595|\n",
      "| 0.6016012758043712|  0.5756161510225485|\n",
      "| 0.6885119469865553|  0.7427372836916623|\n",
      "| 0.7754226181687394|  0.8704247509176717|\n",
      "| 0.8623332893509235|  0.9501835343471421|\n",
      "+-------------------+--------------------+\n",
      "\n",
      "[Info Calibration] Samples to implement the calibration model showed above\n",
      "3\n",
      "[Info Calibration] Training - Num samples in the target class 19070.0\n",
      "[Info Calibration] Training 0.172558055475, 0.00262191924489\n",
      "[Info Calibration] Training 0.25975793855, 0.0153120083901\n",
      "[Info Calibration] Training 0.346957821625, 0.0932354483482\n",
      "[Info Calibration] Training 0.4341577047, 0.219664394337\n",
      "[Info Calibration] Training 0.521357587775, 0.401887781856\n",
      "[Info Calibration] Training 0.60855747085, 0.590089145254\n",
      "[Info Calibration] Training 0.695757353925, 0.755007865758\n",
      "[Info Calibration] Training 0.782957237, 0.876297850026\n",
      "[Info Calibration] Training 0.870157120075, 0.953539590981\n",
      "+-------------------+--------------------+\n",
      "|      model_score_4|         target_prob|\n",
      "+-------------------+--------------------+\n",
      "|0.17255805547520947|0.002621919244887...|\n",
      "|0.25975793855016144|0.015312008390141584|\n",
      "|0.34695782162511335| 0.09323544834819088|\n",
      "| 0.4341577047000653| 0.21966439433665444|\n",
      "| 0.5213575877750173| 0.40188778185631885|\n",
      "| 0.6085574708499691|  0.5900891452543262|\n",
      "| 0.6957573539249211|  0.7550078657577347|\n",
      "| 0.7829572369998731|  0.8762978500262192|\n",
      "|  0.870157120074825|  0.9535395909805978|\n",
      "+-------------------+--------------------+\n",
      "\n",
      "[Info Calibration] Samples to implement the calibration model showed above\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION\n",
    "# Calibration\n",
    "calibration_vec = []\n",
    "for i in range(0,num_tr_dfs):\n",
    "    calibmodel = get_calibration_function_vec(spark, models_vec[i], valdf, 'label', 10, i)\n",
    "    calibration_vec.append(calibmodel)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "getScore = udf(lambda prob: float(prob[1]), DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "tr_calib_vec = []\n",
    "for i in range(0, num_tr_dfs):\n",
    "    tr_preds_df = models_vec[i].transform(vector_train[i]).withColumn(\"model_score_\" + str(i), getScore(col(\"probability\")).cast(DoubleType()))\n",
    "    tr_calib_preds_df = calibration_vec[i][0].transform(tr_preds_df)\n",
    "    tr_calib_vec.append(tr_calib_preds_df)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.754406933206\n",
      "0\n",
      "0.755106476011\n",
      "1\n",
      "0.753097394074\n",
      "2\n",
      "0.755560008586\n",
      "3\n",
      "0.755012309843\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "tr_metrics_vec = []\n",
    "# Train evaluation\n",
    "#tr_calib_vec = []\n",
    "for i in range(0, num_tr_dfs):#'calib_model_score_' + str(num)\n",
    "    trPredictionAndLabels = tr_calib_vec[i].select(['calib_model_score_' + str(i), 'label']).rdd.map(lambda r: (r['calib_model_score_' + str(i)], r['label']))\n",
    "    trmetrics = BinaryClassificationMetrics(trPredictionAndLabels)\n",
    "    print(trmetrics.areaUnderROC)\n",
    "    tr_metrics_vec.append(trmetrics)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# Test eval\n",
    "df_aux = ttdf\n",
    "for i in range(0, num_tr_dfs):\n",
    "    df_aux = models_vec[i].transform(df_aux).withColumn(\"model_score_\" + str(i), getScore(col(\"probability\")).cast(DoubleType())).drop(col('probability'))\n",
    "    # tt_preds_df\n",
    "    df_aux = calibration_vec[i][0].transform(df_aux.drop(col('features')).drop(col('prediction')).drop(col('rawPrediction')))\n",
    "    print(i)\n",
    "    \n",
    "save_dir = 'tests_es.asaezco_fbb_improved_' + str(20190131)\n",
    "df_aux.write.format('parquet').mode('overwrite').saveAsTable(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- msisdn: string (nullable = true)\n",
      " |-- num_cliente: string (nullable = true)\n",
      " |-- rgu: string (nullable = true)\n",
      " |-- campo1: string (nullable = true)\n",
      " |-- msisdn_d: string (nullable = true)\n",
      " |-- nif_cliente: string (nullable = true)\n",
      " |-- mean_dias_desde_fx_srv_basic: double (nullable = false)\n",
      " |-- movil_services: long (nullable = true)\n",
      " |-- tv_services: long (nullable = true)\n",
      " |-- total_penal_cust_pending_n3_penal_amount: double (nullable = false)\n",
      " |-- x_formato_factura: string (nullable = true)\n",
      " |-- x_user_twitter: string (nullable = true)\n",
      " |-- total_penal_cust_pending_n4_penal_amount: double (nullable = false)\n",
      " |-- dias_desde_bam-movil_fx_first: double (nullable = false)\n",
      " |-- max_dias_desde_fx_trybuy_autom_tv: double (nullable = false)\n",
      " |-- total_price_tariff: double (nullable = false)\n",
      " |-- flg_robinson: string (nullable = true)\n",
      " |-- pvr_services: double (nullable = false)\n",
      " |-- num_tariff_redl: double (nullable = false)\n",
      " |-- dias_desde_fx_fbb_upgrade: double (nullable = false)\n",
      " |-- max_dias_desde_fx_srv_basic: double (nullable = false)\n",
      " |-- total_price_trybuy: double (nullable = false)\n",
      " |-- total_penal_srv_pending_n4_penal_amount: double (nullable = false)\n",
      " |-- max_dias_desde_fx_pvr_tv: double (nullable = false)\n",
      " |-- max_dias_desde_fx_football_tv: double (nullable = false)\n",
      " |-- total_max_dias_hasta_penal_cust_pending_end_date: double (nullable = false)\n",
      " |-- trybuy_services: double (nullable = false)\n",
      " |-- min_price_srv_basic: double (nullable = false)\n",
      " |-- total_penal_cust_pending_n2_penal_amount: double (nullable = false)\n",
      " |-- x_datos_trafico: string (nullable = true)\n",
      " |-- total_min_dias_hasta_penal_srv_pending_end_date: double (nullable = false)\n",
      " |-- cta_correo: string (nullable = true)\n",
      " |-- gender2hgbst_elm: string (nullable = true)\n",
      " |-- dias_desde_fixed_fx_first: double (nullable = false)\n",
      " |-- max_price_srv_basic: double (nullable = false)\n",
      " |-- total_penal_srv_pending_n5_penal_amount: double (nullable = false)\n",
      " |-- prepaid_services: long (nullable = true)\n",
      " |-- hz_services: double (nullable = false)\n",
      " |-- dias_desde_tv_fx_first: double (nullable = false)\n",
      " |-- dias_desde_movil_fx_first: double (nullable = false)\n",
      " |-- total_penal_srv_pending_n1_penal_amount: double (nullable = false)\n",
      " |-- total_max_dias_hasta_penal_srv_pending_end_date: double (nullable = false)\n",
      " |-- num_tariff_xs: double (nullable = false)\n",
      " |-- total_price_motor: double (nullable = false)\n",
      " |-- x_user_facebook: string (nullable = true)\n",
      " |-- fixed_services: long (nullable = true)\n",
      " |-- fbb_upgrade: string (nullable = true)\n",
      " |-- max_dias_desde_fx_zapper_tv: double (nullable = false)\n",
      " |-- total_penal_srv_pending_n2_penal_amount: double (nullable = false)\n",
      " |-- factura_electronica: string (nullable = true)\n",
      " |-- num_tariff_plana200min: double (nullable = false)\n",
      " |-- max_dias_desde_fx_trybuy_tv: double (nullable = false)\n",
      " |-- min_price_tariff: double (nullable = false)\n",
      " |-- bam-movil_services: long (nullable = true)\n",
      " |-- total_penal_srv_pending_n3_penal_amount: double (nullable = false)\n",
      " |-- num_tariff_unknown: double (nullable = false)\n",
      " |-- total_min_dias_hasta_penal_cust_pending_end_date: double (nullable = false)\n",
      " |-- x_cesion_datos: string (nullable = true)\n",
      " |-- football_services: double (nullable = false)\n",
      " |-- trybuy_autom_services: double (nullable = false)\n",
      " |-- total_price_zapper: double (nullable = false)\n",
      " |-- marriage2hgbst_elm: string (nullable = true)\n",
      " |-- total_price_pvr: double (nullable = false)\n",
      " |-- num_2lins: double (nullable = false)\n",
      " |-- fbb_services: long (nullable = true)\n",
      " |-- num_tariff_smart: double (nullable = false)\n",
      " |-- x_publicidad_email: string (nullable = true)\n",
      " |-- cliente_migrado: string (nullable = true)\n",
      " |-- x_idioma_factura: string (nullable = true)\n",
      " |-- num_tariff_otros: double (nullable = false)\n",
      " |-- metodo_pago: string (nullable = true)\n",
      " |-- max_price_tariff: double (nullable = false)\n",
      " |-- bam_services: long (nullable = true)\n",
      " |-- total_price_dto_lev2: double (nullable = false)\n",
      " |-- superoferta: string (nullable = true)\n",
      " |-- total_price_dto_lev1: double (nullable = false)\n",
      " |-- total_penal_cust_pending_n5_penal_amount: double (nullable = false)\n",
      " |-- total_price_football: double (nullable = false)\n",
      " |-- x_antiguedad_cuenta: string (nullable = true)\n",
      " |-- dias_desde_prepaid_fx_first: double (nullable = false)\n",
      " |-- total_tv_total_charges: double (nullable = false)\n",
      " |-- num_tariff_minim: double (nullable = false)\n",
      " |-- dias_desde_fbb_fx_first: double (nullable = false)\n",
      " |-- mean_price_srv_basic: double (nullable = false)\n",
      " |-- dias_desde_bam_fx_first: double (nullable = false)\n",
      " |-- zapper_services: double (nullable = false)\n",
      " |-- motor_services: double (nullable = false)\n",
      " |-- num_tariff_maslineasmini: double (nullable = false)\n",
      " |-- num_tariff_redm: double (nullable = false)\n",
      " |-- total_price_srv_basic: double (nullable = false)\n",
      " |-- nacionalidad: string (nullable = true)\n",
      " |-- total_price_trybuy_autom: double (nullable = false)\n",
      " |-- max_dias_desde_fx_motor_tv: double (nullable = false)\n",
      " |-- min_dias_desde_fx_dto_lev1: double (nullable = false)\n",
      " |-- min_dias_desde_fx_dto_lev2: double (nullable = false)\n",
      " |-- num_tariff_planaminilim: double (nullable = false)\n",
      " |-- total_penal_cust_pending_n1_penal_amount: double (nullable = false)\n",
      " |-- tipo_documento: string (nullable = true)\n",
      " |-- x_datos_navegacion: string (nullable = true)\n",
      " |-- min_dias_desde_fx_srv_basic: double (nullable = false)\n",
      " |-- num_tariff_megayuser: double (nullable = false)\n",
      " |-- inc_bill_n1_n3_net: double (nullable = false)\n",
      " |-- inc_bill_n1_n4_net: double (nullable = false)\n",
      " |-- bill_n3_net: double (nullable = false)\n",
      " |-- bill_n1_net: double (nullable = false)\n",
      " |-- bill_n4_net: double (nullable = false)\n",
      " |-- inc_Bill_N1_N3_Amount_To_Pay: double (nullable = false)\n",
      " |-- bill_n2_net: double (nullable = false)\n",
      " |-- bill_n5_net: double (nullable = false)\n",
      " |-- inc_Bill_N1_N4_Amount_To_Pay: double (nullable = false)\n",
      " |-- inc_bill_n1_n2_net: double (nullable = false)\n",
      " |-- inc_bill_n1_n5_net: double (nullable = false)\n",
      " |-- inc_Bill_N1_N2_Amount_To_Pay: double (nullable = false)\n",
      " |-- inc_Bill_N1_N5_Amount_To_Pay: double (nullable = false)\n",
      " |-- Bill_N3_InvoiceCharges: double (nullable = false)\n",
      " |-- Bill_N1_InvoiceCharges: double (nullable = false)\n",
      " |-- Bill_N3_Debt_Amount: double (nullable = false)\n",
      " |-- Bill_N5_Debt_Amount: double (nullable = false)\n",
      " |-- Bill_N2_Amount_To_Pay: double (nullable = false)\n",
      " |-- Bill_N4_Debt_Amount: double (nullable = false)\n",
      " |-- Bill_N4_Tax_Amount: double (nullable = false)\n",
      " |-- Bill_N3_Amount_To_Pay: double (nullable = false)\n",
      " |-- Bill_N1_Amount_To_Pay: double (nullable = false)\n",
      " |-- Bill_N5_InvoiceCharges: double (nullable = false)\n",
      " |-- Bill_N2_InvoiceCharges: double (nullable = false)\n",
      " |-- Bill_N1_Debt_Amount: double (nullable = false)\n",
      " |-- Bill_N3_Tax_Amount: double (nullable = false)\n",
      " |-- Bill_N4_Amount_To_Pay: double (nullable = false)\n",
      " |-- Bill_N5_Tax_Amount: double (nullable = false)\n",
      " |-- Bill_N2_Tax_Amount: double (nullable = false)\n",
      " |-- Bill_N2_Debt_Amount: double (nullable = false)\n",
      " |-- Bill_N1_Tax_Amount: double (nullable = false)\n",
      " |-- Bill_N5_Amount_To_Pay: double (nullable = false)\n",
      " |-- Bill_N4_InvoiceCharges: double (nullable = false)\n",
      " |-- otros_ASOL: long (nullable = true)\n",
      " |-- total_orange: long (nullable = true)\n",
      " |-- otros_AENV: long (nullable = true)\n",
      " |-- unknown_ACON: long (nullable = true)\n",
      " |-- movistar_ASOL: long (nullable = true)\n",
      " |-- masmovil_APOR: long (nullable = true)\n",
      " |-- total_apor: long (nullable = true)\n",
      " |-- movistar_AENV: long (nullable = true)\n",
      " |-- reuskal_PCAN: long (nullable = true)\n",
      " |-- total_movistar: long (nullable = true)\n",
      " |-- simyo_ASOL: long (nullable = true)\n",
      " |-- orange_ASOL: long (nullable = true)\n",
      " |-- masmovil_ASOL: long (nullable = true)\n",
      " |-- simyo_ACAN: long (nullable = true)\n",
      " |-- simyo_AREC: long (nullable = true)\n",
      " |-- movistar_PCAN: long (nullable = true)\n",
      " |-- orange_AACE: long (nullable = true)\n",
      " |-- pepephone_ACON: long (nullable = true)\n",
      " |-- orange_PCAN: long (nullable = true)\n",
      " |-- jazztel_ACON: long (nullable = true)\n",
      " |-- simyo_AENV: long (nullable = true)\n",
      " |-- otros_AREC: long (nullable = true)\n",
      " |-- yoigo_AACE: long (nullable = true)\n",
      " |-- jazztel_AENV: long (nullable = true)\n",
      " |-- simyo_PCAN: long (nullable = true)\n",
      " |-- pepephone_APOR: long (nullable = true)\n",
      " |-- unknown_AENV: long (nullable = true)\n",
      " |-- yoigo_ACAN: long (nullable = true)\n",
      " |-- masmovil_AACE: long (nullable = true)\n",
      " |-- otros_ACAN: long (nullable = true)\n",
      " |-- total_unknown: long (nullable = true)\n",
      " |-- yoigo_AENV: long (nullable = true)\n",
      " |-- total_simyo: long (nullable = true)\n",
      " |-- total_pepephone: long (nullable = true)\n",
      " |-- pepephone_AACE: long (nullable = true)\n",
      " |-- total_otros: long (nullable = true)\n",
      " |-- orange_ACON: long (nullable = true)\n",
      " |-- jazztel_AREC: long (nullable = true)\n",
      " |-- movistar_ACON: long (nullable = true)\n",
      " |-- orange_ACAN: long (nullable = true)\n",
      " |-- reuskal_AACE: long (nullable = true)\n",
      " |-- simyo_AACE: long (nullable = true)\n",
      " |-- pepephone_PCAN: long (nullable = true)\n",
      " |-- otros_APOR: long (nullable = true)\n",
      " |-- nif_var_days_since_port: double (nullable = false)\n",
      " |-- total_reuskal: long (nullable = true)\n",
      " |-- total_jazztel: long (nullable = true)\n",
      " |-- pepephone_ACAN: long (nullable = true)\n",
      " |-- pepephone_ASOL: long (nullable = true)\n",
      " |-- yoigo_AREC: long (nullable = true)\n",
      " |-- unknown_PCAN: long (nullable = true)\n",
      " |-- reuskal_AENV: long (nullable = true)\n",
      " |-- movistar_AACE: long (nullable = true)\n",
      " |-- unknown_ASOL: long (nullable = true)\n",
      " |-- movistar_AREC: long (nullable = true)\n",
      " |-- otros_ACON: long (nullable = true)\n",
      " |-- reuskal_ASOL: long (nullable = true)\n",
      " |-- total_acan: long (nullable = true)\n",
      " |-- orange_AENV: long (nullable = true)\n",
      " |-- reuskal_AREC: long (nullable = true)\n",
      " |-- reuskal_ACON: long (nullable = true)\n",
      " |-- unknown_AREC: long (nullable = true)\n",
      " |-- nif_port_freq_per_day: double (nullable = false)\n",
      " |-- jazztel_APOR: long (nullable = true)\n",
      " |-- unknown_APOR: long (nullable = true)\n",
      " |-- total_arec: long (nullable = true)\n",
      " |-- yoigo_PCAN: long (nullable = true)\n",
      " |-- nif_port_number: long (nullable = true)\n",
      " |-- pepephone_AENV: long (nullable = true)\n",
      " |-- masmovil_AREC: long (nullable = true)\n",
      " |-- jazztel_AACE: long (nullable = true)\n",
      " |-- masmovil_ACAN: long (nullable = true)\n",
      " |-- yoigo_ACON: long (nullable = true)\n",
      " |-- orange_AREC: long (nullable = true)\n",
      " |-- pepephone_AREC: long (nullable = true)\n",
      " |-- yoigo_ASOL: long (nullable = true)\n",
      " |-- jazztel_PCAN: long (nullable = true)\n",
      " |-- total_yoigo: long (nullable = true)\n",
      " |-- nif_min_days_since_port: double (nullable = false)\n",
      " |-- masmovil_AENV: long (nullable = true)\n",
      " |-- otros_PCAN: long (nullable = true)\n",
      " |-- masmovil_PCAN: long (nullable = true)\n",
      " |-- yoigo_APOR: long (nullable = true)\n",
      " |-- movistar_ACAN: long (nullable = true)\n",
      " |-- reuskal_ACAN: long (nullable = true)\n",
      " |-- unknown_ACAN: long (nullable = true)\n",
      " |-- masmovil_ACON: long (nullable = true)\n",
      " |-- nif_max_days_since_port: double (nullable = false)\n",
      " |-- simyo_APOR: long (nullable = true)\n",
      " |-- jazztel_ACAN: long (nullable = true)\n",
      " |-- orange_APOR: long (nullable = true)\n",
      " |-- unknown_AACE: long (nullable = true)\n",
      " |-- jazztel_ASOL: long (nullable = true)\n",
      " |-- nif_port_freq_per_msisdn: double (nullable = false)\n",
      " |-- num_distinct_operators: double (nullable = false)\n",
      " |-- nif_avg_days_since_port: double (nullable = false)\n",
      " |-- nif_distinct_msisdn: long (nullable = true)\n",
      " |-- total_masmovil: long (nullable = true)\n",
      " |-- otros_AACE: long (nullable = true)\n",
      " |-- reuskal_APOR: long (nullable = true)\n",
      " |-- simyo_ACON: long (nullable = true)\n",
      " |-- movistar_APOR: long (nullable = true)\n",
      " |-- days_since_last_order: integer (nullable = false)\n",
      " |-- days_since_first_order: integer (nullable = false)\n",
      " |-- label: double (nullable = true)\n",
      " |-- ccc_num_pbma_srv_total: long (nullable = false)\n",
      " |-- ccc_num_abonos_tipis_total: long (nullable = false)\n",
      " |-- ccc_num_tipis_factura_total: long (nullable = false)\n",
      " |-- ccc_num_tipis_perm_dctos_total: long (nullable = false)\n",
      " |-- ccc_num_tipis_info_total: long (nullable = false)\n",
      " |-- ccc_num_tipis_info_nocomp_total: long (nullable = false)\n",
      " |-- ccc_num_tipis_info_comp_total: long (nullable = false)\n",
      " |-- ccc_num_averias_total: long (nullable = false)\n",
      " |-- ccc_num_incidencias_total: long (nullable = false)\n",
      " |-- ccc_num_interactions_total: long (nullable = false)\n",
      " |-- ccc_num_na_buckets_total: long (nullable = false)\n",
      " |-- ccc_num_ivr_interactions_total: long (nullable = false)\n",
      " |-- ccc_num_diff_buckets_total: long (nullable = false)\n",
      " |-- netapps_ns_apps_facebook_data_mb_total: double (nullable = true)\n",
      " |-- netapps_ns_apps_facebook_total_effective_dl_mb_total: double (nullable = true)\n",
      " |-- netapps_ns_apps_facebook_total_effective_ul_mb_total: double (nullable = true)\n",
      " |-- netapps_ns_apps_facebook_days_total: long (nullable = true)\n",
      " |-- netapps_ns_apps_facebook_timestamps_total: long (nullable = true)\n",
      " |-- netapps_ns_apps_facebookmessages_data_mb_total: double (nullable = true)\n",
      " |-- netapps_ns_apps_facebookmessages_total_effective_dl_mb_total: double (nullable = true)\n",
      " |-- netapps_ns_apps_facebookmessages_total_effective_ul_mb_total: double (nullable = true)\n",
      " |-- netapps_ns_apps_facebookmessages_days_total: long (nullable = true)\n",
      " |-- netapps_ns_apps_facebookmessages_timestamps_total: long (nullable = true)\n",
      " |-- netapps_ns_apps_facebook_video_data_mb_total: double (nullable = true)\n",
      " |-- netapps_ns_apps_facebook_video_total_effective_dl_mb_total: double (nullable = true)\n",
      " |-- netapps_ns_apps_facebook_video_total_effective_ul_mb_total: double (nullable = true)\n",
      " |-- netapps_ns_apps_facebook_video_days_total: long (nullable = true)\n",
      " |-- netapps_ns_apps_facebook_video_timestamps_total: long (nullable = true)\n",
      " |-- netapps_ns_apps_googleplay_data_mb_total: double (nullable = true)\n",
      " |-- netapps_ns_apps_googleplay_total_effective_dl_mb_total: double (nullable = true)\n",
      " |-- netapps_ns_apps_googleplay_total_effective_ul_mb_total: double (nullable = true)\n",
      " |-- netapps_ns_apps_googleplay_days_total: long (nullable = true)\n",
      " |-- netapps_ns_apps_googleplay_timestamps_total: long (nullable = true)\n",
      " |-- netapps_ns_apps_instagram_data_mb_total: double (nullable = true)\n",
      " |-- netapps_ns_apps_instagram_total_effective_dl_mb_total: double (nullable = true)\n",
      " |-- netapps_ns_apps_instagram_total_effective_ul_mb_total: double (nullable = true)\n",
      " |-- netapps_ns_apps_instagram_days_total: long (nullable = true)\n",
      " |-- netapps_ns_apps_instagram_timestamps_total: long (nullable = true)\n",
      " |-- netapps_ns_apps_whatsapp_data_mb_total: double (nullable = true)\n",
      " |-- netapps_ns_apps_whatsapp_total_effective_dl_mb_total: double (nullable = true)\n",
      " |-- netapps_ns_apps_whatsapp_total_effective_ul_mb_total: double (nullable = true)\n",
      " |-- netapps_ns_apps_whatsapp_days_total: long (nullable = true)\n",
      " |-- netapps_ns_apps_whatsapp_timestamps_total: long (nullable = true)\n",
      " |-- netapps_ns_apps_whatsapp_media_message_data_mb_total: double (nullable = true)\n",
      " |-- netapps_ns_apps_whatsapp_media_message_total_effective_dl_mb_total: double (nullable = true)\n",
      " |-- netapps_ns_apps_whatsapp_media_message_total_effective_ul_mb_total: double (nullable = true)\n",
      " |-- netapps_ns_apps_whatsapp_media_message_days_total: long (nullable = true)\n",
      " |-- netapps_ns_apps_whatsapp_media_message_timestamps_total: long (nullable = true)\n",
      " |-- netapps_ns_apps_whatsapp_voice_calling_data_mb_total: double (nullable = true)\n",
      " |-- netapps_ns_apps_whatsapp_voice_calling_total_effective_dl_mb_total: double (nullable = true)\n",
      " |-- netapps_ns_apps_whatsapp_voice_calling_total_effective_ul_mb_total: double (nullable = true)\n",
      " |-- netapps_ns_apps_whatsapp_voice_calling_days_total: long (nullable = true)\n",
      " |-- netapps_ns_apps_whatsapp_voice_calling_timestamps_total: long (nullable = true)\n",
      " |-- ccc_num_pbma_srv: integer (nullable = true)\n",
      " |-- ccc_num_abonos_tipis: integer (nullable = true)\n",
      " |-- ccc_num_tipis_factura: integer (nullable = true)\n",
      " |-- ccc_num_tipis_perm_dctos: integer (nullable = true)\n",
      " |-- ccc_num_tipis_info: integer (nullable = true)\n",
      " |-- ccc_num_tipis_info_nocomp: integer (nullable = true)\n",
      " |-- ccc_num_tipis_info_comp: integer (nullable = true)\n",
      " |-- ccc_num_averias: integer (nullable = true)\n",
      " |-- ccc_num_incidencias: integer (nullable = true)\n",
      " |-- ccc_num_interactions: integer (nullable = true)\n",
      " |-- ccc_num_na_buckets: integer (nullable = true)\n",
      " |-- ccc_num_ivr_interactions: integer (nullable = true)\n",
      " |-- ccc_num_diff_buckets: integer (nullable = true)\n",
      " |-- tgs_stack: string (nullable = false)\n",
      " |-- tgs_decil: string (nullable = false)\n",
      " |-- tgs_descuento: string (nullable = false)\n",
      " |-- tgs_sum_ind_under_use: integer (nullable = true)\n",
      " |-- tgs_sum_ind_over_use: integer (nullable = true)\n",
      " |-- tgs_clasificacion_uso: string (nullable = false)\n",
      " |-- tgs_blindaje_bi: string (nullable = false)\n",
      " |-- tgs_blinda_bi_n2: string (nullable = false)\n",
      " |-- tgs_blinda_bi_n4: string (nullable = false)\n",
      " |-- tgs_blindaje_bi_expirado: string (nullable = false)\n",
      " |-- tgs_target_accionamiento: string (nullable = false)\n",
      " |-- tgs_days_since_f_inicio_bi: double (nullable = false)\n",
      " |-- tgs_days_since_f_inicio_bi_exp: double (nullable = false)\n",
      " |-- tgs_days_until_f_fin_bi: double (nullable = false)\n",
      " |-- tgs_days_until_f_fin_bi_exp: double (nullable = false)\n",
      " |-- tgs_days_until_fecha_fin_dto: double (nullable = false)\n",
      " |-- tgs_has_discount: integer (nullable = true)\n",
      " |-- tgs_discount_proc: string (nullable = false)\n",
      " |-- orders_days_since_last_order: integer (nullable = true)\n",
      " |-- orders_days_since_first_order: integer (nullable = true)\n",
      " |-- orders_order_n1_class: string (nullable = false)\n",
      " |-- orders_num_orders: integer (nullable = true)\n",
      " |-- orders_avg_days_per_order: double (nullable = false)\n",
      " |-- model_score_0: double (nullable = true)\n",
      " |-- calib_model_score_0: double (nullable = true)\n",
      " |-- model_score_1: double (nullable = true)\n",
      " |-- calib_model_score_1: double (nullable = true)\n",
      " |-- model_score_2: double (nullable = true)\n",
      " |-- calib_model_score_2: double (nullable = true)\n",
      " |-- model_score_3: double (nullable = true)\n",
      " |-- calib_model_score_3: double (nullable = true)\n",
      " |-- model_score_4: double (nullable = true)\n",
      " |-- calib_model_score_4: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_aux.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "ttPredictionAndLabels = tt_calib_preds_df.select(['calib_model_score', 'label']).rdd.map(lambda r: (r['calib_model_score'], r['label']))\n",
    "ttmetrics = BinaryClassificationMetrics(ttPredictionAndLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info FbbChurn] Wed May 29 11:42:15 2019 Area under ROC(tr) = 0.760427050776 - Area under ROC(tt) = 0.714090517079\n"
     ]
    }
   ],
   "source": [
    "print(\"[Info FbbChurn] \" + time.ctime() + \" Area under ROC(tr) = \" + str(trmetrics.areaUnderROC) + \" - Area under ROC(tt) = \" + str(ttmetrics.areaUnderROC))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Model 0: Area under ROC(tr) = 0.754406933206\n",
      "Model 0: Area under ROC(tt) = 0.713514753724\n",
      "1\n",
      "Model 1: Area under ROC(tr) = 0.755106476011\n",
      "Model 1: Area under ROC(tt) = 0.713128028314\n",
      "2\n",
      "Model 2: Area under ROC(tr) = 0.753097394074\n",
      "Model 2: Area under ROC(tt) = 0.712410491476\n",
      "3\n",
      "Model 3: Area under ROC(tr) = 0.755560008586\n",
      "Model 3: Area under ROC(tt) = 0.712465860634\n",
      "4\n",
      "Model 4: Area under ROC(tr) = 0.755012309843\n",
      "Model 4: Area under ROC(tt) = 0.713019849073\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, num_tr_dfs):\n",
    "    ttPredictionAndLabels = df_aux.select(['calib_model_score_' + str(i), 'label']).rdd.map(lambda r: (r['calib_model_score_' + str(i)], r['label']))\n",
    "    ttmetrics = BinaryClassificationMetrics(ttPredictionAndLabels)\n",
    "    #print(ttmetrics.areaUnderROC)\n",
    "    print(i)\n",
    "    print \"Model {}: Area under ROC(tr) = {}\".format(i, str(tr_metrics_vec[i].areaUnderROC))\n",
    "    print \"Model {}: Area under ROC(tt) = {}\".format(i, str(ttmetrics.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['calib_model_score_0',\n",
       " 'calib_model_score_1',\n",
       " 'calib_model_score_2',\n",
       " 'calib_model_score_3',\n",
       " 'calib_model_score_4']"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols_scores = [name_ for name_ in df_aux.columns if name_.startswith('calib_model_')]\n",
    "\n",
    "from pyspark.sql.functions import greatest, least, mean, sum\n",
    "\n",
    "df_aux2 = df_aux.withColumn('maximo', greatest(*cols_scores)).withColumn('minimo', least(*cols_scores))\\\n",
    ".withColumn('medio', (df_aux['calib_model_score_0'] + df_aux['calib_model_score_1'] + df_aux['calib_model_score_2'] + df_aux['calib_model_score_3'] + df_aux['calib_model_score_4'])/5)#.withColumn('minimo', least(*cols_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 4: Area under ROC(tt) = 0.712157131246\n",
      "Model 4: Area under ROC(tt) = 0.713841078181\n"
     ]
    }
   ],
   "source": [
    "for name in ['maximo','minimo']:\n",
    "    tt_ensemble = df_aux2.select([name, 'label']).rdd.map(lambda r: (r[name], r['label']))\n",
    "    ttmetrics = BinaryClassificationMetrics(tt_ensemble)\n",
    "    print \"Model {}: Area under ROC(tt) = {}\".format(i, str(ttmetrics.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.767949194081\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,2):\n",
    "    print(tr_metrics_vec[i].areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 4: Area under ROC(tt) = 0.713220048003\n"
     ]
    }
   ],
   "source": [
    "for name in ['medio']:\n",
    "    tt_ensemble = df_aux2.select([name, 'label']).rdd.map(lambda r: (r[name], r['label']))\n",
    "    ttmetrics = BinaryClassificationMetrics(tt_ensemble)\n",
    "    print \"Model {}: Area under ROC(tt) = {}\".format(i, str(ttmetrics.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = tr_1s.union(vec_selected_0s[num_tr_dfs].limit(num_1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o32030.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 37 in stage 29996.0 failed 4 times, most recent failure: Lost task 37.3 in stage 29996.0 (TID 4397224, vgddp601hr.dc.sedc.internal.vodafone.com, executor 364): java.io.FileNotFoundException: File does not exist: hdfs://nameservice1/data/udf/vf_es/churn/fbb_tmp/numclidf_tmp_20181130/part-00115-96c44731-02fb-4b41-bb45-ac2d0396c457-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:182)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage29.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage29.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:381)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2775)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2774)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2774)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: File does not exist: hdfs://nameservice1/data/udf/vf_es/churn/fbb_tmp/numclidf_tmp_20181130/part-00115-96c44731-02fb-4b41-bb45-ac2d0396c457-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:182)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage29.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage29.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:381)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-250-6906561333f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \"\"\"\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/Anaconda-2.5.0/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/Anaconda-2.5.0/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o32030.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 37 in stage 29996.0 failed 4 times, most recent failure: Lost task 37.3 in stage 29996.0 (TID 4397224, vgddp601hr.dc.sedc.internal.vodafone.com, executor 364): java.io.FileNotFoundException: File does not exist: hdfs://nameservice1/data/udf/vf_es/churn/fbb_tmp/numclidf_tmp_20181130/part-00115-96c44731-02fb-4b41-bb45-ac2d0396c457-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:182)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage29.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage29.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:381)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2775)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2774)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2774)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: File does not exist: hdfs://nameservice1/data/udf/vf_es/churn/fbb_tmp/numclidf_tmp_20181130/part-00115-96c44731-02fb-4b41-bb45-ac2d0396c457-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:182)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage29.scan_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage29.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:381)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "tr.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "df_aux_tr = tr\n",
    "for i in range(0, num_tr_dfs):\n",
    "    df_aux_tr = models_vec[i].transform(df_aux_tr).withColumn(\"model_score_\" + str(i), getScore(col(\"probability\")).cast(DoubleType())).drop(col('probability'))\n",
    "    # tt_preds_df\n",
    "    df_aux_tr = calibration_vec[i][0].transform(df_aux_tr.drop(col('features')).drop(col('prediction')).drop(col('rawPrediction')))\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ = cols_scores\n",
    "assembler = VectorAssembler(inputCols = features_, outputCol = \"features\")\n",
    "\n",
    "classifier = RandomForestClassifier(featuresCol=\"features\",\\\n",
    "        labelCol=\"label\",\\\n",
    "        maxDepth=10,\\\n",
    "        maxBins=32,\\\n",
    "        minInstancesPerNode=200,\\\n",
    "        impurity=\"gini\",\\\n",
    "        featureSubsetStrategy=\"sqrt\",\\\n",
    "        subsamplingRate=0.7,\\\n",
    "        numTrees=800,\\\n",
    "        seed = 1234,)\n",
    "\n",
    "pipeline = Pipeline(stages= [assembler, classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o31302.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 29896.0 failed 4 times, most recent failure: Lost task 8.3 in stage 29896.0 (TID 4397001, vgddp426hr.dc.sedc.internal.vodafone.com, executor 361): java.io.FileNotFoundException: File does not exist: hdfs://nameservice1/data/udf/vf_es/churn/fbb_tmp/numclidf_tmp_20181130/part-00175-96c44731-02fb-4b41-bb45-ac2d0396c457-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:182)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:381)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2775)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2774)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2774)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: File does not exist: hdfs://nameservice1/data/udf/vf_es/churn/fbb_tmp/numclidf_tmp_20181130/part-00175-96c44731-02fb-4b41-bb45-ac2d0396c457-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:182)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:381)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-243-6906561333f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \"\"\"\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/Anaconda-2.5.0/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/Anaconda-2.5.0/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o31302.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 29896.0 failed 4 times, most recent failure: Lost task 8.3 in stage 29896.0 (TID 4397001, vgddp426hr.dc.sedc.internal.vodafone.com, executor 361): java.io.FileNotFoundException: File does not exist: hdfs://nameservice1/data/udf/vf_es/churn/fbb_tmp/numclidf_tmp_20181130/part-00175-96c44731-02fb-4b41-bb45-ac2d0396c457-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:182)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:381)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2775)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2774)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2774)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: File does not exist: hdfs://nameservice1/data/udf/vf_es/churn/fbb_tmp/numclidf_tmp_20181130/part-00175-96c44731-02fb-4b41-bb45-ac2d0396c457-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:182)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:381)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "tr.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o31774.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 29790.0 failed 4 times, most recent failure: Lost task 8.3 in stage 29790.0 (TID 4396812, vgddp426hr.dc.sedc.internal.vodafone.com, executor 361): java.io.FileNotFoundException: File does not exist: hdfs://nameservice1/data/udf/vf_es/churn/fbb_tmp/numclidf_tmp_20181130/part-00175-96c44731-02fb-4b41-bb45-ac2d0396c457-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:182)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:381)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\n\tat org.apache.spark.ml.classification.Classifier.getNumClasses(Classifier.scala:111)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:121)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:45)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: File does not exist: hdfs://nameservice1/data/udf/vf_es/churn/fbb_tmp/numclidf_tmp_20181130/part-00175-96c44731-02fb-4b41-bb45-ac2d0396c457-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:182)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:381)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-240-ef1561e5271b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_aux_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \"\"\"\n\u001b[1;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/Anaconda-2.5.0/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/Anaconda-2.5.0/lib/python2.7/site-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o31774.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 29790.0 failed 4 times, most recent failure: Lost task 8.3 in stage 29790.0 (TID 4396812, vgddp426hr.dc.sedc.internal.vodafone.com, executor 361): java.io.FileNotFoundException: File does not exist: hdfs://nameservice1/data/udf/vf_es/churn/fbb_tmp/numclidf_tmp_20181130/part-00175-96c44731-02fb-4b41-bb45-ac2d0396c457-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:182)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:381)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1872)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:363)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\n\tat org.apache.spark.ml.classification.Classifier.getNumClasses(Classifier.scala:111)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:121)\n\tat org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:45)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.FileNotFoundException: File does not exist: hdfs://nameservice1/data/udf/vf_es/churn/fbb_tmp/numclidf_tmp_20181130/part-00175-96c44731-02fb-4b41-bb45-ac2d0396c457-c000.snappy.parquet\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:131)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:182)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:381)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "model_final = pipeline.fit(df_aux_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".transform(tr).withColumn(\"model_score\", getScore(col(\"probability\")).cast(DoubleType()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
