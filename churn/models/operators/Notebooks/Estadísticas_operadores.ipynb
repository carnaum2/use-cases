{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "from common.src.main.python.utils.hdfs_generic import *\n",
    "import os\n",
    "\n",
    "MAX_N_EXECUTORS=15\n",
    "MIN_N_EXECUTORS=1\n",
    "N_CORES_EXECUTOR=4\n",
    "EXECUTOR_IDLE_MAX_TIME=120\n",
    "EXECUTOR_MEMORY='32g'\n",
    "DRIVER_MEMORY='16g'\n",
    "N_CORES_DRIVER=1\n",
    "MEMORY_OVERHEAD=N_CORES_EXECUTOR*2048\n",
    "#QUEUE=\"root.datascience.normal\"\n",
    "QUEUE=\"root.BDPtenants.es.medium\"\n",
    "\n",
    "BDA_CORE_VERSION=\"1.0.0\"\n",
    "\n",
    "SPARK_COMMON_OPTS=os.environ.get('SPARK_COMMON_OPTS', '')\n",
    "SPARK_COMMON_OPTS+=\" --executor-memory %s --driver-memory %s\" % (EXECUTOR_MEMORY, DRIVER_MEMORY)\n",
    "SPARK_COMMON_OPTS+=\" --conf spark.shuffle.manager=tungsten-sort\"\n",
    "SPARK_COMMON_OPTS+=\"  --queue %s\" % QUEUE\n",
    "APP_NAME='estadisticas_operadores'\n",
    "\n",
    "# Dynamic allocation configuration\n",
    "SPARK_COMMON_OPTS+=\" --conf spark.driver.allowMultipleContexts=true\"\n",
    "SPARK_COMMON_OPTS+=\" --conf spark.dynamicAllocation.enabled=true\"\n",
    "SPARK_COMMON_OPTS+=\" --conf spark.shuffle.service.enabled=true\"\n",
    "SPARK_COMMON_OPTS+=\" --conf spark.dynamicAllocation.maxExecutors=%s\" % (MAX_N_EXECUTORS)\n",
    "SPARK_COMMON_OPTS+=\" --conf spark.dynamicAllocation.minExecutors=%s\" % (MIN_N_EXECUTORS)\n",
    "SPARK_COMMON_OPTS+=\" --conf spark.dynamicAllocation.executorIdleTimeout=%s\" % (EXECUTOR_IDLE_MAX_TIME)\n",
    "SPARK_COMMON_OPTS+=\" --conf spark.ui.port=58201\"\n",
    "SPARK_COMMON_OPTS+=\" --conf spark.port.maxRetries=200\"\n",
    "SPARK_COMMON_OPTS+=\" --executor-cores=%s\" % (N_CORES_EXECUTOR)\n",
    "SPARK_COMMON_OPTS+=\" --conf spark.app.name=%s\" % (APP_NAME)\n",
    "\n",
    "BDA_ENV = os.environ.get('BDA_USER_HOME', '')\n",
    "\n",
    "# Attach bda-core-ra codebase\n",
    "SPARK_COMMON_OPTS+=\" --files \\\n",
    "{}/scripts/properties/red_agent/nodes.properties,\\\n",
    "{}/scripts/properties/red_agent/nodes-de.properties,\\\n",
    "{}/scripts/properties/red_agent/nodes-es.properties,\\\n",
    "{}/scripts/properties/red_agent/nodes-ie.properties,\\\n",
    "{}/scripts/properties/red_agent/nodes-it.properties,\\\n",
    "{}/scripts/properties/red_agent/nodes-pt.properties,\\\n",
    "{}/scripts/properties/red_agent/nodes-uk.properties\".format(*[BDA_ENV]*7)\n",
    "\n",
    "os.environ[\"SPARK_COMMON_OPTS\"] = SPARK_COMMON_OPTS\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"%s pyspark-shell \" % SPARK_COMMON_OPTS\n",
    "\n",
    "#print os.environ.get('SPARK_COMMON_OPTS', '')\n",
    "#print os.environ.get('PYSPARK_SUBMIT_ARGS', '')\n",
    "\n",
    "sc, sparkSession, sqlContext = run_sc()\n",
    "print sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This literal_eval is needed since \n",
    "# we have to read from a textfile\n",
    "# which is formatted as python objects.\n",
    "# It is totally safe.\n",
    "from ast import literal_eval\n",
    "\n",
    "# Standard Library stuff:\n",
    "from functools import partial\n",
    "from datetime import date, timedelta, datetime\n",
    "\n",
    "# Numpy stuff\n",
    "from numpy import (nan as np_nan, round as np_round, int64 as np_int64)\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Spark stuff\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import (udf, col, decode, when, lit, lower, upper, concat,\n",
    "                                   translate, count, sum as sql_sum, max as sql_max, min as sql_min,\n",
    "                                   round, \n",
    "                                   mean, stddev, datediff,\n",
    "                                   length,\n",
    "                                   countDistinct,\n",
    "                                   hour, date_format, collect_set, collect_list,\n",
    "                                   year, month, dayofmonth,\n",
    "                                   rank, expr, lag, coalesce, row_number,\n",
    "                                   isnull, isnan,\n",
    "                                   unix_timestamp,\n",
    "                                   regexp_replace\n",
    "                                  )\n",
    "\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType, ArrayType, FloatType, StructType, StructField\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql.functions import row_number, col\n",
    "\n",
    "from pyspark.sql import DataFrameStatFunctions as statFunc\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "from subprocess import Popen, PIPE\n",
    "import datetime, calendar\n",
    "from pyspark.sql import functions as F\n",
    "import datetime as dt\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from common.src.main.python.utils.hdfs_generic import *\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "# Spark utils\n",
    "from pyspark.sql.functions import (udf, col, decode, when, lit, lower, concat,\n",
    "                                   translate, count, max, avg, min as sql_min,\n",
    "                                   greatest,\n",
    "                                   least,\n",
    "                                   isnull,\n",
    "                                   isnan,\n",
    "                                   struct,\n",
    "                                   substring,\n",
    "                                   size,\n",
    "                                   length,\n",
    "                                   year,\n",
    "                                   month,\n",
    "                                   dayofmonth,\n",
    "                                   unix_timestamp,\n",
    "                                   date_format,\n",
    "                                   from_unixtime,\n",
    "                                   datediff,\n",
    "                                   to_date,\n",
    "                                   desc,\n",
    "                                   asc,\n",
    "                                   countDistinct,\n",
    "                                   row_number,\n",
    "                                   regexp_replace,\n",
    "                                   lpad,\n",
    "                                   rpad,\n",
    "                                   trim,\n",
    "                                   split,\n",
    "                                   coalesce,\n",
    "                                   array)\n",
    "from pyspark.sql import Row, DataFrame, Column, Window\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType, DateType, ArrayType\n",
    "# from pyspark.ml import Pipeline\n",
    "# from pyspark.ml.classification import RandomForestClassifier\n",
    "# from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler, SQLTransformer, OneHotEncoder\n",
    "# from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "# from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .master(\"yarn\")\n",
    "         .config(\"spark.submit.deployMode\", \"client\")\n",
    "         .config(\"spark.ui.showConsoleProgress\", \"true\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate()\n",
    "         )\n",
    "\n",
    "# sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#import re\n",
    "import subprocess\n",
    "#import sys\n",
    "import time\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "\n",
    "def printHTML(df, sample=7):\n",
    "    display(HTML(tabulate.tabulate([df.columns]+df.take(sample), tablefmt='html', headers='firstrow')))\n",
    "    \n",
    "# Spark utils\n",
    "from pyspark.sql.functions import (array_contains, bround, col, collect_set, concat, count, decode, desc, \n",
    "                                   isnull, length, lit, lower, lpad, max as sql_max, \n",
    "                                   size, struct, substring, sum as sql_sum, \n",
    "                                   translate, trim, udf, upper, when\n",
    "                                  )\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, StructField, StructType\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importo el dataset de test con las variables b√°sicas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_basic = spark.read.load('/data/udf/vf_es/churn/portabPropension_model/test_final_30sept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_basic=test_basic.select('Cust_Agg_L2_fbb_fx_first_days_since_nc','Cust_Agg_L2_fixed_fx_first_days_since_nc','Cust_Agg_L2_mobile_fx_first_days_since_nc',\n",
    "'Cust_Agg_L2_tv_fx_first_days_since_nc','Cust_Agg_fbb_services_nc','Cust_Agg_fixed_services_nc','Cust_Agg_mobile_services_nc',\n",
    "'Cust_Agg_tv_services_nc', 'Bill_N1_Amount_To_Pay','GNV_Data_L2_total_data_volume','Operador_target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para las variables del grupo 1, antes de calcular la media elimino los nulos (los -1)\n",
    "\n",
    "\n",
    "grupo1=['Cust_Agg_L2_fbb_fx_first_days_since_nc','Cust_Agg_L2_fixed_fx_first_days_since_nc','Cust_Agg_L2_mobile_fx_first_days_since_nc',\n",
    "'Cust_Agg_L2_tv_fx_first_days_since_nc','GNV_Data_L2_total_data_volume'] \n",
    "\n",
    "for variable in grupo1[0:len(grupo1)]:\n",
    "        \n",
    "    if variable==grupo1[0]:\n",
    "        \n",
    "        test_filtered=test_basic.filter((test_basic[variable]!=-1))\n",
    "        mean1=test_filtered.groupby('Operador_target').mean(variable)  #con la primera variable creo el dataframe, luego hago inner joins\n",
    "        \n",
    "    else:\n",
    "\n",
    "        test_filtered=test_basic.filter((test_basic[variable]!=-1))\n",
    "\n",
    "        mean=test_filtered.groupby('Operador_target').mean(variable)\n",
    "\n",
    "        mean1=mean1.join(mean,on='Operador_target',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  Operador_target</th><th style=\"text-align: right;\">  avg(Cust_Agg_L2_fbb_fx_first_days_since_nc)</th><th style=\"text-align: right;\">  avg(Cust_Agg_L2_fixed_fx_first_days_since_nc)</th><th style=\"text-align: right;\">  avg(Cust_Agg_L2_mobile_fx_first_days_since_nc)</th><th style=\"text-align: right;\">  avg(Cust_Agg_L2_tv_fx_first_days_since_nc)</th><th style=\"text-align: right;\">  avg(GNV_Data_L2_total_data_volume)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">                                      586.105</td><td style=\"text-align: right;\">                                        1206.55</td><td style=\"text-align: right;\">                                         1708.91</td><td style=\"text-align: right;\">                                     479.749</td><td style=\"text-align: right;\">                             6070.59</td></tr>\n",
       "<tr><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">                                      547.78 </td><td style=\"text-align: right;\">                                        1091.79</td><td style=\"text-align: right;\">                                         1536.05</td><td style=\"text-align: right;\">                                     477.405</td><td style=\"text-align: right;\">                             5636.82</td></tr>\n",
       "<tr><td style=\"text-align: right;\">                4</td><td style=\"text-align: right;\">                                      620.066</td><td style=\"text-align: right;\">                                        1191.72</td><td style=\"text-align: right;\">                                         1831.2 </td><td style=\"text-align: right;\">                                     527.303</td><td style=\"text-align: right;\">                             5309.04</td></tr>\n",
       "<tr><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">                                      614.806</td><td style=\"text-align: right;\">                                        1314.95</td><td style=\"text-align: right;\">                                         1765.91</td><td style=\"text-align: right;\">                                     555.084</td><td style=\"text-align: right;\">                             5224.3 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printHTML(mean1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para el grupo 2, no tengo que eliminar registros (se imputan con 0 o la media, por lo que no se pueden detectar los nulos)\n",
    "\n",
    "means_grupo2=test_basic.groupby('Operador_target').mean('Cust_Agg_fbb_services_nc','Cust_Agg_fixed_services_nc','Cust_Agg_mobile_services_nc',\n",
    "'Cust_Agg_tv_services_nc', 'Bill_N1_Amount_To_Pay','GNV_Data_L2_total_data_volume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  Operador_target</th><th style=\"text-align: right;\">  avg(Cust_Agg_fbb_services_nc)</th><th style=\"text-align: right;\">  avg(Cust_Agg_fixed_services_nc)</th><th style=\"text-align: right;\">  avg(Cust_Agg_mobile_services_nc)</th><th style=\"text-align: right;\">  avg(Cust_Agg_tv_services_nc)</th><th style=\"text-align: right;\">  avg(Bill_N1_Amount_To_Pay)</th><th style=\"text-align: right;\">  avg(GNV_Data_L2_total_data_volume)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">                1</td><td style=\"text-align: right;\">                       0.663499</td><td style=\"text-align: right;\">                         0.699715</td><td style=\"text-align: right;\">                           2.11619</td><td style=\"text-align: right;\">                      0.312149</td><td style=\"text-align: right;\">                     76.9303</td><td style=\"text-align: right;\">                             6070.59</td></tr>\n",
       "<tr><td style=\"text-align: right;\">                3</td><td style=\"text-align: right;\">                       0.694755</td><td style=\"text-align: right;\">                         0.72945 </td><td style=\"text-align: right;\">                           1.99355</td><td style=\"text-align: right;\">                      0.345613</td><td style=\"text-align: right;\">                     77.2682</td><td style=\"text-align: right;\">                             5636.82</td></tr>\n",
       "<tr><td style=\"text-align: right;\">                4</td><td style=\"text-align: right;\">                       0.603307</td><td style=\"text-align: right;\">                         0.645988</td><td style=\"text-align: right;\">                           1.94774</td><td style=\"text-align: right;\">                      0.26363 </td><td style=\"text-align: right;\">                     69.2233</td><td style=\"text-align: right;\">                             5309.04</td></tr>\n",
       "<tr><td style=\"text-align: right;\">                2</td><td style=\"text-align: right;\">                       0.714944</td><td style=\"text-align: right;\">                         0.746474</td><td style=\"text-align: right;\">                           2.16508</td><td style=\"text-align: right;\">                      0.463184</td><td style=\"text-align: right;\">                     82.2189</td><td style=\"text-align: right;\">                             5224.3 </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "printHTML(means_grupo2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
