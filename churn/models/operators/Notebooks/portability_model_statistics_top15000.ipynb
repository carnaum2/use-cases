{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELOS DE PORTA A OPERADORES CON TODAS LAS VARIABLES NUMÉRICAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En lugar de las variables del ids_basic creado, utilizaremos todas las variables numéricas del ids para ver si mejora la performance. Para coger las variables numéricas (excluyendo las no informativas, etc.), utilizaremos el código de Carlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from common.src.main.python.utils.hdfs_generic import *\n",
    "import os\n",
    "\n",
    "MAX_N_EXECUTORS=15\n",
    "MIN_N_EXECUTORS=1\n",
    "N_CORES_EXECUTOR=4\n",
    "EXECUTOR_IDLE_MAX_TIME=120\n",
    "EXECUTOR_MEMORY='32g'\n",
    "DRIVER_MEMORY='16g'\n",
    "N_CORES_DRIVER=1\n",
    "MEMORY_OVERHEAD=N_CORES_EXECUTOR*2048\n",
    "#QUEUE=\"root.datascience.normal\"\n",
    "QUEUE=\"root.BDPtenants.es.medium\"\n",
    "\n",
    "BDA_CORE_VERSION=\"1.0.0\"\n",
    "\n",
    "SPARK_COMMON_OPTS=os.environ.get('SPARK_COMMON_OPTS', '')\n",
    "SPARK_COMMON_OPTS+=\" --executor-memory %s --driver-memory %s\" % (EXECUTOR_MEMORY, DRIVER_MEMORY)\n",
    "SPARK_COMMON_OPTS+=\" --conf spark.shuffle.manager=tungsten-sort\"\n",
    "SPARK_COMMON_OPTS+=\"  --queue %s\" % QUEUE\n",
    "APP_NAME='new_portability_model'\n",
    "\n",
    "# Dynamic allocation configuration\n",
    "SPARK_COMMON_OPTS+=\" --conf spark.driver.allowMultipleContexts=true\"\n",
    "SPARK_COMMON_OPTS+=\" --conf spark.dynamicAllocation.enabled=true\"\n",
    "SPARK_COMMON_OPTS+=\" --conf spark.shuffle.service.enabled=true\"\n",
    "SPARK_COMMON_OPTS+=\" --conf spark.dynamicAllocation.maxExecutors=%s\" % (MAX_N_EXECUTORS)\n",
    "SPARK_COMMON_OPTS+=\" --conf spark.dynamicAllocation.minExecutors=%s\" % (MIN_N_EXECUTORS)\n",
    "SPARK_COMMON_OPTS+=\" --conf spark.dynamicAllocation.executorIdleTimeout=%s\" % (EXECUTOR_IDLE_MAX_TIME)\n",
    "SPARK_COMMON_OPTS+=\" --conf spark.ui.port=58201\"\n",
    "SPARK_COMMON_OPTS+=\" --conf spark.port.maxRetries=200\"\n",
    "SPARK_COMMON_OPTS+=\" --executor-cores=%s\" % (N_CORES_EXECUTOR)\n",
    "SPARK_COMMON_OPTS+=\" --conf spark.app.name=%s\" % (APP_NAME)\n",
    "\n",
    "BDA_ENV = os.environ.get('BDA_USER_HOME', '')\n",
    "\n",
    "# Attach bda-core-ra codebase\n",
    "SPARK_COMMON_OPTS+=\" --files \\\n",
    "{}/scripts/properties/red_agent/nodes.properties,\\\n",
    "{}/scripts/properties/red_agent/nodes-de.properties,\\\n",
    "{}/scripts/properties/red_agent/nodes-es.properties,\\\n",
    "{}/scripts/properties/red_agent/nodes-ie.properties,\\\n",
    "{}/scripts/properties/red_agent/nodes-it.properties,\\\n",
    "{}/scripts/properties/red_agent/nodes-pt.properties,\\\n",
    "{}/scripts/properties/red_agent/nodes-uk.properties\".format(*[BDA_ENV]*7)\n",
    "\n",
    "os.environ[\"SPARK_COMMON_OPTS\"] = SPARK_COMMON_OPTS\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"%s pyspark-shell \" % SPARK_COMMON_OPTS\n",
    "\n",
    "#print os.environ.get('SPARK_COMMON_OPTS', '')\n",
    "#print os.environ.get('PYSPARK_SUBMIT_ARGS', '')\n",
    "\n",
    "sc, sparkSession, sqlContext = run_sc()\n",
    "print sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This literal_eval is needed since \n",
    "# we have to read from a textfile\n",
    "# which is formatted as python objects.\n",
    "# It is totally safe.\n",
    "from ast import literal_eval\n",
    "\n",
    "# Standard Library stuff:\n",
    "from functools import partial\n",
    "from datetime import date, timedelta, datetime\n",
    "\n",
    "# Numpy stuff\n",
    "from numpy import (nan as np_nan, round as np_round, int64 as np_int64)\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Spark stuff\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import (udf, col, decode, when, lit, lower, upper, concat,\n",
    "                                   translate, count, sum as sql_sum, max as sql_max, min as sql_min,\n",
    "                                   round, \n",
    "                                   mean, stddev, datediff,\n",
    "                                   length,\n",
    "                                   countDistinct,\n",
    "                                   hour, date_format, collect_set, collect_list,\n",
    "                                   year, month, dayofmonth,\n",
    "                                   rank, expr, lag, coalesce, row_number,\n",
    "                                   isnull, isnan,\n",
    "                                   unix_timestamp,\n",
    "                                   regexp_replace\n",
    "                                  )\n",
    "\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType, ArrayType, FloatType, StructType, StructField\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.sql.functions import row_number, col\n",
    "\n",
    "from pyspark.sql import DataFrameStatFunctions as statFunc\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "from subprocess import Popen, PIPE\n",
    "import datetime, calendar\n",
    "from pyspark.sql import functions as F\n",
    "import datetime as dt\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from common.src.main.python.utils.hdfs_generic import *\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "# Spark utils\n",
    "from pyspark.sql.functions import (udf, col, decode, when, lit, lower, concat,\n",
    "                                   translate, count, max, avg, min as sql_min,\n",
    "                                   greatest,\n",
    "                                   least,\n",
    "                                   isnull,\n",
    "                                   isnan,\n",
    "                                   struct,\n",
    "                                   substring,\n",
    "                                   size,\n",
    "                                   length,\n",
    "                                   year,\n",
    "                                   month,\n",
    "                                   dayofmonth,\n",
    "                                   unix_timestamp,\n",
    "                                   date_format,\n",
    "                                   from_unixtime,\n",
    "                                   datediff,\n",
    "                                   to_date,\n",
    "                                   desc,\n",
    "                                   asc,\n",
    "                                   countDistinct,\n",
    "                                   row_number,\n",
    "                                   regexp_replace,\n",
    "                                   lpad,\n",
    "                                   rpad,\n",
    "                                   trim,\n",
    "                                   split,\n",
    "                                   coalesce,\n",
    "                                   array)\n",
    "from pyspark.sql import Row, DataFrame, Column, Window\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType, DateType, ArrayType\n",
    "# from pyspark.ml import Pipeline\n",
    "# from pyspark.ml.classification import RandomForestClassifier\n",
    "# from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler, SQLTransformer, OneHotEncoder\n",
    "# from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "# from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .master(\"yarn\")\n",
    "         .config(\"spark.submit.deployMode\", \"client\")\n",
    "         .config(\"spark.ui.showConsoleProgress\", \"true\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate()\n",
    "         )\n",
    "\n",
    "# sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#import re\n",
    "import subprocess\n",
    "#import sys\n",
    "import time\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "import tabulate\n",
    "\n",
    "def printHTML(df, sample=7):\n",
    "    display(HTML(tabulate.tabulate([df.columns]+df.take(sample), tablefmt='html', headers='firstrow')))\n",
    "    \n",
    "# Spark utils\n",
    "from pyspark.sql.functions import (array_contains, bround, col, collect_set, concat, count, decode, desc, \n",
    "                                   isnull, length, lit, lower, lpad, max as sql_max, \n",
    "                                   size, struct, substring, sum as sql_sum, \n",
    "                                   translate, trim, udf, upper, when\n",
    "                                  )\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType, StructField, StructType\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "USECASES_SRC = os.path.join(os.environ.get('BDA_USER_HOME', ''), \"repositorios\", \"use-cases\")\n",
    "if USECASES_SRC not in sys.path: \n",
    "    sys.path.append(USECASES_SRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PYKHAOS_SRC = os.path.join(os.environ.get('BDA_USER_HOME', ''), \"repositorios\")\n",
    "if PYKHAOS_SRC not in sys.path: \n",
    "    sys.path.append(PYKHAOS_SRC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escojo las variables numéricas  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/var/SP/data/home/carnaum2/ids/amdocs_inf_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.main.python.configuration.constants import ENVIRONMENT\n",
    "from src.main.python.utils.spark_creator import SparkCreator\n",
    "from src.main.python.pipelines.billing import Billing\n",
    "from src.main.python.pipelines.breakdowns import Breakdowns\n",
    "from src.main.python.pipelines.call_centre_calls import CallCentreCalls\n",
    "from src.main.python.pipelines.campaigns import Campaigns\n",
    "from src.main.python.pipelines.claims import Claims\n",
    "from src.main.python.pipelines.competitors_web import CompWeb\n",
    "from src.main.python.pipelines.customer import Customer\n",
    "from src.main.python.pipelines.customer_aggregations import Customer_Aggregations\n",
    "from src.main.python.pipelines.penalties import PenaltiesCustomer, PenaltiesServices\n",
    "from src.main.python.pipelines.device_catalogue import Device_Catalogue\n",
    "from src.main.python.pipelines.geneva_traffic import GenevaVoiceTypeUsage\n",
    "from src.main.python.pipelines.geneva_traffic import GenevaVoiceUsage\n",
    "from src.main.python.pipelines.geneva_traffic import GenevaRoamVoiceUsage\n",
    "from src.main.python.pipelines.geneva_traffic import GenevaDataUsage\n",
    "from src.main.python.pipelines.geneva_traffic import GenevaRoamDataUsage\n",
    "from src.main.python.pipelines.mobile_spinners_extractor import Mobile_spinners_extractor\n",
    "from src.main.python.pipelines.netscout import Netscout\n",
    "from src.main.python.pipelines.orders import Orders\n",
    "from src.main.python.pipelines.orders_aggregations import OrdersAgg\n",
    "from src.main.python.pipelines.permsandprefs import Perms_and_prefs\n",
    "from src.main.python.pipelines.services import Services\n",
    "from src.main.python.pipelines.services_problems import ServiceProblems\n",
    "from src.main.python.pipelines.tech_suprt import TechSupport\n",
    "from src.main.python.pipelines.tgs import Tgs\n",
    "from src.main.python.pipelines.tnps import Tnps\n",
    "from src.main.python.pipelines.orders_sla import Orders_sla\n",
    "from src.main.python.pipelines.tickets import Tickets\n",
    "from src.main.python.pipelines.refund import Refund\n",
    "sc = SparkCreator()\n",
    "date = \"20191014\"\n",
    "module_constructors = (Customer(sc, date, ENVIRONMENT),\n",
    "                       Services(sc, date, ENVIRONMENT),\n",
    "                       Customer_Aggregations(sc, date, ENVIRONMENT),\n",
    "                       Billing(sc, date, ENVIRONMENT),\n",
    "                       Campaigns(sc, date, date, ENVIRONMENT),\n",
    "                       GenevaVoiceTypeUsage(sc, date, date, ENVIRONMENT),\n",
    "                       GenevaVoiceUsage(sc, date, date, ENVIRONMENT),\n",
    "                       GenevaDataUsage(sc, date, date, ENVIRONMENT),\n",
    "                       #GenevaRoamVoiceUsage(sc, date, date, ENVIRONMENT),\n",
    "                       #GenevaRoamDataUsage(sc, date, date, ENVIRONMENT),\n",
    "                       Orders(sc, date, date, ENVIRONMENT),\n",
    "                       OrdersAgg(sc, date, date, ENVIRONMENT),\n",
    "                       PenaltiesCustomer(sc, date, ENVIRONMENT),\n",
    "                       PenaltiesServices(sc, date, ENVIRONMENT),\n",
    "                       Device_Catalogue(sc, date, date, ENVIRONMENT),\n",
    "                       Perms_and_prefs(sc, date, ENVIRONMENT),\n",
    "                       CallCentreCalls(sc, date, date, ENVIRONMENT),\n",
    "                       Tnps(sc, date, date, ENVIRONMENT),\n",
    "                       Tgs(sc, date, ENVIRONMENT),\n",
    "                       Claims(sc, date, ENVIRONMENT),\n",
    "                       Breakdowns(sc, date, ENVIRONMENT),\n",
    "                       TechSupport(sc, date, ENVIRONMENT),\n",
    "                       Netscout(sc, date, date, ENVIRONMENT),\n",
    "                       CompWeb(sc, date, date, ENVIRONMENT),\n",
    "                       ServiceProblems(sc, date, ENVIRONMENT),\n",
    "                       Mobile_spinners_extractor(sc, date, ENVIRONMENT),\n",
    "                       Orders_sla(sc, date, ENVIRONMENT),\n",
    "                       Refund(sc, date, ENVIRONMENT),\n",
    "                       #Tickets(sc, date, ENVIRONMENT)\n",
    "                       )\n",
    "na_map = {}\n",
    "for module in module_constructors:\n",
    "    metadata = module.set_module_metadata()\n",
    "    na_map.update(metadata)\n",
    "final_map = {colmn: na_map[colmn][0] for colmn in na_map.keys()\n",
    "             if colmn in na_map.keys() and na_map[colmn][1] != \"id\"}\n",
    "categ_map = {colmn: na_map[colmn][0] for colmn in na_map.keys()\n",
    "             if colmn in na_map.keys() and na_map[colmn][1] == \"categorical\"}\n",
    "numeric_map = {colmn: na_map[colmn][0] for colmn in na_map.keys()\n",
    "               if colmn in na_map.keys() and na_map[colmn][1] == \"numerical\"}\n",
    "date_map = {colmn: na_map[colmn][0] for colmn in na_map.keys()\n",
    "            if colmn in na_map.keys() and na_map[colmn][1] == \"date\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_variables=numeric_map.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_variables.append('msisdn') #añado el msisdn para hacer el join después"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraigo el ids con las variables numéricas seleccionadas  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_numeric_selection(year_, month_, day_):\n",
    "\n",
    "\n",
    "    ids_completo = (spark.read.load(\n",
    "            '/data/udf/vf_es/amdocs_inf_dataset/amdocs_ids_service_level/year=' + year_ + '/month=' + month_ + '/day=' + day_))\n",
    "\n",
    "    ids_numeric=ids_completo.select(numeric_variables) #aqui cojo las variables que se han seleccionado\n",
    "\n",
    "    return ids_numeric\n",
    "\n",
    "#Guardo este ids para train y test y luego hago inner join on msisdn con las columnas de target que habiamos creado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_julio_numeric=ids_numeric_selection('2019','7','31')\n",
    "ids_sept_numeric=ids_numeric_selection('2019','9','30') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricas=[item[0] for item in ids_julio_numeric.dtypes if item[1].startswith('string')] #elimino estas variables que no son numericas: no deberian aparecer (carlos lo va a corregir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tgs_ind_riesgo_o2',\n",
       " 'tgs_ind_riesgo_mm',\n",
       " 'tgs_ind_riesgo_mv',\n",
       " 'tgs_meses_fin_dto_ok',\n",
       " 'CCC_L2_bucket_1st_interaction',\n",
       " 'CCC_L2_bucket_latest_interaction',\n",
       " 'CCC_L2_first_interaction',\n",
       " 'Cust_Agg_flag_prepaid_nc',\n",
       " 'tgs_ind_riesgo_max',\n",
       " 'tgs_sol_24m',\n",
       " 'CCC_L2_latest_interaction',\n",
       " 'tgs_tg_marta',\n",
       " 'tgs_blinda_bi_pos_n12',\n",
       " 'msisdn']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_julio_numeric=ids_julio_numeric.drop('tgs_ind_riesgo_o2',\n",
    " 'tgs_ind_riesgo_mm',\n",
    " 'tgs_ind_riesgo_mv',\n",
    " 'tgs_meses_fin_dto_ok',\n",
    " 'CCC_L2_bucket_1st_interaction',\n",
    " 'CCC_L2_bucket_latest_interaction',\n",
    " 'CCC_L2_first_interaction',\n",
    " 'Cust_Agg_flag_prepaid_nc',\n",
    " 'tgs_ind_riesgo_max',\n",
    " 'tgs_sol_24m',\n",
    " 'CCC_L2_latest_interaction',\n",
    " 'tgs_tg_marta',\n",
    " 'tgs_blinda_bi_pos_n12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_sept_numeric=ids_sept_numeric.drop('tgs_ind_riesgo_o2',\n",
    " 'tgs_ind_riesgo_mm',\n",
    " 'tgs_ind_riesgo_mv',\n",
    " 'tgs_meses_fin_dto_ok',\n",
    " 'CCC_L2_bucket_1st_interaction',\n",
    " 'CCC_L2_bucket_latest_interaction',\n",
    " 'CCC_L2_first_interaction',\n",
    " 'Cust_Agg_flag_prepaid_nc',\n",
    " 'tgs_ind_riesgo_max',\n",
    " 'tgs_sol_24m',\n",
    " 'CCC_L2_latest_interaction',\n",
    " 'tgs_tg_marta',\n",
    " 'tgs_blinda_bi_pos_n12')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraigo el train y test etiquetado que habíamos utilizado en el otro modelo, para etiquetar el nuevo train y test (con un inner join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_basic= spark.read.load('/data/udf/vf_es/churn/portabPropension_model/train_final_31julio')\n",
    "test_basic= spark.read.load('/data/udf/vf_es/churn/portabPropension_model/test_final_30sept')\n",
    "\n",
    "msisdn_target_train=train_basic.select('msisdn','Operador_target','masmovil','movistar','orange','otros')\n",
    "msisdn_target_test=test_basic.select('msisdn','Operador_target','masmovil','movistar','orange','otros')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_numeric_final=ids_julio_numeric.join(msisdn_target_train,on='msisdn',how='inner')\n",
    "test_numeric_final=ids_sept_numeric.join(msisdn_target_test,on='msisdn',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169429"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_numeric_final=train_numeric_final.cache()\n",
    "train_numeric_final.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161893"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_numeric_final=test_numeric_final.cache()\n",
    "test_numeric_final.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|Operador_target|count|\n",
      "+---------------+-----+\n",
      "|              1|37039|\n",
      "|              3|41050|\n",
      "|              4|33022|\n",
      "|              2|58318|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_numeric_final.groupby('Operador_target').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplico el modelo sobre el nuevo train y test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento previo de los datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_elim=['msisdn','Operador_target','masmovil','movistar','orange','otros']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [i for i in train_numeric_final.columns if i not in variables_elim] #cojo las variables predictoras para el assemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5766"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexer_assembler(df_no_transformed): \n",
    "    \n",
    "    assembler = VectorAssembler(inputCols=variables, outputCol=\"features\")\n",
    "\n",
    "    stages = [assembler]\n",
    "\n",
    "    pipeline = Pipeline(stages = stages)\n",
    "    \n",
    "    pipeline_fit = pipeline.fit(df_no_transformed)\n",
    "    \n",
    "    df_transformed=pipeline_fit.transform(df_no_transformed)\n",
    "    \n",
    "    return df_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model2=indexer_assembler(train_numeric_final)\n",
    "test_model2=indexer_assembler(test_numeric_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "#LIFT\n",
    "\n",
    "import utils_model\n",
    "from utils_model import get_lift\n",
    "getScore = udf(lambda prob: float(prob[1]), DoubleType())\n",
    "\n",
    "#FEATURE IMPORTANCE\n",
    "\n",
    "def ExtractFeatureImp(featureImp, dataset, featuresCol):\n",
    "   list_extract = []\n",
    "   for i in dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "       list_extract = list_extract + dataset.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "   varlist = pd.DataFrame(list_extract)\n",
    "   varlist['score'] = varlist['idx'].apply(lambda x: featureImp[x])\n",
    "   return(varlist.sort_values('score', ascending = False))\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estadísticas básicas para el top 15.000: del test_basic cojo las variables y las clasifico según imputación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_basic=test_basic.select('msisdn','Cust_Agg_L2_fbb_fx_first_days_since_nc','Cust_Agg_L2_fixed_fx_first_days_since_nc','Cust_Agg_L2_mobile_fx_first_days_since_nc',\n",
    "'Cust_Agg_L2_tv_fx_first_days_since_nc','Cust_Agg_fbb_services_nc','Cust_Agg_fixed_services_nc','Cust_Agg_mobile_services_nc',\n",
    "'Cust_Agg_tv_services_nc', 'Bill_N1_Amount_To_Pay','GNV_Data_L2_total_data_volume')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables que se imputan con -1:\n",
    "\n",
    "grupo1=['Cust_Agg_L2_fbb_fx_first_days_since_nc','Cust_Agg_L2_fixed_fx_first_days_since_nc','Cust_Agg_L2_mobile_fx_first_days_since_nc',\n",
    "'Cust_Agg_L2_tv_fx_first_days_since_nc'] \n",
    "\n",
    "#Variables que se imputan con 0 o la media: los nulos no se pueden identificar\n",
    "\n",
    "grupo2=['Cust_Agg_fbb_services_nc','Cust_Agg_fixed_services_nc','Cust_Agg_mobile_services_nc',\n",
    "'Cust_Agg_tv_services_nc', 'Bill_N1_Amount_To_Pay','GNV_Data_L2_total_data_volume']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MASMOVIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Balanceando clases:\n",
    "\n",
    "n=float(37039)/float(169429-37039) #proporcion que hay que coger de los que no solicitan porta a masmovil (misma que los que sí: nº clientes que van a masmovil)\n",
    "\n",
    "train_masmovil=train_model2.filter(train_model2['Operador_target']==1).union(train_model2.filter(train_model2['Operador_target']!=1).sample(False, n,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target='masmovil'\n",
    "\n",
    "#model = RandomForestClassifier(featuresCol = 'features', labelCol = target, maxDepth=8, numTrees=3000)\n",
    "\n",
    "model = GBTClassifier(featuresCol = 'features',labelCol=target, maxDepth=5,maxIter=20)\n",
    "\n",
    "model2_masmovil = model.fit(train_masmovil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_masmovil_test=model2_masmovil.transform(test_model2)\n",
    "pred_masmovil_test=pred_masmovil_test.withColumn(\"score\", getScore(col(\"probability\")).cast(DoubleType()))\n",
    "pred_masmovil_test=pred_masmovil_test.orderBy('score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_masmovil_top=pred_masmovil_test.limit(15000)\n",
    "pred_masmovil_top=pred_masmovil_top.select('msisdn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_masmovil=test_basic.join(pred_masmovil_top, on='msisdn',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[546.7833352674945, 982.5836843272002, 1969.7710666666667, 395.87844801454986, 8033.274144270833]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "#Para las variables del grupo 1, antes de calcular la media elimino los nulos (los -1)\n",
    "\n",
    "means_masmovil=[]\n",
    "\n",
    "for variable in grupo1[0:len(grupo1)]:\n",
    "        \n",
    "    df_masmovil_filtered=df_masmovil.filter((df_masmovil[variable]!=-1))\n",
    "    \n",
    "    mean=df_masmovil_filtered.agg(avg(col(variable))).collect()[0][0]\n",
    "        \n",
    "    means_masmovil.append(mean)\n",
    "    \n",
    "print(means_masmovil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5746666666666667, 0.6014666666666667, 2.0316666666666667, 0.22806666666666667, 66.23533682158777, 8033.274144270833]\n"
     ]
    }
   ],
   "source": [
    "#Para el grupo 2, no tengo que eliminar registros\n",
    "\n",
    "means_masmovil_2=[]\n",
    "\n",
    "for variable in grupo2[0:len(grupo2)]:\n",
    "\n",
    "    mean=df_masmovil.agg(avg(col(variable))).collect()[0][0]\n",
    "    \n",
    "    means_masmovil_2.append(mean)\n",
    "    \n",
    "print(means_masmovil_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOVISTAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=float(58318)/float(169429-58318) #proporcion que hay que coger de los que no solicitan porta a masmovil (misma que los que sí: nº clientes que van a masmovil)\n",
    "\n",
    "train_movistar=train_model2.filter(train_model2['Operador_target']==2).union(train_model2.filter(train_model2['Operador_target']!=2).sample(False, n,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target='movistar'\n",
    "#model = RandomForestClassifier(featuresCol = 'features', labelCol = target, maxDepth=8, numTrees=3000)\n",
    "model = GBTClassifier(featuresCol = 'features',labelCol=target, maxDepth=5,maxIter=20)\n",
    "model2_movistar = model.fit(train_movistar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_movistar_test=model2_movistar.transform(test_model2)\n",
    "pred_movistar_test=pred_movistar_test.withColumn(\"score\", getScore(col(\"probability\")).cast(DoubleType()))\n",
    "pred_movistar_test=pred_movistar_test.orderBy('score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_movistar_top=pred_movistar_test.limit(15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_movistar_top=pred_movistar_top.select('msisdn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movistar=test_basic.join(pred_movistar_top, on='msisdn',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "#Para las variables del grupo 1, antes de calcular la media elimino los nulos (los -1)\n",
    "\n",
    "means_movistar=[]\n",
    "\n",
    "for variable in grupo1[0:len(grupo1)]:\n",
    "        \n",
    "    df_movistar_filtered=df_movistar.filter((df_movistar[variable]!=-1))\n",
    "    \n",
    "    mean=df_movistar_filtered.agg(avg(col(variable))).collect()[0][0]\n",
    "        \n",
    "    means_movistar.append(mean)\n",
    "    \n",
    "print(means_movistar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para el grupo 2, no tengo que eliminar registros\n",
    "\n",
    "means_movistar_2=[]\n",
    "\n",
    "for variable in grupo2[0:len(grupo2)]:\n",
    "\n",
    "    mean=df_movistar.agg(avg(col(variable))).collect()[0][0]\n",
    "    \n",
    "    means_movistar_2.append(mean)\n",
    "    \n",
    "print(means_movistar_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=float(41050)/float(169429-41050) #proporcion que hay que coger de los que no solicitan porta a masmovil (misma que los que sí: nº clientes que van a masmovil)\n",
    "\n",
    "train_orange=train_model2.filter(train_model2['Operador_target']==3).union(train_model2.filter(train_model2['Operador_target']!=3).sample(False, n,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target='orange'\n",
    "#model = RandomForestClassifier(featuresCol = 'features', labelCol = target, maxDepth=8, numTrees=3000)\n",
    "model = GBTClassifier(featuresCol = 'features',labelCol=target, maxDepth=5,maxIter=20)\n",
    "model2_orange = model.fit(train_orange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_orange_test=model2_orange.transform(test_model2)\n",
    "pred_orange_test=pred_orange_test.withColumn(\"score\", getScore(col(\"probability\")).cast(DoubleType()))\n",
    "pred_orange_test=pred_orange_test.orderBy('score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_orange_top=pred_orange_test.limit(15000)\n",
    "pred_orange_top=pred_orange_top.select('msisdn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orange=test_basic.join(pred_orange_top, on='msisdn',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para las variables del grupo 1, antes de calcular la media elimino los nulos (los -1)\n",
    "\n",
    "means_orange=[]\n",
    "\n",
    "for variable in grupo1[0:len(grupo1)]:\n",
    "        \n",
    "    df_orange_filtered=df_orange.filter((df_orange[variable]!=-1))\n",
    "    \n",
    "    mean=df_orange_filtered.agg(avg(col(variable))).collect()[0][0]\n",
    "        \n",
    "    means_orange.append(mean)\n",
    "    \n",
    "print(means_orange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para el grupo 2, no tengo que eliminar registros\n",
    "\n",
    "means_orange_2=[]\n",
    "\n",
    "for variable in grupo2[0:len(grupo2)]:\n",
    "\n",
    "    mean=df_orange.agg(avg(col(variable))).collect()[0][0]\n",
    "    \n",
    "    means_orange_2.append(mean)\n",
    "    \n",
    "print(means_orange_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTROS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=float(33022)/float(169429-33022) #proporcion que hay que coger de los que no solicitan porta a masmovil (misma que los que sí: nº clientes que van a masmovil)\n",
    "\n",
    "train_otros=train_model2.filter(train_model2['Operador_target']==4).union(train_model2.filter(train_model2['Operador_target']!=4).sample(False, n,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target='otros'\n",
    "#model = RandomForestClassifier(featuresCol = 'features', labelCol = target, maxDepth=8, numTrees=3000)\n",
    "model = GBTClassifier(featuresCol = 'features',labelCol=target, maxDepth=5,maxIter=20)\n",
    "model2_otros = model.fit(train_otros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_otros_test=model2_otros.transform(test_model2)\n",
    "pred_otros_test=pred_otros_test.withColumn(\"score\", getScore(col(\"probability\")).cast(DoubleType()))\n",
    "pred_otros_test=pred_otros_test.orderBy('score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_otros_top=pred_otros_test.limit(15000)\n",
    "pred_otros_top=pred_otros_top.select('msisdn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_otros=test_basic.join(pred_otros_top, on='msisdn',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para las variables del grupo 1, antes de calcular la media elimino los nulos (los -1)\n",
    "\n",
    "means_otros=[]\n",
    "\n",
    "for variable in grupo1[0:len(grupo1)]:\n",
    "        \n",
    "    df_otros_filtered=df_otros.filter((df_otros[variable]!=-1))\n",
    "    \n",
    "    mean=df_otros_filtered.agg(avg(col(variable))).collect()[0][0]\n",
    "        \n",
    "    means_otros.append(mean)\n",
    "    \n",
    "print(means_otros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para el grupo 2, no tengo que eliminar registros\n",
    "\n",
    "means_otros_2=[]\n",
    "\n",
    "for variable in grupo2[0:len(grupo2)]:\n",
    "\n",
    "    mean=df_otros.agg(avg(col(variable))).collect()[0][0]\n",
    "    \n",
    "    means_otros_2.append(mean)\n",
    "    \n",
    "print(means_otros_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
