{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20181119-082337 [INFO ] Logging to file /var/SP/data/home/csanc109/logging/out_20181119_082337.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_initialize spark\n",
      "Ended spark session: 31.3939368725 secs | default parallelism=2\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import datetime as dt\n",
    "DEVEL_SRC = os.path.join(os.environ.get('BDA_USER_HOME', ''), \"src\", \"devel\")\n",
    "if DEVEL_SRC not in sys.path:\n",
    "    sys.path.append(DEVEL_SRC)\n",
    "\n",
    "USECASES_SRC = os.path.join(DEVEL_SRC, \"use-cases\") # TODO when - is removed, remove also this line and adapt imports\n",
    "if USECASES_SRC not in sys.path: \n",
    "    sys.path.append(USECASES_SRC)\n",
    "    \n",
    "AMDOCS_SRC = os.path.join(DEVEL_SRC, \"amdocs_informational_dataset\") # TODO when - is removed, remove also this line and adapt imports\n",
    "if AMDOCS_SRC not in sys.path: \n",
    "    sys.path.append(AMDOCS_SRC)\n",
    "    \n",
    "import pykhaos.utils.custom_logger as clogger\n",
    "logging_file = os.path.join(os.environ.get('BDA_USER_HOME', ''), \"logging\",\n",
    "                                    \"out_\" + dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".log\")\n",
    "logger = clogger.configure_logger(log_filename=logging_file, std_channel=sys.stderr, logger_name=\"\")\n",
    "logger.info(\"Logging to file {}\".format(logging_file))    \n",
    "    \n",
    "    \n",
    "from project.project_generic import Project\n",
    "\n",
    "\n",
    "import pykhaos.utils.notebooks as nb\n",
    "\n",
    "project_obj = Project(\"CCC model\", \"CCC model\")   \n",
    "\n",
    "RUNNING_FROM_NOTEBOOK = nb.isnotebook()\n",
    "import matplotlib.pyplot as plt\n",
    "if RUNNING_FROM_NOTEBOOK:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    %matplotlib inline  \n",
    "    \n",
    "#logger = my_project.logger\n",
    "\n",
    "if not RUNNING_FROM_NOTEBOOK:\n",
    "    args = my_project.arg_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark=project_obj.spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "/var/SP/data/home/csanc109/src/devel/use-cases/churn\n",
      "Reading config from file /var/SP/data/home/csanc109/src/devel/use-cases/churn/input/datapreparation_churn.yaml\n",
      "Reading internal config from file /var/SP/data/home/csanc109/src/devel/use-cases/churn/datapreparation/config/internal_config.yaml\n",
      "----- CHECKING INPUT PARAMETERS ------\n",
      "if\n",
      "elif\n",
      "20180807\n",
      "if\n",
      "20180814\n",
      "if\n",
      "20180821\n",
      "else\n",
      "20180831\n",
      "if\n",
      "elif\n",
      "20180907\n",
      "if\n",
      "20180914\n",
      "if\n",
      "20180921\n",
      "else\n",
      "20180930\n",
      "('20180731', 8, '20180930')\n",
      "----- INPUT PARAMETERS OK! ------\n",
      "{'closing_day': 20180731,\n",
      " 'cycles_horizon': 8,\n",
      " 'discard_vars': ['order_id'],\n",
      " 'discarded_cycles': 4,\n",
      " 'internal_config_file': '/var/SP/data/home/csanc109/src/devel/use-cases/churn/datapreparation/config/internal_config.yaml',\n",
      " 'labeled': True,\n",
      " 'level': 'service',\n",
      " 'model_target': 'port',\n",
      " 'save_car': True,\n",
      " 'segment_filter': 'mobileandfbb',\n",
      " 'service_set': 'mobile',\n",
      " 'sources': {'ids': {'address': False,\n",
      "                     'billing': True,\n",
      "                     'call_centre_calls': False,\n",
      "                     'campaigns': True,\n",
      "                     'customer': True,\n",
      "                     'customer_aggregations': True,\n",
      "                     'customer_penalties': True,\n",
      "                     'device_catalogue': True,\n",
      "                     'geneva': True,\n",
      "                     'netscout': False,\n",
      "                     'orders': True,\n",
      "                     'spinners': True,\n",
      "                     'tnps': True}},\n",
      " 'user_config_file': '/var/SP/data/home/csanc109/src/devel/use-cases/churn/input/datapreparation_churn.yaml'}\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline \n",
    "from churn.datapreparation.data_loader import get_port_requests_table\n",
    "from churn.datapreparation.config import Config\n",
    "\n",
    "config_obj = Config(None)\n",
    "\n",
    "port_start_date = \"20180901\"\n",
    "port_end_date = \"20180930\"\n",
    "ref_date = \"20181021\"\n",
    "\n",
    "select_cols = [\"portout_date\",  \"msisdn_a\", \"label\", \"days_from_portout\"]\n",
    "\n",
    "#df_port = get_port_requests_table(spark, config_obj, port_start_date, port_end_date, ref_date, select_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_port' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-8c209dbc9839>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mportout_dates_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_port\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'portout_date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mportout_dates_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mportout_dates_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mportout_dates_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mportout_dates_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mccc_start_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"20180801\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_port' is not defined"
     ]
    }
   ],
   "source": [
    "portout_dates_list = df_port.select('portout_date').distinct().collect()\n",
    "portout_dates_min = min(portout_dates_list)\n",
    "portout_dates_max = max(portout_dates_list)\n",
    "\n",
    "ccc_start_date = \"20180801\"\n",
    "ccc_end_date = \"20180930\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from engine.call_centre_calls import CallCentreCalls\n",
    "\n",
    "#Call Centre Calls\n",
    "ccc = CallCentreCalls(spark)\n",
    "df_ccc = ccc.get_ccc_service_df(ccc_end_date,ccc_start_date)\n",
    "df_all = ccc.all_interactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HDFS_DIR_DATA = '/user/csanc109/projects/churn/data/ccc_model/'\n",
    "# df_all = df_all.repartition(1)\n",
    "# df_all.write.mode('overwrite').format('csv').option(\"header\", \"true\").save('/user/csanc109/projects/churn/data/ccc_model/all_interactions_20181114.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LAS QUE NO CRUZAN\n",
    "# from pyspark.sql.functions import desc, upper\n",
    "# df_nocruzan=df_all.where(col(\"Bucket\")==\"NA\").groupby(\"source\", \"INT_Tipo\", \"INT_Subtipo\", \"INT_Razon\", \"INT_Resultado\", \"X_WORKGROUP\", \"direction\", \"type_td\").agg(sql_count(\"*\").alias(\"count\")).sort(desc(\"count\"))\n",
    "# df_nocruzan.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### EJECUTAR SOLO EN CASO DE NECESIDAD!!!!!!\n",
    "# df_pandas=df_nocruzan.limit(100).toPandas()\n",
    "# writer = pd.ExcelWriter(\"/var/SP/data/home/csanc109/data/churn/ccc/nocruzan_20181114.xlsx\", engine='xlsxwriter')\n",
    "# df_pandas.to_excel(writer, startrow = 0, sheet_name='Bucket NA')\n",
    "# writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+\n",
      "|              Bucket|  count|\n",
      "+--------------------+-------+\n",
      "|             Bucket_|     76|\n",
      "|Bucket_Billing - ...| 887119|\n",
      "|Bucket_Churn/Canc...|1409282|\n",
      "|  Bucket_Collections| 273738|\n",
      "|Bucket_DSL/FIBER ...| 781377|\n",
      "|Bucket_Device del...| 179853|\n",
      "|Bucket_Device upg...| 259134|\n",
      "|Bucket_New adds p...| 707436|\n",
      "|Bucket_Other cust...| 630229|\n",
      "|Bucket_Prepaid ba...|  10286|\n",
      "|Bucket_Product an...| 927302|\n",
      "|Bucket_Quick Closing|  76563|\n",
      "|Bucket_Tariff man...| 169176|\n",
      "|Bucket_Voice and ...| 251504|\n",
      "|                  NA|2210417|\n",
      "+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Para ver los buckets a NA NA|2269376\n",
    "df_all.select('Bucket').groupby('Bucket').count().sort('Bucket').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1847227"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as sql_sum\n",
    "# df_all distinct count 2719030 df_sinNA distinct count 1836042\n",
    "df_sinNA = ((df_all\n",
    "            .groupby('msisdn')\n",
    "            .agg(sql_sum(when(col('Bucket')==\"NA\", lit(1)).otherwise(lit(0))).alias(\"bucket_sum\")))\n",
    "            .where(col(\"bucket_sum\")==0))\n",
    "df_sinNA.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_set, concat, size, coalesce, col, lpad, struct, count as sql_count, lit, min as sql_min, max as sql_max, collect_list, udf, when\n",
    "from pyspark.sql.types import StringType, ArrayType, MapType, StructType, StructField, IntegerType\n",
    "\n",
    "from engine.general_functions import format_date, compute_diff_days\n",
    "\n",
    "\n",
    "df_agg = (df_all.where(col(\"Bucket\")!=\"NA\")\\\n",
    "            .withColumn(\"fx_interaction\",concat(col('year'),lpad(col('month'),2, '0'),lpad(col('day'),2, '0')))\n",
    "            .withColumn(\"tuples\", struct([\"fx_interaction\",\"Bucket\"]))\n",
    "            .groupby('msisdn')\n",
    "            .agg(sql_count(lit(1)).alias(\"num_interactions\"),\n",
    "                 sql_min(col('fx_interaction')).alias(\"first_interaction\"),\n",
    "                 sql_max(col('fx_interaction')).alias(\"latest_interaction\"),\n",
    "                 sql_max(col(\"tuples\")).alias(\"tuples_max\"), # latest interaction: [max(fx_interaction), bucket]\n",
    "                 sql_min(col(\"tuples\")).alias(\"tuples_min\"), # first interaction: [min(fx_interaction), bucket]\n",
    "                 collect_list('Bucket').alias(\"bucket_list\"),\n",
    "                 collect_set('Bucket').alias(\"bucket_set\")\n",
    "             ))\n",
    "\n",
    "df_agg = df_agg.withColumn(\"ref_date\", format_date(lit(ref_date)))\n",
    "df_agg = (df_agg.withColumn(\"bucket_1st_interaction\", col(\"tuples_min\")[\"Bucket\"])\n",
    "                .withColumn(\"bucket_latest_interaction\", col(\"tuples_max\")[\"Bucket\"])\n",
    "                .withColumn(\"nb_diff_buckets\", size(\"bucket_set\"))\n",
    "                .withColumn(\"num_interactions\", size(\"bucket_list\"))\n",
    "          .drop(*['tuples_max','tuples_min'])\n",
    "         )\n",
    "\n",
    "for cc in [\"first_interaction\", \"latest_interaction\"]:\n",
    "    df_agg = (df_agg.withColumn(\"fx_{}\".format(cc), format_date(cc, filter_dates_1900=True))  # days before 1900 converted to None\n",
    "                    .withColumn(\"days_since_{}\".format(cc), compute_diff_days(\"fx_{}\".format(cc), \"ref_date\")))\n",
    "    \n",
    "from collections import Counter\n",
    "from pyspark.sql.types import StringType, ArrayType, MapType, StructType, StructField\n",
    "\n",
    "def get_mode(lst):\n",
    "    dd = Counter(lst).most_common(2)\n",
    "    return dd[0][0] if (len(dd)==1 or dd[0][1] > dd[1][1]) else \"TIE\"\n",
    "def get_mode_freq(lst):\n",
    "    dd = Counter(lst).most_common(2)\n",
    "    return dd[0][1] # value\n",
    "\n",
    "get_mode_udf = udf(lambda lst: get_mode(lst), StringType())\n",
    "get_mode_freq_udf = udf(lambda lst: get_mode_freq(lst), IntegerType())\n",
    "\n",
    "df_agg = (df_agg.withColumn(\"most_common_bucket_with_ties\", when(coalesce(size(col(\"bucket_list\")),lit(0))==0,\"None\").otherwise(get_mode_udf(col(\"bucket_list\"))))\n",
    "                .withColumn(\"most_common_bucket\", when(col(\"most_common_bucket_with_ties\").rlike(\"^TIE\"), col('bucket_latest_interaction')).otherwise(col(\"most_common_bucket_with_ties\")))\n",
    "                .withColumn(\"most_common_bucket_interactions\", when(coalesce(size(col(\"bucket_list\")),lit(0))==0,-1).otherwise(get_mode_freq_udf(col(\"bucket_list\"))))\n",
    "          )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------+-------------------------+----------------------------+-------------------------+\n",
      "|bucket_list                                                                                                             |bucket_latest_interaction|most_common_bucket_with_ties|most_common_bucket       |\n",
      "+------------------------------------------------------------------------------------------------------------------------+-------------------------+----------------------------+-------------------------+\n",
      "|[Bucket_Product and Service management, Bucket_Billing - Postpaid, Bucket_Tariff management, Bucket_Churn/Cancellations]|Bucket_Billing - Postpaid|TIE                         |Bucket_Billing - Postpaid|\n",
      "+------------------------------------------------------------------------------------------------------------------------+-------------------------+----------------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_agg.where(col(\"msisdn\")==605256544).select(\"bucket_list\",\"bucket_latest_interaction\",\"most_common_bucket_with_ties\", \"most_common_bucket\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablita = spark.read.load('/user/csanc109/projects/churn/port/mobileandfbb/df_port_mobileandfbb_service_20181021_c8_d4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('20180731', 8, '20180930')\n"
     ]
    }
   ],
   "source": [
    "from churn.utils.constants import *\n",
    "import os\n",
    "import sys\n",
    "from pykhaos.utils.date_functions import move_date_n_cycles\n",
    "import datetime as dt       \n",
    "    \n",
    "closing_day = \"20180731\"\n",
    "cycles_horizon = 8\n",
    "end_date = move_date_n_cycles(closing_day, n=cycles_horizon)\n",
    "print(closing_day, cycles_horizon, end_date)\n",
    "\n",
    "if end_date > dt.date.today().strftime(\"%Y%m%d\"):\n",
    "    print(\"Program cannot be run with cycles={0} since closing_day + {0} cycles is in the future {1}\".format(cycles_horizon, end_date))\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config from file /var/SP/data/home/csanc109/src/devel/use-cases/churn/models/ccc/input/ccc_train_test.yaml\n",
      "Reading internal config from file /var/SP/data/home/csanc109/src/devel/use-cases/churn/datapreparation/config/internal_config.yaml\n",
      "{'campaign_creation': {'campaign_date': '201810',\n",
      "                       'create_csv_file': True,\n",
      "                       'storage_format': 'hdfs'},\n",
      " 'data_preparation': {'ccc_period': 60,\n",
      "                      'force_generation': False,\n",
      "                      'level': 'service',\n",
      "                      'model_target': 'port',\n",
      "                      'port_period': [20180901, 20180930],\n",
      "                      'save': 'true,',\n",
      "                      'segment_filter': 'mobileandfbb',\n",
      "                      'service_set': 'mobile'},\n",
      " 'internal_config_file': '/var/SP/data/home/csanc109/src/devel/use-cases/churn/datapreparation/config/internal_config.yaml',\n",
      " 'predict': {'dd_prepago': [201808],\n",
      "             'do_predict': 'true,',\n",
      "             'use_train_model_predict': True},\n",
      " 'sources': {'ids': {'address': False,\n",
      "                     'billing': True,\n",
      "                     'call_centre_calls': False,\n",
      "                     'campaigns': True,\n",
      "                     'customer': True,\n",
      "                     'customer_aggregations': True,\n",
      "                     'customer_penalties': True,\n",
      "                     'device_catalogue': True,\n",
      "                     'geneva': True,\n",
      "                     'netscout': False,\n",
      "                     'orders': True,\n",
      "                     'spinners': True,\n",
      "                     'tnps': True}},\n",
      " 'train': {'do_train': True, 'save_train_results': True, 'train_alg': 0},\n",
      " 'user_config_file': '/var/SP/data/home/csanc109/src/devel/use-cases/churn/models/ccc/input/ccc_train_test.yaml'}\n",
      "Get port requests table: start=20180901 end=20180930 ref_date=20181021\n",
      "Number of portouts in range 221704\n"
     ]
    }
   ],
   "source": [
    "from churn.datapreparation.config import Config\n",
    "config_obj = Config(filename='/var/SP/data/home/csanc109/src/devel/use-cases/churn/models/ccc/input/ccc_train_test.yaml',\n",
    "                   check_args=False)\n",
    "\n",
    "from churn.datapreparation.data_loader import get_port_requests_table\n",
    "select_cols = [\"msisdn_a\", \"portout_date\"]\n",
    "df_port = get_port_requests_table(spark, config_obj=config_obj, start_date=\"20180901\", end_date=\"20180930\",\n",
    "                                  ref_date=\"20181021\", select_cols=select_cols)\n",
    "\n",
    "print(\"Number of portouts in range {}\".format(df_port.count()))\n",
    "dist_portouts_dates = df_port.select(\"portout_date\").distinct().rdd.map(lambda x: x[\"portout_date\"]).collect()\n",
    "\n",
    "from amdocs_informational_dataset.engine.call_centre_calls import CallCentreCalls\n",
    "ccc = CallCentreCalls(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def union_all(dfs):\n",
    "    if len(dfs) > 1:\n",
    "        return dfs[0].unionAll(union_all(dfs[1:]))\n",
    "    else:\n",
    "        return dfs[0]\n",
    "\n",
    "from pykhaos.utils.date_functions import move_date_n_days\n",
    "n_ccc = -60 # FIXME -abs(config_obj.['data_preparation'][60])\n",
    "dfs = []\n",
    "from pyspark.sql.functions import col\n",
    "from churn.models.ccc.data.ccc_preparation import fill_missing_buckets\n",
    "counter = 0\n",
    "\n",
    "for port_date in dist_portouts_dates:\n",
    "    \n",
    "    print(port_date)\n",
    "    msisdn_list = df_port.where(col(\"portout_date\")==port_date).select(\"msisdn_a\").distinct().rdd.map(lambda x: x[\"msisdn_a\"]).collect()\n",
    "    yyyymmdd_portout_date = str(port_date.split(\" \")[0].replace(\"-\",\"\"))\n",
    "    \n",
    "    print(\"There are {} services with portout_date {}\".format(len(msisdn_list), yyyymmdd_portout_date))\n",
    "    \n",
    "    ccc_start_date = move_date_n_days(yyyymmdd_portout_date, n=n_ccc, str_fmt=\"%Y%m%d\")\n",
    "    ccc_end_date = yyyymmdd_portout_date\n",
    "    _ = ccc.get_ccc_service_df(ccc_end_date, ccc_start_date).where(col(\"msisdn\").isin(msisdn_list))\n",
    "    \n",
    "    df_all = fill_missing_buckets(spark, ccc.all_interactions)\n",
    "    ccc.all_interactions = df_all\n",
    "    df_agg = ccc.add_l2_ccc_variables(self, closing_day)\n",
    "    \n",
    "    dfs.append(df_agg)\n",
    "    counter = counter+1\n",
    "    \n",
    "    if counter==2: break\n",
    "\n",
    "df_agg_all = union_all(dfs)\n",
    "\n",
    "print(\"Number of rows {}\".format(df_agg_all.count()))\n",
    "\n",
    "df_all_model = df_all.select(\"msisdn\", \"most_common_bucket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add data to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from amdocs_informational_dataset.engine.call_centre_calls import CallCentreCalls\n",
    "from pyspark.sql.functions import collect_set, concat, size, coalesce, col, lpad, struct, count as sql_count, lit, min as sql_min, max as sql_max, collect_list, udf, when\n",
    "from pyspark.sql.types import StringType, ArrayType, MapType, StructType, StructField, IntegerType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Call Centre Calls Information...\n",
      "Calling to add_l2_ccc_variables....\n"
     ]
    }
   ],
   "source": [
    "from amdocs_informational_dataset.engine.call_centre_calls import CallCentreCalls\n",
    "from pyspark.sql.functions import collect_set, concat, size, coalesce, col, lpad, struct, count as sql_count, lit, min as sql_min, max as sql_max, collect_list, udf, when\n",
    "from pyspark.sql.types import StringType, ArrayType, MapType, StructType, StructField, IntegerType\n",
    "\n",
    "#ccc = CallCentreCalls(spark)\n",
    "\n",
    "ccc_end_date_ = \"20181031\"\n",
    "ccc_start_date_ = \"20181001\"\n",
    "\n",
    "closing_day = \"20181031\"\n",
    "df_agg = CallCentreCalls(spark).get_ccc_service_df(ccc_end_date_, ccc_start_date_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, regexp_extract\n",
    "\n",
    "df_agg = df_agg.withColumn(\"IND_PBMA_SRV\", when(col(\"num_averias\")+col(\"num_incidencias\")>0, 1).otherwise(0))\n",
    "\n",
    "cols_averia = [col_ for col_ in df_agg.columns if col_.startswith(\"ccc_Raw_Averia\")]\n",
    "cols_incidencia = [col_ for col_ in df_agg.columns if col_.startswith(\"ccc_Raw_Incidencia\")]\n",
    "\n",
    "\n",
    "COLS_PROB = cols_averia + cols_incidencia\n",
    "# Return the column name of the column with max value\n",
    "colname_with_max_udf = udf(lambda milist: COLS_PROB[\n",
    "        sorted([(vv, idx) for idx, vv in enumerate(milist)], key=lambda tup: tup[0], reverse=True)[0][1]],\n",
    "                               StringType())\n",
    "\n",
    "df_agg = df_agg.withColumn(\"DETALLE_PBMA_SRV\", when(col(\"IND_PBMA_SRV\")>0, colname_with_max_udf(array(COLS_PROB))).otherwise(\"None\"))\n",
    "df_agg = df_agg.withColumn(\"DETALLE_PBMA_SRV\", when(col(\"DETALLE_PBMA_SRV\").like(\"ccc_Raw_%\"), regexp_extract(col('DETALLE_PBMA_SRV'), \"^ccc_Raw_(.*)$\", 1)).otherwise(col('DETALLE_PBMA_SRV')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+-----------------+--------------------+-------------------+---------------------------+---------------------------------+----------------------------------+--------------------------------+--------------------------+----------------------+------------------------+----------------------------------+\n",
      "|    DETALLE_PBMA_SRV|ccc_Raw_Averia_DSL|ccc_Raw_Averia_Fibra|ccc_Raw_Averia_TV|ccc_Raw_Averia_Resto|ccc_Raw_Averia_Neba|ccc_Raw_Averia_Modem_Router|ccc_Raw_Incidencia_Provision_Neba|ccc_Raw_Incidencia_Provision_Fibra|ccc_Raw_Incidencia_Provision_DSL|ccc_Raw_Incidencia_Tecnica|ccc_Raw_Incidencia_SGI|ccc_Raw_Incidencia_Resto|ccc_Raw_Incidencia_Provision_Movil|\n",
      "+--------------------+------------------+--------------------+-----------------+--------------------+-------------------+---------------------------+---------------------------------+----------------------------------+--------------------------------+--------------------------+----------------------+------------------------+----------------------------------+\n",
      "|Incidencia_Provis...|                 0|                   0|                0|                   0|                  0|                          0|                                0|                                 0|                               0|                         0|                     0|                       0|                                 1|\n",
      "|          Averia_DSL|                 2|                   0|                0|                   0|                  0|                          0|                                0|                                 0|                               0|                         0|                     0|                       0|                                 0|\n",
      "|    Incidencia_Resto|                 0|                   0|                0|                   0|                  0|                          0|                                0|                                 0|                               0|                         0|                     0|                       1|                                 0|\n",
      "|         Averia_Neba|                 0|                   0|                0|                   0|                  1|                          0|                                0|                                 0|                               0|                         0|                     0|                       0|                                 0|\n",
      "|           Averia_TV|                 0|                   0|                1|                   0|                  0|                          0|                                0|                                 0|                               0|                         0|                     0|                       0|                                 0|\n",
      "|        Averia_Fibra|                 0|                   1|                0|                   0|                  0|                          0|                                0|                                 0|                               0|                         0|                     0|                       0|                                 0|\n",
      "|          Averia_DSL|                 2|                   0|                0|                   0|                  0|                          0|                                0|                                 0|                               0|                         0|                     0|                       0|                                 0|\n",
      "|          Averia_DSL|                 1|                   0|                0|                   0|                  0|                          0|                                0|                                 0|                               0|                         0|                     0|                       0|                                 0|\n",
      "|    Incidencia_Resto|                 0|                   0|                0|                   0|                  0|                          0|                                0|                                 0|                               0|                         0|                     0|                       2|                                 0|\n",
      "|          Averia_DSL|                 1|                   0|                0|                   0|                  0|                          0|                                0|                                 0|                               0|                         0|                     0|                       0|                                 0|\n",
      "+--------------------+------------------+--------------------+-----------------+--------------------+-------------------+---------------------------+---------------------------------+----------------------------------+--------------------------------+--------------------------+----------------------+------------------------+----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_agg.where(col(\"IND_PBMA_SRV\")>0).select([\"DETALLE_PBMA_SRV\"]+COLS_PROB).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fichero=\"/user/csanc109/projects/churn/preds_comb_20181031_all.txt\" # msisdn es msisdn_d\n",
    "df_scores = spark.read.option(\"delimiter\", \"|\").option(\"header\", True).csv(fichero)\n",
    "df_scores = df_scores.withColumnRenamed(\"msisdn\", \"msisdn_d_scores\")\n",
    "\n",
    "hdfs_write_path_common = '/data/udf/vf_es/amdocs_inf_dataset/'\n",
    "hdfs_partition_path = 'year=' + str(int(ccc_end_date_[:4])) + '/month=' + str(int(ccc_end_date_[4:6])) + '/day=' + str(int(ccc_end_date_[6:8]))\n",
    "path_service = hdfs_write_path_common +'service/'+hdfs_partition_path\n",
    "df_service = (spark.read.load(path_service)).select(\"msisdn\", \"campo2\").withColumnRenamed(\"msisdn\", \"msisdn_a_service\")\n",
    "\n",
    "df_scores_join = df_scores.join(df_service, on=df_scores[\"msisdn_d_scores\"]==df_service[\"campo2\"], how=\"left\")\n",
    "\n",
    "df_scores_incidencias = df_scores_join.join(df_agg, on=df_scores_join[\"msisdn_a_service\"]==df_agg[\"msisdn\"], how=\"left\")\n",
    "\n",
    "# pyspark cannot write arrays to csv's\n",
    "from pyspark.sql.functions import concat_ws\n",
    "df_scores_incidencias = df_scores_incidencias.withColumn('bucket_list', concat_ws(\",\", col('bucket_list'))).withColumn('bucket_set', concat_ws(\",\", col('bucket_set')))\n",
    "\n",
    "df_scores_incidencias = df_scores_incidencias.fillna( { 'IND_PBMA_SRV':0, 'DETALLE_PBMA_SRV':\"None\" } )\n",
    "for col_ in cols_averia+cols_incidencia+[\"num_incidencias\", \"num_averias\", \"num_interactions\", \n",
    "                                         \"days_since_first_interaction\", \"days_since_latest_interaction\", \n",
    "                                         \"num_ivr_interactions\"]:\n",
    "    df_scores_incidencias = df_scores_incidencias.fillna( { col_:0} )\n",
    "    \n",
    "cols_problemas = [] # Rename columns to make names more readable\n",
    "for col_ in cols_averia+cols_incidencia:\n",
    "    col_renamed=re.match(\"^ccc_Raw_(.*)$\", col_).group(1)\n",
    "    df_scores_incidencias = df_scores_incidencias.withColumnRenamed(col_, col_renamed)\n",
    "    cols_problemas.append(col_renamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_scores_incidencias = df_scores_incidencias.drop(\"msisdn\") # anonimized\n",
    "df_scores_incidencias = df_scores_incidencias.withColumnRenamed(\"msisdn_d_scores\", \"msisdn\") # deanonimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_scores_incidencias = df_scores_incidencias.withColumn(\"comb_score\", col(\"comb_score\").cast(FloatType()))\n",
    "df_scores_incidencias = df_scores_incidencias.orderBy('comb_score', ascending=False)\n",
    "\n",
    "df_scores_incidencias = df_scores_incidencias.repartition(1)\n",
    "# df_scores_incidencias.select(*['msisdn', 'comb_score', 'comb_decile', 'IND_PBMA_SRV', 'DETALLE_PBMA_SRV', 'num_averias', 'num_incidencias', 'num_interactions', 'days_since_first_interaction',\n",
    "#  'days_since_latest_interaction', 'num_ivr_interactions'] + cols_problemas).write.mode('overwrite').format('csv').option('sep', '|').option('header', 'true').save('/tmp/csanc109/churn/preds_comb_20181031_all_incidences.csv.v13')\n",
    "\n",
    "df_scores_incidencias.select(*['msisdn', 'comb_score', 'comb_decile', 'IND_PBMA_SRV', 'DETALLE_PBMA_SRV']).write.mode('overwrite').format('csv').option('sep', '|').option('header', 'true').save('/tmp/csanc109/churn/preds_comb_20181031_all_incidences.csv.v14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|comb_score|\n",
      "+----------+\n",
      "|  0.995526|\n",
      "|  0.995526|\n",
      "|  0.995526|\n",
      "|  0.995526|\n",
      "|  0.995525|\n",
      "|  0.995525|\n",
      "|  0.995525|\n",
      "|  0.995525|\n",
      "|  0.995524|\n",
      "|  0.995524|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_scores_incidencias.select(\"comb_score\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-852525e3b454>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mrdd_histogram_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_scores_incidencias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mrdd_histogram_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "# def create_hist(rdd_histogram_data):\n",
    "#     heights = np.array(rdd_histogram_data[1])\n",
    "#     full_bins = rdd_histogram_data[0]\n",
    "#     mid_point_bins = full_bins[:-1]\n",
    "#     widths = [abs(i - j) for i, j in zip(full_bins[:-1], full_bins[1:])]\n",
    "#     bar = plt.bar(mid_point_bins, heights, width=widths, color='b')\n",
    "#     return bar\n",
    "\n",
    "# rdd_histogram_data = df_scores_incidencias.select(\"score\").rdd.flatMap(lambda x: x).histogram(20)\n",
    "# rdd_histogram_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
