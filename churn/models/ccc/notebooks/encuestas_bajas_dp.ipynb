{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20181213-195030 [INFO ] Logging to file /var/SP/data/home/csanc109/logging/out_20181213_195030.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_initialize spark\n",
      "Ended spark session: 0.0331211090088 secs | default parallelism=4\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, sys\n",
    "import datetime as dt\n",
    "DEVEL_SRC = os.path.join(os.environ.get('BDA_USER_HOME', ''), \"src\", \"devel\")\n",
    "if DEVEL_SRC not in sys.path:\n",
    "    sys.path.append(DEVEL_SRC)\n",
    "\n",
    "USECASES_SRC = os.path.join(DEVEL_SRC, \"use-cases\") # TODO when - is removed, remove also this line and adapt imports\n",
    "if USECASES_SRC not in sys.path: \n",
    "    sys.path.append(USECASES_SRC)\n",
    "    \n",
    "AMDOCS_SRC = os.path.join(DEVEL_SRC, \"amdocs_informational_dataset\") # TODO when - is removed, remove also this line and adapt imports\n",
    "if AMDOCS_SRC not in sys.path: \n",
    "    sys.path.append(AMDOCS_SRC)\n",
    "    \n",
    "import pykhaos.utils.custom_logger as clogger\n",
    "logging_file = os.path.join(os.environ.get('BDA_USER_HOME', ''), \"logging\",\n",
    "                                    \"out_\" + dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".log\")\n",
    "logger = clogger.configure_logger(log_filename=logging_file, std_channel=sys.stderr, logger_name=\"\")\n",
    "logger.info(\"Logging to file {}\".format(logging_file))    \n",
    "    \n",
    "    \n",
    "from project.project_generic import Project\n",
    "\n",
    "\n",
    "import pykhaos.utils.notebooks as nb\n",
    "\n",
    "project_obj = Project(\"encuestas bajas\", \"encuestas bajas\")   \n",
    "\n",
    "RUNNING_FROM_NOTEBOOK = nb.isnotebook()\n",
    "import matplotlib.pyplot as plt\n",
    "if RUNNING_FROM_NOTEBOOK:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    %matplotlib inline\n",
    "    EXTERNAL_LIB = os.path.join(os.environ.get('BDA_USER_HOME', ''), \"lib\", \"external_libs\")\n",
    "    sys.path.append(EXTERNAL_LIB)\n",
    "    %load_ext jupyternotify\n",
    "    \n",
    "#logger = my_project.logger\n",
    "\n",
    "if not RUNNING_FROM_NOTEBOOK:\n",
    "    args = my_project.arg_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark=project_obj.spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Reading config from file /var/SP/data/home/csanc109/src/devel/use-cases/churn/datapreparation/input/dp_ccc_model.yaml\n",
      "Reading internal config from file /var/SP/data/home/csanc109/src/devel/use-cases/churn/config_manager/config/internal_config_ccc_model.yaml\n",
      "----- CHECKING INPUT PARAMETERS CCCmodel------\n",
      "{'agg_by': 'msisdn',\n",
      " 'ccc_days': -60,\n",
      " 'closing_day': 20180430,\n",
      " 'end_port': 20180531,\n",
      " 'internal_config_file': '/var/SP/data/home/csanc109/src/devel/use-cases/churn/config_manager/config/internal_config_ccc_model.yaml',\n",
      " 'labeled': True,\n",
      " 'level': 'nif',\n",
      " 'model_target': 'comercial',\n",
      " 'save_car': True,\n",
      " 'sources': {'ids': {'address': False,\n",
      "                     'billing': True,\n",
      "                     'call_centre_calls': True,\n",
      "                     'campaigns': False,\n",
      "                     'customer': True,\n",
      "                     'customer_aggregations': True,\n",
      "                     'customer_penalties': False,\n",
      "                     'device_catalogue': False,\n",
      "                     'geneva': False,\n",
      "                     'netscout': False,\n",
      "                     'orders': False,\n",
      "                     'spinners': False,\n",
      "                     'tnps': False}},\n",
      " 'start_port': 20180501,\n",
      " 'user_config_file': '/var/SP/data/home/csanc109/src/devel/use-cases/churn/datapreparation/input/dp_ccc_model.yaml'}\n",
      "----- CHECKING INPUT PARAMETERS CCCmodel------\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline \n",
    "from churn.datapreparation.config import Config\n",
    "\n",
    "\n",
    "\n",
    "default_filename = os.path.join(USECASES_SRC, \"churn\", \"datapreparation\", \"input\", \"dp_ccc_model.yaml\")\n",
    "\n",
    "\n",
    "from churn.config_manager.ccc_model_config_mgr import  CCCmodelConfig\n",
    "config_obj = CCCmodelConfig(default_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning all information churn_reason...\n"
     ]
    }
   ],
   "source": [
    "from churn.datapreparation.engine.bajas_data_loader import load_reasons_encuestas\n",
    "from pyspark.sql.functions import concat, lpad, col\n",
    "df_encuestas = load_reasons_encuestas(spark, config_obj=None).withColumnRenamed(\"NIF_reasons\", \"NIF\")\n",
    "df_encuestas = df_encuestas.withColumnRenamed(\"reason\", \"label\")\n",
    "df_encuestas = df_encuestas.withColumn(\"portout_encuesta\", concat(col('year'), lpad(col('month'), 2, '0'), lpad(col('day'), 2, '0')))\n",
    "df_encuestas = df_encuestas.select(\"NIF\", \"yearmonth\", \"REASON_ENCUESTA\", \"portout_encuesta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "CAT_PRECIO = \"PRECIO\"\n",
    "CAT_TERMINAL = \"TERMINAL\"\n",
    "CAT_CONTENIDOS = \"CONTENIDOS\"\n",
    "CAT_COMERCIAL = \"COMERCIAL\"\n",
    "CAT_NO_COMERCIAL = \"NO_COMERCIAL\"\n",
    "CAT_SERV_ATENCION = \"SERVICIO/ATENCION\"\n",
    "CAT_TECNICO = \"TECNICO\"\n",
    "CAT_BILLING = \"BILLING\"\n",
    "CAT_FRAUD = \"FRAUD\"\n",
    "NO_PROB = \"NO_PROB\"\n",
    "NA = \"NA\"\n",
    "\n",
    "# ADD CATEGORIES TO CALLS\n",
    "REASON1_PRECIO = [\"Cliente quiere pagar menos\", \"Clte quiere pagar menos\"]\n",
    "\n",
    "\n",
    "def __set_categories(df_all):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    df_all = df_all.withColumn(\"is_fraud\", when(col(\"INT_RAZON\").rlike(\"(?i)enga.o comercial|venta enga.osa\") ,1).otherwise(0))\n",
    "    df_all = df_all.withColumn(\"Queja_Trato\", when(col(\"INT_SUBTIPO\").isin([\"Queja trato\", \"Quejas sevicios de atencion\"])\n",
    "                                    ,1).otherwise(0))\n",
    "\n",
    "    df_all = df_all.withColumn(\"CATEGORY_2\",\n",
    "                               # - - - - - - - - - PRECIO\n",
    "                               when(col('INT_TIPO').isin(REASON1_PRECIO) , CAT_PRECIO)\n",
    "                               # - - - - - - - - - SERVICIO/ATENCION\n",
    "                               .when(lower(col(\"INT_SUBTIPO\")).isin\n",
    "                                   ([\"Queja trato\".lower(), \"Quejas sevicios de atencion\".lower()]) ,CAT_SERV_ATENCION)\n",
    "                               # - - - - - - - - - ENGAÃ‘O\n",
    "                               .when(col(\"INT_RAZON\").rlike(\"(?i)enga.o comercial|venta enga.osa\"), CAT_FRAUD)\n",
    "                               # - - - - - - - - - TECNICO\n",
    "                               .when(col('INT_TIPO').rlike('(?i)^Averia') ,CAT_TECNICO)\n",
    "                               .when((col('INT_TIPO').rlike('(?i)^Transferencia') )&\n",
    "                                   (col(\"INT_SUBTIPO\").rlike('(?i)^Aver.as')), CAT_TECNICO)\n",
    "                               .when(col('INT_TIPO').rlike(\n",
    "                                   '(?i)^Inc Provis.*Neba|^Inc Provision Fibra|^Inc Provision DSL|^Incidencia%ecnica|^Incidencia%SGI|^Inc|^Incidencia'),\n",
    "                                     CAT_TECNICO)\n",
    "                               .when(col('INT_TIPO').rlike('(?i)^Consulta tec'), CAT_TECNICO)\n",
    "                               .when(col(\"Raw_Resultado\").rlike(\n",
    "                                   \"Raw_Resultado_Escalo|Raw_Resultado_Envio_tecnico|Raw_Resultado_Transferencia|Raw_Resultado_Reclamacion\"),\n",
    "                                     CAT_TECNICO)\n",
    "                               # - - - - - - - - - FACTURA\n",
    "                               .when(col('INT_TIPO').rlike('(?i)factura'), CAT_BILLING)\n",
    "                               # - - - - - - - - - TERMINAL\n",
    "                               .when(col(\"INT_TIPO\") == \"TERMINAL\", CAT_TERMINAL)\n",
    "                               # - - - - - - - - - NO_PROB :)\n",
    "                               .when((col(\"INT_TIPO\") == \"INFORMACION\") | ((col(\"INT_TIPO\") == \"TRANSFERENCIA\") & (\n",
    "                                           col(\"Bucket\") == \"Other customer information management\")),\n",
    "                                     NO_PROB).otherwise(NA)\n",
    "                               )  # end\n",
    "\n",
    "    df_all = df_all.na.fill(NA, subset=[\"CATEGORY_2\"])\n",
    "\n",
    "    df_all = (df_all.withColumn(\"CATEGORY_1\",\n",
    "                                when(col('CATEGORY_2').isin([CAT_PRECIO, CAT_TERMINAL, CAT_CONTENIDOS]), CAT_COMERCIAL)\n",
    "                                .when(col('CATEGORY_2').isin([CAT_SERV_ATENCION, CAT_TECNICO, CAT_BILLING, CAT_FRAUD]),\n",
    "                                      CAT_NO_COMERCIAL)\n",
    "                                .otherwise(col('CATEGORY_2'))))\n",
    "\n",
    "    print(\"Elapsed time _set_categories = {} minutes\".format((time.time()-start_time)/60.0))\n",
    "    # FALTA\n",
    "    # col('Raw_Productos') hay que desglosarlo\n",
    "    return df_all\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def __aggregate_ccc_calls_by(df_all, process_date, agg_by=\"nif\"):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "    print(\"Calling to __aggregate_ccc_calls_by_{}.... {}\".format(agg_by, process_date))\n",
    "\n",
    "    df_agg = (df_all\n",
    "              .withColumn(\"fx_interaction\",\n",
    "                          concat(col('year'), lpad(col('month'), 2, '0'), lpad(col('day'), 2, '0')))\n",
    "              .withColumn(\"tuples\", struct([\"fx_interaction\", \"Bucket\"]))\n",
    "              .groupby(agg_by)\n",
    "              .agg(sql_count(lit(1)).alias(\"num_interactions\"),\n",
    "                   sql_min(col('fx_interaction')).alias(\"first_interaction\"),\n",
    "                   sql_max(col('fx_interaction')).alias(\"latest_interaction\"),\n",
    "                   sql_max(col(\"tuples\")).alias(\"tuples_max\"),  # latest interaction: [max(fx_interaction), bucket]\n",
    "                   sql_min(col(\"tuples\")).alias(\"tuples_min\"),  # first interaction: [min(fx_interaction), bucket]\n",
    "                   collect_list('Bucket').alias(\"bucket_list\"),\n",
    "                   collect_set('Bucket').alias(\"bucket_set\"),\n",
    "                   sql_sum(\"Bucket_NA\").alias(\"num_NA_buckets\"),\n",
    "                   sql_sum(\"IVR\").alias(\"num_ivr_interactions\"),\n",
    "                   collect_list('CATEGORY_1').alias(\"cat1_list\"),\n",
    "                   collect_list('CATEGORY_2').alias(\"cat2_list\"),\n",
    "            ))\n",
    "\n",
    "    df_agg = df_agg.withColumn(\"ref_date\", format_date(lit(process_date)))\n",
    "    df_agg = (df_agg.withColumn(\"bucket_1st_interaction\", col(\"tuples_min\")[\"Bucket\"])\n",
    "                    .withColumn(\"bucket_latest_interaction\", col(\"tuples_max\")[\"Bucket\"])\n",
    "                    .withColumn(\"nb_diff_buckets\", size(\"bucket_set\"))\n",
    "                   .drop(*['tuples_max', 'tuples_min'])\n",
    "              )\n",
    "\n",
    "    for cc in [\"first_interaction\", \"latest_interaction\"]:\n",
    "        df_agg = (df_agg.withColumn(\"fx_{}\".format(cc), format_date(cc, filter_dates_1900=True))  # days before 1900 converted to None\n",
    "                        .withColumn(\"days_since_{}\".format(cc), compute_diff_days(\"fx_{}\".format(cc), \"ref_date\")))\n",
    "\n",
    "    df_agg = df_agg.drop(\"ref_date\")\n",
    "\n",
    "    def get_mode_problems(lst):\n",
    "        # filter NA y NO_PROB\n",
    "        if not lst: return None\n",
    "        lst=[ll for ll in lst if ll and ll!=NO_PROB and ll!=\"NA\"]\n",
    "        if not lst: return None\n",
    "        dd = Counter(lst).most_common(2)\n",
    "        return dd[0][0]\n",
    "    get_mode_udf = udf(lambda lst: get_mode_problems(lst), StringType())\n",
    "\n",
    "    df_agg = (df_agg.withColumn(\"CAT1_MODE\", when(coalesce(size(col(\"cat1_list\")), lit(0)) == 0, \"None\").otherwise(get_mode_udf(col(\"cat1_list\"))))\n",
    "                    .withColumn(\"CAT2_MODE\", when(coalesce(size(col(\"cat2_list\")), lit(0)) == 0, \"None\").otherwise(get_mode_udf(col(\"cat2_list\")))))\n",
    "\n",
    "\n",
    "\n",
    "    df_cat_1 = df_all.groupby(agg_by).pivot(\"CATEGORY_1\", values=[CAT_COMERCIAL, CAT_NO_COMERCIAL]).agg(sql_count(\"*\")).fillna(0)\n",
    "    df_cat_2 = df_all.groupby(agg_by).pivot(\"CATEGORY_2\", values=[CAT_PRECIO, CAT_TERMINAL, CAT_CONTENIDOS, CAT_SERV_ATENCION,\n",
    "CAT_TECNICO, CAT_BILLING, CAT_FRAUD, NO_PROB]).agg(sql_count(\"*\")).fillna(0)\n",
    "\n",
    "    df_agg = df_agg.join(df_cat_1, [agg_by], how=\"left\").join(df_cat_2,  [agg_by], how=\"left\")\n",
    "\n",
    "    df_agg = (df_agg.withColumn(\"TOTAL_COMERCIAL\", sum_horizontal([CAT_PRECIO, CAT_TERMINAL, CAT_CONTENIDOS]))\n",
    "                    .withColumn(\"TOTAL_NO_COMERCIAL\", sum_horizontal([CAT_SERV_ATENCION, CAT_TECNICO, CAT_BILLING, CAT_FRAUD])))\n",
    "\n",
    "\n",
    "    print(\"Elapsed time __aggregate_ccc_calls_by_{} = {} minutes\".format(agg_by, (time.time()-start_time)/60.0))\n",
    "\n",
    "    return df_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from churn.datapreparation.general.data_loader import get_port_requests_table\n",
    "\n",
    "\n",
    "from pykhaos.utils.date_functions import move_date_n_days\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from amdocs_informational_dataset.engine.call_centre_calls import CallCentreCalls\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import col, concat, concat_ws, date_format, dayofmonth, format_string, from_unixtime, length, \\\n",
    "\tlit, lower, lpad, month, regexp_replace, translate, udf, unix_timestamp, year, when, upper, collect_set, collect_list, \\\n",
    "    count as sql_count, min as sql_min, max as sql_max, struct, size, coalesce, sum as sql_sum\n",
    "from pyspark.sql.types import DateType, IntegerType, StringType, StructField, StructType\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pykhaos.utils.pyspark_utils import format_date, compute_diff_days\n",
    "from collections import Counter\n",
    "import time\n",
    "from engine.general_functions import sum_horizontal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('amdocs_table_reader', 'customer', '20180331')\n",
      "Loading /data/udf/vf_es/amdocs_inf_dataset/customer/year=2018/month=3/day=31\n",
      "('amdocs_table_reader', 'service', '20180331')\n",
      "Loading /data/udf/vf_es/amdocs_inf_dataset/service/year=2018/month=3/day=31\n",
      "('amdocs_table_reader', 'customer', '20180430')\n",
      "Loading /data/udf/vf_es/amdocs_inf_dataset/customer/year=2018/month=4/day=30\n",
      "('amdocs_table_reader', 'service', '20180430')\n",
      "Loading /data/udf/vf_es/amdocs_inf_dataset/service/year=2018/month=4/day=30\n"
     ]
    }
   ],
   "source": [
    "# def amdocs_table_reader(spark, table_name, closing_day, new=True):\n",
    "#     print(\"amdocs_table_reader\", table_name, closing_day)\n",
    "#     path_car = \"amdocs_inf_dataset\" if new else \"amdocs_ids\"\n",
    "#     table_name = '/data/udf/vf_es/{}/{}/year={}/month={}/day={}'.format(path_car, table_name, int(closing_day[:4]),\n",
    "#                                                                                                     int(closing_day[4:6]),\n",
    "#                                                                                                     int(closing_day[6:]))\n",
    "#     print(\"Loading {}\".format(table_name))\n",
    "#     df_src = spark.read.load(table_name)\n",
    "#     return df_src\n",
    "\n",
    "# from pykhaos.utils.pyspark_utils import union_all\n",
    "\n",
    "# df_s = []\n",
    "\n",
    "# for closing_day in [\"20180331\", \"20180430\"]:\n",
    "\n",
    "\n",
    "#     df_customer = (amdocs_table_reader(spark, \"customer\", closing_day)\n",
    "#                  #.where(col(\"clase_cli_cod_clase_cliente\") == \"RS\")  # customer\n",
    "#                  #.where(col(\"cod_estado_general\").isin([\"01\", \"09\"]))  # customer\n",
    "#                  .select(\"num_cliente\", \"cod_estado_general\", \"clase_cli_cod_clase_cliente\", \"nif_cliente\")\n",
    "#                  .withColumnRenamed(\"num_cliente\", \"num_cliente_customer\"))\n",
    "\n",
    "#     df_service = (amdocs_table_reader(spark, \"service\", closing_day)\n",
    "#                   #.where(~col(\"srv_basic\").isin([\"MRSUI\", \"MPSUI\"]))  # service\n",
    "#                   #.where(col(\"rgu\").isNotNull())\n",
    "#                   .select(\"msisdn\", \"num_cliente\", \"campo2\", \"rgu\", \"srv_basic\")\n",
    "#                   .withColumnRenamed(\"num_cliente\", \"num_cliente_service\"))\n",
    "\n",
    "#     df_services = df_customer.join(df_service,\n",
    "#                   on=(df_customer[\"num_cliente_customer\"] == df_service[\"num_cliente_service\"]), how=\"inner\") # intersection\n",
    "\n",
    "#     df_services = df_services.withColumnRenamed(\"nif_cliente\", \"nif\")\n",
    "\n",
    "#     df_services = df_services.select(\"msisdn\", \"nif\")\n",
    "\n",
    "#     df_s.append(df_services)\n",
    "    \n",
    "# df_services = union_all(df_s)\n",
    "\n",
    "# df_services = df_services.dropDuplicates([\"msisdn\"])\n",
    "\n",
    "# df_services_encuestas = df_services.join(df_encuestas, [\"NIF\"], how=\"right\")\n",
    "\n",
    "# #df_services_encuestas = df_services_encuestas.where(col(\"msisdn\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6036"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_services_encuestas.dropDuplicates([\"NIf\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_portout = df_encuestas.select(sql_min(\"portout_encuesta\").alias(\"min_portout_date\")).rdd.map(\n",
    "    lambda x: x[\"min_portout_date\"]).collect()[0]\n",
    "max_portout = df_encuestas.select(sql_max(\"portout_encuesta\").alias(\"max_portout_date\")).rdd.map(\n",
    "    lambda x: x[\"max_portout_date\"]).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'20180901'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_portout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_ccc=config_obj.get_ccc_range_duration()\n",
    "\n",
    "min_portout = df_encuestas.select(sql_min(\"portout_encuesta\").alias(\"min_portout_date\")).rdd.map(\n",
    "    lambda x: x[\"min_portout_date\"]).collect()[0]\n",
    "max_portout = df_encuestas.select(sql_max(\"portout_encuesta\").alias(\"max_portout_date\")).rdd.map(\n",
    "    lambda x: x[\"max_portout_date\"]).collect()[0]\n",
    "\n",
    "yyyymmdd_min_portout_date = str(min_portout.split(\" \")[0].replace(\"-\", \"\"))\n",
    "yyyymmdd_max_portout_date = str(max_portout.split(\" \")[0].replace(\"-\", \"\"))\n",
    "\n",
    "ccc_start_date = move_date_n_days(yyyymmdd_min_portout_date, n=n_ccc-30, str_fmt=\"%Y%m%d\")\n",
    "ccc_end_date  = move_date_n_days(yyyymmdd_max_portout_date, n=30, str_fmt=\"%Y%m%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('20180101', '20181001')\n",
      "df_all_encuestas unique NIFS=6036\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"f6fa0fec-9bb8-4dd0-825a-f4dc7abae96e\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"f6fa0fec-9bb8-4dd0-825a-f4dc7abae96e\") === null) {\n",
       "                var notificationPayload = {\"body\": \"encuestas: ended dp \", \"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify -m \"encuestas: ended dp \"\n",
    "\n",
    "\n",
    "print(ccc_start_date, ccc_end_date)\n",
    "ccc = CallCentreCalls(spark)\n",
    "ccc.prepareFeatures(ccc_end_date, ccc_start_date)\n",
    "df_all = ccc.all_interactions\n",
    "df_all = df_all.withColumn(\"fx_interaction\", concat(col('year'), lpad(col('month'), 2, '0'), lpad(col('day'), 2, '0')))\n",
    "#df_all = df_all.drop(\"year\", \"month\", \"day\")\n",
    "\n",
    "# CALLS OF POLLS USERS\n",
    "df_all_encuestas = df_all.join(df_encuestas, [\"NIF\"], how=\"right\") \n",
    "\n",
    "print(\"df_all_encuestas unique NIFS={}\".format(df_all_encuestas.dropDuplicates([\"NIF\"]).count()))\n",
    "# ('20180101', '20181001')\n",
    "# df_all_encuestas unique NIFS=6036\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NIF', 'string'),\n",
       " ('msisdn', 'string'),\n",
       " ('partitioned_month', 'string'),\n",
       " ('year', 'int'),\n",
       " ('month', 'int'),\n",
       " ('day', 'int'),\n",
       " ('INT_Tipo', 'string'),\n",
       " ('INT_Subtipo', 'string'),\n",
       " ('INT_Razon', 'string'),\n",
       " ('INT_Resultado', 'string'),\n",
       " ('DIRECTION', 'string'),\n",
       " ('x_workgroup', 'string'),\n",
       " ('source', 'string'),\n",
       " ('TYPE_TD', 'string'),\n",
       " ('Sub_Bucket', 'string'),\n",
       " ('Raw_Pagar_menos', 'string'),\n",
       " ('Raw_Cobro', 'string'),\n",
       " ('Raw_Precios', 'string'),\n",
       " ('Raw_Averia', 'string'),\n",
       " ('Raw_Alta', 'string'),\n",
       " ('Raw_Desactivacion', 'string'),\n",
       " ('Raw_Factura', 'string'),\n",
       " ('Raw_Ofrecimiento', 'string'),\n",
       " ('Bucket_Sub_Bucket', 'string'),\n",
       " ('Raw_Incidencia', 'string'),\n",
       " ('Raw_Consulta', 'string'),\n",
       " ('Raw_Informacion', 'string'),\n",
       " ('Raw_Portabilidad', 'string'),\n",
       " ('Raw_Cierre', 'string'),\n",
       " ('Raw_Productos', 'string'),\n",
       " ('Raw_Resultado', 'string'),\n",
       " ('Raw_Provision', 'string'),\n",
       " ('Bucket', 'string'),\n",
       " ('Raw_Transferencia', 'string'),\n",
       " ('Raw_Baja', 'string'),\n",
       " ('IVR', 'int'),\n",
       " ('Bucket_NA', 'int'),\n",
       " ('fx_interaction', 'string'),\n",
       " ('yearmonth', 'string'),\n",
       " ('REASON_ENCUESTA', 'string'),\n",
       " ('portout_encuesta', 'string')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_encuestas.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------+----------------+\n",
      "|days_since_fx_interaction|fx_fx_interaction  |portout_encuesta|\n",
      "+-------------------------+-------------------+----------------+\n",
      "|3.0                      |2018-06-28 00:00:00|20180701        |\n",
      "|237.0                    |2018-01-07 00:00:00|20180901        |\n",
      "|87.0                     |2018-03-06 00:00:00|20180601        |\n",
      "|154.0                    |2018-02-28 00:00:00|20180801        |\n",
      "|154.0                    |2018-02-28 00:00:00|20180801        |\n",
      "|10.0                     |2018-03-22 00:00:00|20180401        |\n",
      "|10.0                     |2018-03-22 00:00:00|20180401        |\n",
      "|180.0                    |2018-01-02 00:00:00|20180701        |\n",
      "|137.0                    |2018-01-15 00:00:00|20180601        |\n",
      "|137.0                    |2018-01-15 00:00:00|20180601        |\n",
      "+-------------------------+-------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_all_encuestas = (df_all_encuestas.withColumn(\"fx_fx_interaction\", format_date(\"fx_interaction\", filter_dates_1900=True))  # days before 1900 converted to None\n",
    "                    .withColumn(\"fx_portout_encuestas\", format_date(\"portout_encuesta\", filter_dates_1900=True))\n",
    "               .withColumn(\"days_since_fx_interaction\", compute_diff_days(\"fx_fx_interaction\", \"fx_portout_encuestas\")))\n",
    "\n",
    "df_all_encuestas.select(\"days_since_fx_interaction\", \"fx_fx_interaction\", \"portout_encuesta\").show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time _set_categories = 0.00739806890488 minutes\n",
      "Calling to __aggregate_ccc_calls_by_NIF.... 20181001\n",
      "Elapsed time __aggregate_ccc_calls_by_NIF = 0.040777250131 minutes\n",
      "df_agg_all_ccc=1502\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_all_encuestas = (df_all_encuestas.withColumn(\"fx_fx_interaction\", format_date(\"fx_interaction\", filter_dates_1900=True))  # days before 1900 converted to None\n",
    "                    .withColumn(\"fx_portout_encuestas\", format_date(\"portout_encuesta\", filter_dates_1900=True))\n",
    "               .withColumn(\"days_since_fx_interaction\", compute_diff_days(\"fx_fx_interaction\", \"fx_portout_encuestas\")))\n",
    "\n",
    "df_all_encuestas = df_all_encuestas.where( (col(\"days_since_fx_interaction\")>=0) & (col(\"days_since_fx_interaction\")<=abs(n_ccc)))\n",
    "\n",
    "df_all_encuestas = __set_categories(df_all_encuestas)\n",
    "\n",
    "df_agg_all_ccc = __aggregate_ccc_calls_by(df_all_encuestas, ccc_end_date, agg_by=\"NIF\")\n",
    "\n",
    "print(\"df_agg_all_ccc={}\".format(df_agg_all_ccc.count()))\n",
    "\n",
    "df_agg_all_ccc = df_agg_all_ccc.drop('bucket_list', 'bucket_set', 'cat1_list', 'cat2_list')\n",
    "\n",
    "df_agg_all_ccc = df_encuestas.join(df_agg_all_ccc, [\"NIF\"], how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6036"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_agg_all_ccc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1502"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_agg_all_ccc.where(col(\"num_interactions\")>0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WRITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_20180501_20180531_c20180430_n60_comercial_msisdn\n",
      "True\n",
      "comercial\n",
      "Built filename: /data/udf/vf_es/churn/encuestas/comercial/df_20180501_20180531_c20180430_n60_comercial_msisdn\n",
      "Directory '/data/udf/vf_es/churn/encuestas/comercial' does not exist. Creating...\n",
      "create_directory return_state=''\n",
      "Created directory returned True\n",
      "Started to save...\n",
      "Saved df successfully - '/data/udf/vf_es/churn/encuestas/comercial/df_20180501_20180531_c20180430_n60_comercial_msisdn'\n",
      "Saved df in '/data/udf/vf_es/churn/encuestas/comercial/df_20180501_20180531_c20180430_n60_comercial_msisdn' format csv\n",
      "Directory '/data/udf/vf_es/churn/encuestas/comercial/df_20180501_20180531_c20180430_n60_comercial_msisdn/yaml' does not exist. Creating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20181213-004731 [INFO ] Moving '/var/SP/data/home/csanc109/src/devel/use-cases/churn/datapreparation/input/dp_ccc_model.yaml' to hdfs '/data/udf/vf_es/churn/encuestas/comercial/df_20180501_20180531_c20180430_n60_comercial_msisdn/yaml'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_directory return_state=''\n",
      "Running system command: hdfs dfs -put /var/SP/data/home/csanc109/src/devel/use-cases/churn/datapreparation/input/dp_ccc_model.yaml /data/udf/vf_es/churn/encuestas/comercial/df_20180501_20180531_c20180430_n60_comercial_msisdn/yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20181213-004736 [INFO ] Moving '/var/SP/data/home/csanc109/src/devel/use-cases/churn/config_manager/config/internal_config_ccc_model.yaml' to hdfs '/data/udf/vf_es/churn/encuestas/comercial/df_20180501_20180531_c20180430_n60_comercial_msisdn/yaml'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running system command: hdfs dfs -put /var/SP/data/home/csanc109/src/devel/use-cases/churn/config_manager/config/internal_config_ccc_model.yaml /data/udf/vf_es/churn/encuestas/comercial/df_20180501_20180531_c20180430_n60_comercial_msisdn/yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/data/udf/vf_es/churn/encuestas/comercial/df_20180501_20180531_c20180430_n60_comercial_msisdn'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from churn.datapreparation.engine.bajas_data_loader import save_results\n",
    "df_agg_all_ccc = df_agg_all_ccc.repartition(1)\n",
    "save_results(df_agg_all_ccc,config_obj, csv=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
