{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20181207-141032 [INFO ] Logging to file /var/SP/data/home/csanc109/logging/out_20181207_141032.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_initialize spark\n",
      "Ended spark session: 908.770467043 secs | default parallelism=2\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import datetime as dt\n",
    "DEVEL_SRC = os.path.join(os.environ.get('BDA_USER_HOME', ''), \"src\", \"devel\")\n",
    "if DEVEL_SRC not in sys.path:\n",
    "    sys.path.append(DEVEL_SRC)\n",
    "\n",
    "USECASES_SRC = os.path.join(DEVEL_SRC, \"use-cases\") # TODO when - is removed, remove also this line and adapt imports\n",
    "if USECASES_SRC not in sys.path: \n",
    "    sys.path.append(USECASES_SRC)\n",
    "    \n",
    "AMDOCS_SRC = os.path.join(DEVEL_SRC, \"amdocs_informational_dataset\") # TODO when - is removed, remove also this line and adapt imports\n",
    "if AMDOCS_SRC not in sys.path: \n",
    "    sys.path.append(AMDOCS_SRC)\n",
    "    \n",
    "import pykhaos.utils.custom_logger as clogger\n",
    "logging_file = os.path.join(os.environ.get('BDA_USER_HOME', ''), \"logging\",\n",
    "                                    \"out_\" + dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \".log\")\n",
    "logger = clogger.configure_logger(log_filename=logging_file, std_channel=sys.stderr, logger_name=\"\")\n",
    "logger.info(\"Logging to file {}\".format(logging_file))    \n",
    "    \n",
    "    \n",
    "from project.project_generic import Project\n",
    "\n",
    "\n",
    "import pykhaos.utils.notebooks as nb\n",
    "\n",
    "project_obj = Project(\"CCC_model_nb\", \"CCC_model_nb\")   \n",
    "\n",
    "RUNNING_FROM_NOTEBOOK = nb.isnotebook()\n",
    "import matplotlib.pyplot as plt\n",
    "if RUNNING_FROM_NOTEBOOK:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    %matplotlib inline  \n",
    "    \n",
    "#logger = my_project.logger\n",
    "\n",
    "if not RUNNING_FROM_NOTEBOOK:\n",
    "    args = my_project.arg_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" --master yarn --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.kryoserializer.buffer.max=1g --py-files /var/SP/data/home/csanc109/artifacts/bda-core-ra-complete-assembly-2.0.0.jar,/var/SP/data/home/csanc109/artifacts/common.zip,/var/SP/data/home/csanc109/artifacts/graphframes.zip,/var/SP/data/home/csanc109/artifacts/scripts.zip,/var/SP/data/home/csanc109/artifacts/xgboost4j-spark-2.1.1-0.7-jar-with-dependencies.jar --jars /var/SP/data/home/csanc109/artifacts/bda-core-ra-complete-assembly-2.0.0.jar,/var/SP/data/home/csanc109/artifacts/xgboost4j-spark-2.1.1-0.7-jar-with-dependencies.jar --files /var/SP/data/home/csanc109/scripts/properties/red_agent/nodes-cz.yaml,/var/SP/data/home/csanc109/scripts/properties/red_agent/nodes-de.yaml,/var/SP/data/home/csanc109/scripts/properties/red_agent/nodes-es.yaml,/var/SP/data/home/csanc109/scripts/properties/red_agent/nodes-gr.yaml,/var/SP/data/home/csanc109/scripts/properties/red_agent/nodes-ie.yaml,/var/SP/data/home/csanc109/scripts/properties/red_agent/nodes-it.yaml,/var/SP/data/home/csanc109/scripts/properties/red_agent/nodes-pt.yaml,/var/SP/data/home/csanc109/scripts/properties/red_agent/nodes-uk.yaml,/var/SP/data/home/csanc109/scripts/properties/red_agent/nodes-ut.yaml,/var/SP/data/home/csanc109/scripts/properties/red_agent/nodes-vge.yaml,/var/SP/data/home/csanc109/scripts/properties/red_agent/nodes.yaml --conf spark.network.timeout=30000 --conf spark.rpc.message.maxSize=1000 --conf spark.driver.maxResultSize=8g --conf spark.local.dir=/var/SP/data/home/csanc109/local-spark-dir/ --conf spark.driver.port=58035 --conf spark.blockManager.port=58036 --conf spark.broadcast.port=58037 --conf spark.replClassServer.port=58038 --conf spark.ui.port=58039 --conf spark.executor.port=58040 --conf spark.fileserver.port=58041 --executor-memory 32g --driver-memory 32g --conf spark.shuffle.manager=tungsten-sort  --queue root.BDPtenants.es.medium --conf spark.dynamicAllocation.enabled=true --conf spark.shuffle.service.enabled=true --conf spark.dynamicAllocation.maxExecutors=15 --conf spark.dynamicAllocation.minExecutors=1 --conf spark.executor.cores=4 --conf spark.dynamicAllocation.executorIdleTimeout=120 --conf spark.port.maxRetries=100 --conf spark.app.name='CCC_model_nb' --conf spark.submit.deployMode=client --conf spark.ui.showConsoleProgress=true --conf spark.sql.broadcastTimeout=1200 --conf spark.yarn.executor.memoryOverhead=3g --conf spark.yarn.executor.driverOverhead=1g --files /var/SP/data/home/csanc109/scripts/properties/red_agent/nodes.properties,/var/SP/data/home/csanc109/scripts/properties/red_agent/nodes-de.properties,/var/SP/data/home/csanc109/scripts/properties/red_agent/nodes-es.properties,/var/SP/data/home/csanc109/scripts/properties/red_agent/nodes-ie.properties,/var/SP/data/home/csanc109/scripts/properties/red_agent/nodes-it.properties,/var/SP/data/home/csanc109/scripts/properties/red_agent/nodes-pt.properties,/var/SP/data/home/csanc109/scripts/properties/red_agent/nodes-uk.properties\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_COMMON_OPTS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark=project_obj.spark\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from amdocs_informational_dataset.engine.call_centre_calls import CallCentreCalls\n",
    "from pyspark.sql.functions import collect_set, concat, size, coalesce, col, lpad, struct, count as sql_count, lit, min as sql_min, max as sql_max, collect_list, udf, when\n",
    "from pyspark.sql.types import StringType, ArrayType, MapType, StructType, StructField, IntegerType\n",
    "from pyspark.sql.functions import array, regexp_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from churn.datapreparation.data_loader import get_port_requests_table\n",
    "from churn.datapreparation.config import Config\n",
    "\n",
    "DEBUG=True\n",
    "\n",
    "port_start_date = \"20181001\"\n",
    "port_end_date = \"20181031\"\n",
    "ref_date = port_end_date\n",
    "\n",
    "PORTOUT_TABLE_COLS = [\"msisdn_a\", \"portout_date\"] # [\"portout_date\",  \"msisdn_a\", \"label\", \"days_from_portout\"]\n",
    "\n",
    "ccc_start_model = \"20180801\"\n",
    "ccc_end_model = \"20180930\"\n",
    "\n",
    "n_ccc = -60 # FIXME -abs(config_obj.['data_preparation'][60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading config from file /var/SP/data/home/csanc109/src/devel/use-cases/churn/input/dp_ccc_model.yaml\n",
      "Reading internal config from file /var/SP/data/home/csanc109/src/devel/use-cases/churn/config_manager/config/internal_config_ccc_model.yaml\n",
      "----- CHECKING INPUT PARAMETERS CCCmodel------\n",
      "{'ccc_days': -60,\n",
      " 'closing_day': 20180531,\n",
      " 'end_port': 20180531,\n",
      " 'internal_config_file': '/var/SP/data/home/csanc109/src/devel/use-cases/churn/config_manager/config/internal_config_ccc_model.yaml',\n",
      " 'labeled': True,\n",
      " 'level': 'nif',\n",
      " 'model_target': 'comercial',\n",
      " 'save_car': True,\n",
      " 'sources': {'ids': {'address': False,\n",
      "                     'billing': True,\n",
      "                     'call_centre_calls': True,\n",
      "                     'campaigns': False,\n",
      "                     'customer': True,\n",
      "                     'customer_aggregations': True,\n",
      "                     'customer_penalties': False,\n",
      "                     'device_catalogue': False,\n",
      "                     'geneva': False,\n",
      "                     'netscout': False,\n",
      "                     'orders': False,\n",
      "                     'spinners': False,\n",
      "                     'tnps': False}},\n",
      " 'start_port': 20180501,\n",
      " 'user_config_file': '/var/SP/data/home/csanc109/src/devel/use-cases/churn/input/dp_ccc_model.yaml'}\n",
      "----- CHECKING INPUT PARAMETERS CCCmodel------\n"
     ]
    }
   ],
   "source": [
    "from churn.config_manager.ccc_model_config_mgr import  CCCmodelConfig\n",
    "config_obj = CCCmodelConfig('/var/SP/data/home/csanc109/src/devel/use-cases/churn/input/dp_ccc_model.yaml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "from amdocs_informational_dataset.engine.call_centre_calls import CallCentreCalls\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import col, concat, concat_ws, date_format, dayofmonth, format_string, from_unixtime, length, \\\n",
    "\tlit, lower, lpad, month, regexp_replace, translate, udf, unix_timestamp, year, when, upper, collect_set, collect_list, \\\n",
    "    count as sql_count, min as sql_min, max as sql_max, struct, size, coalesce, sum as sql_sum\n",
    "from pyspark.sql.types import DateType, IntegerType, StringType, StructField, StructType\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from engine.general_functions import format_date, compute_diff_days, sum_horizontal\n",
    "from collections import Counter\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "CAT_PRECIO = \"PRECIO\"\n",
    "CAT_TERMINAL = \"TERMINAL\"\n",
    "CAT_CONTENIDOS = \"CONTENIDOS\"\n",
    "CAT_COMERCIAL = \"COMERCIAL\"\n",
    "CAT_NO_COMERCIAL = \"NO_COMERCIAL\"\n",
    "CAT_SERV_ATENCION = \"SERVICIO/ATENCION\"\n",
    "CAT_TECNICO = \"TECNICO\"\n",
    "CAT_BILLING = \"BILLING\"\n",
    "CAT_FRAUD = \"FRAUD\"\n",
    "NO_PROB = \"NO_PROB\"\n",
    "NA = \"NA\"\n",
    "\n",
    "# ADD CATEGORIES TO CALLS\n",
    "REASON1_PRECIO = [\"Cliente quiere pagar menos\", \"Clte quiere pagar menos\"]\n",
    "\n",
    "\n",
    "\n",
    "def __aggregate_ccc_calls_by_nif(df_all, process_date):\n",
    "\n",
    "    print(\"Calling to __aggregate_ccc_calls_by_nif.... {}\".format(process_date))\n",
    "\n",
    "    df_agg = (df_all\n",
    "              .withColumn(\"fx_interaction\",\n",
    "                          concat(col('year'), lpad(col('month'), 2, '0'), lpad(col('day'), 2, '0')))\n",
    "              .withColumn(\"tuples\", struct([\"fx_interaction\", \"Bucket\"]))\n",
    "              .groupby(\"NIF\")\n",
    "              .agg(sql_count(lit(1)).alias(\"num_interactions\"),\n",
    "                   sql_min(col('fx_interaction')).alias(\"first_interaction\"),\n",
    "                   sql_max(col('fx_interaction')).alias(\"latest_interaction\"),\n",
    "                   sql_max(col(\"tuples\")).alias(\"tuples_max\"),  # latest interaction: [max(fx_interaction), bucket]\n",
    "                   sql_min(col(\"tuples\")).alias(\"tuples_min\"),  # first interaction: [min(fx_interaction), bucket]\n",
    "                   collect_list('Bucket').alias(\"bucket_list\"),\n",
    "                   collect_set('Bucket').alias(\"bucket_set\"),\n",
    "                   sql_sum(\"Bucket_NA\").alias(\"num_NA_buckets\"),\n",
    "                   sql_sum(\"IVR\").alias(\"num_ivr_interactions\"),\n",
    "                   collect_list('CATEGORY_1').alias(\"cat1_list\"),\n",
    "                   collect_list('CATEGORY_2').alias(\"cat2_list\"),\n",
    "            ))\n",
    "\n",
    "    df_agg = df_agg.withColumn(\"ref_date\", format_date(lit(process_date)))\n",
    "    df_agg = (df_agg.withColumn(\"bucket_1st_interaction\", col(\"tuples_min\")[\"Bucket\"])\n",
    "              .withColumn(\"bucket_latest_interaction\", col(\"tuples_max\")[\"Bucket\"])\n",
    "              .withColumn(\"nb_diff_buckets\", size(\"bucket_set\"))\n",
    "              .drop(*['tuples_max', 'tuples_min'])\n",
    "              )\n",
    "\n",
    "    for cc in [\"first_interaction\", \"latest_interaction\"]:\n",
    "        df_agg = (df_agg.withColumn(\"fx_{}\".format(cc),\n",
    "                                    format_date(cc, filter_dates_1900=True))  # days before 1900 converted to None\n",
    "                  .withColumn(\"days_since_{}\".format(cc), compute_diff_days(\"fx_{}\".format(cc), \"ref_date\")))\n",
    "\n",
    "    df_agg = df_agg.drop(\"ref_date\")\n",
    "\n",
    "    def get_mode_problems(lst):\n",
    "        # filter NA y NO_PROB\n",
    "        if not lst: return None\n",
    "        lst=[ll for ll in lst if ll and ll!=NO_PROB]\n",
    "        if not lst: return None\n",
    "        dd = Counter(lst).most_common(2)\n",
    "        return dd[0][0]\n",
    "    get_mode_udf = udf(lambda lst: get_mode_problems(lst), StringType())\n",
    "\n",
    "    df_agg = (df_agg.withColumn(\"CAT1_MODE\", when(coalesce(size(col(\"cat1_list\")), lit(0)) == 0, \"None\").otherwise(get_mode_udf(col(\"cat1_list\"))))\n",
    "              .withColumn(\"CAT2_MODE\", when(coalesce(size(col(\"cat1_list\")), lit(0)) == 0, \"None\").otherwise(get_mode_udf(col(\"cat2_list\")))))\n",
    "\n",
    "    return df_agg\n",
    "\n",
    "\n",
    "def __get_agg_ccc_nif(spark, ccc_start_model, ccc_end_model, msisdn_list=None):\n",
    "    '''\n",
    "    msisdn_list a list of msisdn's to filter the calls. Usually, this list corresponds to portouts\n",
    "    '''\n",
    "    ccc = CallCentreCalls(spark)\n",
    "    ccc.prepareFeatures(ccc_end_model, ccc_start_model)\n",
    "    df_all = ccc.all_interactions\n",
    "    if msisdn_list:\n",
    "        df_all = df_all.where(col(\"msisdn\").isin(msisdn_list))\n",
    "        #df_ccc = df_ccc.where(col(\"msisdn\").isin(msisdn_list))\n",
    "    df_all = __set_categories(df_all)\n",
    "    df_agg = __aggregate_ccc_calls_by_nif(df_all, ccc_end_model)\n",
    "    return df_agg\n",
    "\n",
    "\n",
    "def __set_categories(df_all):\n",
    "\n",
    "    df_all = df_all.withColumn(\"is_fraud\", when(col(\"INT_RAZON\").rlike(\"(?i)enga.o comercial|venta enga.osa\") ,1).otherwise(0))\n",
    "    df_all = df_all.withColumn(\"Queja_Trato\", when(col(\"INT_SUBTIPO\").isin([\"Queja trato\", \"Quejas sevicios de atencion\"])\n",
    "                                    ,1).otherwise(0))\n",
    "\n",
    "    df_all = df_all.withColumn(\"CATEGORY_2\",\n",
    "                               # - - - - - - - - - PRECIO\n",
    "                               when(col('INT_TIPO').isin(REASON1_PRECIO) , CAT_PRECIO)\n",
    "                               # - - - - - - - - - SERVICIO/ATENCION\n",
    "                               .when(lower(col(\"INT_SUBTIPO\")).isin\n",
    "                                   ([\"Queja trato\".lower(), \"Quejas sevicios de atencion\".lower()]) ,CAT_SERV_ATENCION)\n",
    "                               # - - - - - - - - - ENGAÃ‘O\n",
    "                               .when(col(\"INT_RAZON\").rlike(\"(?i)enga.o comercial|venta enga.osa\"), CAT_FRAUD)\n",
    "                               # - - - - - - - - - TECNICO\n",
    "                               .when(col('INT_TIPO').rlike('(?i)^Averia') ,CAT_TECNICO)\n",
    "                               .when((col('INT_TIPO').rlike('(?i)^Transferencia') )&\n",
    "                                   (col(\"INT_SUBTIPO\").rlike('(?i)^Aver.as')), CAT_TECNICO)\n",
    "                               .when(col('INT_TIPO').rlike(\n",
    "                                   '(?i)^Inc Provis.*Neba|^Inc Provision Fibra|^Inc Provision DSL|^Incidencia%ecnica|^Incidencia%SGI|^Inc|^Incidencia'),\n",
    "                                     CAT_TECNICO)\n",
    "                               .when(col('INT_TIPO').rlike('(?i)^Consulta tec'), CAT_TECNICO)\n",
    "                               .when(col(\"Raw_Resultado\").rlike(\n",
    "                                   \"Raw_Resultado_Escalo|Raw_Resultado_Envio_tecnico|Raw_Resultado_Transferencia|Raw_Resultado_Reclamacion\"),\n",
    "                                     CAT_TECNICO)\n",
    "                               # - - - - - - - - - FACTURA\n",
    "                               .when(col('INT_TIPO').rlike('(?i)factura'), CAT_BILLING)\n",
    "                               # - - - - - - - - - TERMINAL\n",
    "                               .when(col(\"INT_TIPO\") == \"TERMINAL\", CAT_TERMINAL)\n",
    "                               # - - - - - - - - - NO_PROB :)\n",
    "                               .when((col(\"INT_TIPO\") == \"INFORMACION\") | ((col(\"INT_TIPO\") == \"TRANSFERENCIA\") & (\n",
    "                                           col(\"Bucket\") == \"Other customer information management\")),\n",
    "                                     NO_PROB).otherwise(NA)\n",
    "                               )  # end\n",
    "\n",
    "    df_all = df_all.na.fill(NA, subset=[\"CATEGORY_2\"])\n",
    "\n",
    "    df_all = (df_all.withColumn(\"CATEGORY_1\",\n",
    "                                when(col('CATEGORY_2').isin([CAT_PRECIO, CAT_TERMINAL, CAT_CONTENIDOS]), CAT_COMERCIAL)\n",
    "                                .when(col('CATEGORY_2').isin([CAT_SERV_ATENCION, CAT_TECNICO, CAT_BILLING, CAT_FRAUD]),\n",
    "                                      CAT_NO_COMERCIAL)\n",
    "                                .otherwise(col('CATEGORY_2'))))\n",
    "    # FALTA\n",
    "    # col('Raw_Productos') hay que desglosarlo\n",
    "    return df_all\n",
    "\n",
    "\n",
    "def check_partitions (df):\n",
    "    l = df.rdd.glom().map(len).collect()  # get length of each partition\n",
    "    print 'Smallest partition {}'.format(min(l))\n",
    "    print 'Largest partitions {}'.format(max(l))\n",
    "    print 'Avg. partition size {}'.format(sum(l)/len(l))\n",
    "    print 'Num partitions {}'.format(len(l))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get port requests table: start=20180501 end=20180531 ref_date=20180531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20181207-191652 [INFO ] '/user/csanc109/projects/churn/data/ccc_model/df_calls_for_portouts_nif_20180501_20180531_60' does not exist!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get port requests table: start=20180501 end=20180531 ref_date=20180531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20181207-191658 [INFO ] df_port=177171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time _set_categories = 0.00680894851685 minutes\n",
      "Calling to __aggregate_ccc_calls_by_nif.... 20180530\n",
      "Elapsed time __aggregate_ccc_calls_by_nif = 0.0564715027809 minutes\n",
      "Elapsed 5.16573588053e-08 minutes\n"
     ]
    }
   ],
   "source": [
    "from pykhaos.utils.pyspark_utils import union_all\n",
    "from pykhaos.utils.date_functions import move_date_n_days\n",
    "\n",
    "from churn.datapreparation.general.data_loader import get_port_requests_table\n",
    "n_ccc= config_obj.get_ccc_range_duration()\n",
    "portout_select_cols = [\"msisdn_a\", \"portout_date\"]  # [\"portout_date\",  \"msisdn_a\", \"label\", \"days_from_portout\"]\n",
    "\n",
    "\n",
    "df_port = get_port_requests_table(spark, config_obj, ref_date=None,\n",
    "                                  select_cols=portout_select_cols)\n",
    "\n",
    "portout_dates_list = df_port.select(\"portout_date\").distinct().rdd.map(lambda x: x[\"portout_date\"]).collect()\n",
    "\n",
    "from pykhaos.utils.pyspark_utils import union_all\n",
    "from pykhaos.utils.date_functions import move_date_n_days\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "dfs = []\n",
    "import time\n",
    "\n",
    "from churn.datapreparation.general.ccc_data_loader import get_calls_for_portouts, get_calls_for_portouts_filename\n",
    "from pyspark.sql.functions import concat_ws\n",
    "calls_for_portouts_filename = get_calls_for_portouts_filename(config_obj)\n",
    "from pykhaos.utils.hdfs_functions import check_hdfs_exists\n",
    "if not check_hdfs_exists(calls_for_portouts_filename):\n",
    "    if logger: logger.info(\"'{}' does not exist!\".format(calls_for_portouts_filename))\n",
    "    df_agg_all_ccc = get_calls_for_portouts(spark, config_obj, ref_date=None, portout_select_cols=[\"msisdn_a\", \"portout_date\"], logger=logger)\n",
    "    #if logger: logger.info(\"[Info ccc_model_data_loader] Number of NIFs df_agg_all_ccc: df_agg_all_ccc={} \".format(df_agg_all_ccc.count()))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #check_partitions(df_agg_all_ccc)\n",
    "\n",
    "    df_agg_all_ccc = df_agg_all_ccc.withColumn('cat1_list', concat_ws(\",\", col('cat1_list')))\n",
    "    df_agg_all_ccc = df_agg_all_ccc.withColumn('cat2_list', concat_ws(\",\", col('cat2_list')))\n",
    "    df_agg_all_ccc = df_agg_all_ccc.drop('bucket_list', 'bucket_set')\n",
    "    \n",
    "    print(\"Elapsed {} minutes\".format((time.time()-start_time)/60.0))\n",
    "    \n",
    "#     print(df_agg_all_ccc.dtypes)\n",
    "#     if logger: logger.info(\"Prepared to write to {}\".format(calls_for_portouts_filename))\n",
    "    #df_agg_all_ccc.write.mode('overwrite').format('csv').option('sep', '|').option('header', 'true').save(calls_for_portouts_filename)\n",
    "# else:\n",
    "#     if logger: logger.info(\"'{}' exists! Loading!\".format(calls_for_portouts_filename))\n",
    "#     df_agg_all_ccc = spark.read.option(\"delimiter\", \"|\").option(\"header\", True).csv(calls_for_portouts_filename)\n",
    "\n",
    "# if logger: logger.info(\"[Info ccc_model_data_loader] Number of NIFs df_agg_all_ccc: df_agg_all_ccc={} \".format(df_agg_all_ccc.count()))\n",
    "\n",
    "# if config_obj.get_model_target() == \"comercial\":\n",
    "#     df_agg_all_ccc = df_agg_all_ccc.withColumn(\"label\", when(col(\"CAT1_MODE\")==\"COMERCIAL\", 1).when(col(\"CAT1_MODE\")==\"NO_COMERCIAL\", 0).otherwise(-1))\n",
    "# else:\n",
    "#     if logger: logger.error(\"Model target {} is not implemented\".format(config_obj.get_model_target()))\n",
    "#     sys.exit()\n",
    "\n",
    "#check_partitions(df_agg_all_ccc)\n",
    "\n",
    "#if logger: logger.info(\"Number of rows {} (each row is a NIF)\".format(df_agg_all_ccc.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check_partitions(df_agg_all_ccc)\n",
    "df_agg_all_ccc = df_agg_all_ccc.repartition(200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_agg_all_ccc = df_agg_all_ccc.withColumn('cat1_list', concat_ws(\",\", col('cat1_list')))\n",
    "df_agg_all_ccc = df_agg_all_ccc.withColumn('cat2_list', concat_ws(\",\", col('cat2_list')))\n",
    "df_agg_all_ccc = df_agg_all_ccc.drop('bucket_list', 'bucket_set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user/csanc109/projects/churn/data/ccc_model/df_calls_for_portouts_nif_20180501_20180531_60\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-4bc8d26df299>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpykhaos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhdfs_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_hdfs_exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcheck_hdfs_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalls_for_portouts_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf_agg_all_ccc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sep'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'|'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'header'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalls_for_portouts_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/Anaconda/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(calls_for_portouts_filename)\n",
    "#calls_for_portouts_filename = '/user/csanc109/projects/churn/data/ccc_model/df_calls_for_portouts_nif_20180501_20180630_60'\n",
    "from pykhaos.utils.hdfs_functions import check_hdfs_exists\n",
    "check_hdfs_exists(calls_for_portouts_filename)\n",
    "df_agg_all_ccc.write.mode('overwrite').format('csv').option('sep', '|').option('header', 'true').save(calls_for_portouts_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROBANDO CATEGORIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_all.where(col(\"CATEGORY_1\")==\"NA\").select(\"INT_Tipo\", \"INT_Subtipo\",\"INT_Resultado\").groupBy(\"INT_Tipo\", \"INT_Tipo\",\"INT_Resultado\").agg(sql_count(\"*\").alias(\"count\")).sort(desc(\"count\")).show(30, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-319-5a05bf3cfa57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_agg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CAT1_MODE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CAT1_MODE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1133\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/Anaconda/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rbufsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_agg_all_ccc.select(\"CAT1_MODE\").groupBy(\"CAT1_MODE\").agg(sql_count(\"*\").alias(\"count\")).sort(desc(\"count\")).show(30, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'desc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-8f1817c82aaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_agg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CAT2_MODE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CAT2_MODE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'desc' is not defined"
     ]
    }
   ],
   "source": [
    "df_agg_all_ccc.select(\"CAT2_MODE\").groupBy(\"CAT2_MODE\").agg(sql_count(\"*\").alias(\"count\")).sort(desc(\"count\")).show(30, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "REVISANDO RESULTADOS ENCUESTAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from churn.utils.general_functions import get_nif_msisdn\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc, collect_set, concat, size, coalesce, col, lpad, struct, count as sql_count, regexp_replace, lit, min as sql_min, max as sql_max, collect_list, udf, when, desc, row_number\n",
    "from churn.datapreparation.engine.bajas_data_loader import load_reasons_encuestas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calls_for_portouts_filename = '/user/csanc109/projects/churn/data/ccc_model/df_calls_for_portouts_filename_nif_encuestas_20180501_20180630_60'\n",
    "df_agg_ccc_encuestas = spark.read.option(\"delimiter\", \"|\").option(\"header\", True).csv(calls_for_portouts_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NIF',\n",
       " 'num_interactions',\n",
       " 'first_interaction',\n",
       " 'latest_interaction',\n",
       " 'num_NA_buckets',\n",
       " 'num_ivr_interactions',\n",
       " 'cat1_list',\n",
       " 'cat2_list',\n",
       " 'bucket_1st_interaction',\n",
       " 'bucket_latest_interaction',\n",
       " 'nb_diff_buckets',\n",
       " 'fx_first_interaction',\n",
       " 'days_since_first_interaction',\n",
       " 'fx_latest_interaction',\n",
       " 'days_since_latest_interaction',\n",
       " 'CAT1_MODE',\n",
       " 'CAT2_MODE',\n",
       " 'REASON_ENCUESTA']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_agg_ccc_encuestas.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ccc = CallCentreCalls(spark)\n",
    "ccc.prepareFeatures(\"20180601\", \"20180301\")\n",
    "df_all = ccc.all_interactions\n",
    "df_all2 = df_all.where(col(\"NIF\").isin([\"67302557O\"])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------------+----+-----+---+-------------+---------------+----------------------+-------------+----------+-----------------+------+-------+--------------------------------+---------------+---------+-----------+----------+--------+-----------------+-----------+----------------+----------------------------------------------------------+--------------+------------+---------------+----------------+----------+-------------+-------------------------+-------------+-------------------------+-----------------+--------+---+---------+\n",
      "|nif      |msisdn   |partitioned_month|year|month|day|INT_Tipo     |INT_Subtipo    |INT_Razon             |INT_Resultado|DIRECTION |x_workgroup      |source|TYPE_TD|Sub_Bucket                      |Raw_Pagar_menos|Raw_Cobro|Raw_Precios|Raw_Averia|Raw_Alta|Raw_Desactivacion|Raw_Factura|Raw_Ofrecimiento|Bucket_Sub_Bucket                                         |Raw_Incidencia|Raw_Consulta|Raw_Informacion|Raw_Portabilidad|Raw_Cierre|Raw_Productos|Raw_Resultado            |Raw_Provision|Bucket                   |Raw_Transferencia|Raw_Baja|IVR|Bucket_NA|\n",
      "+---------+---------+-----------------+----+-----+---+-------------+---------------+----------------------+-------------+----------+-----------------+------+-------+--------------------------------+---------------+---------+-----------+----------+--------+-----------------+-----------+----------------+----------------------------------------------------------+--------------+------------+---------------+----------------+----------+-------------+-------------------------+-------------+-------------------------+-----------------+--------+---+---------+\n",
      "|67302557O|658111636|201803           |2018|3    |28 |INFORMACION  |VOLDEMORT      |N/A                   |CONTROL      |De entrada|IVR123PostA      |vf    |Llamada|NA                              |null           |null     |null       |null      |null    |null             |null       |null            |NA_NA                                                     |null          |null        |Raw_Informacion|null            |null      |null         |null                     |null         |NA                       |null             |null    |1  |1        |\n",
      "|67302557O|658111636|201804           |2018|4    |27 |TRANSFERENCIA|CONVERGENTE    |TARIFAS               |NULL         |De entrada|IVR123PostA      |vf    |Llamada|NA                              |null           |null     |null       |null      |null    |null             |null       |null            |NA_NA                                                     |null          |null        |null           |null            |null      |null         |null                     |null         |NA                       |Raw_Transferencia|null    |1  |1        |\n",
      "|67302557O|658111636|201803           |2018|3    |20 |TRANSFERENCIA|CONVERGENTE    |MENOS 30 DIAS         |NULL         |De entrada|IVR123PostA      |vf    |Llamada|NA                              |null           |null     |null       |null      |null    |null             |null       |null            |NA_NA                                                     |null          |null        |null           |null            |null      |null         |null                     |null         |NA                       |Raw_Transferencia|null    |1  |1        |\n",
      "|67302557O|658111636|201803           |2018|3    |28 |TRANSFERENCIA|CONVERGENTE    |MENOS 30 DIAS         |NULL         |De entrada|IVR123PostA      |vf    |Llamada|NA                              |null           |null     |null       |null      |null    |null             |null       |null            |NA_NA                                                     |null          |null        |null           |null            |null      |null         |null                     |null         |NA                       |Raw_Transferencia|null    |1  |1        |\n",
      "|67302557O|658111636|201803           |2018|3    |20 |INFORMACION  |MCARE FACTURA  |PRECONDICIONES KO     |CONTROL      |De entrada|VisualIVR123PostA|vf    |Llamada|NA                              |null           |null     |null       |null      |null    |null             |null       |null            |NA_NA                                                     |null          |null        |Raw_Informacion|null            |null      |null         |null                     |null         |NA                       |null             |null    |1  |1        |\n",
      "|67302557O|658111636|201803           |2018|3    |28 |INFORMACION  |MCARE FACTURA  |PRECONDICIONES KO     |CONTROL      |De entrada|VisualIVR123PostA|vf    |Llamada|NA                              |null           |null     |null       |null      |null    |null             |null       |null            |NA_NA                                                     |null          |null        |Raw_Informacion|null            |null      |null         |null                     |null         |NA                       |null             |null    |1  |1        |\n",
      "|67302557O|658111636|201804           |2018|4    |27 |INFORMACION  |MCARE FACTURA  |PRECONDICIONES KO     |CONTROL      |De entrada|VisualIVR123PostA|vf    |Llamada|NA                              |null           |null     |null       |null      |null    |null             |null       |null            |NA_NA                                                     |null          |null        |Raw_Informacion|null            |null      |null         |null                     |null         |NA                       |null             |null    |1  |1        |\n",
      "|67302557O|658111636|201804           |2018|4    |27 |INFORMACION  |MCARE FACTURA  |PRECONDICIONES KO     |CONTROL      |De entrada|VisualIVR123PostA|vf    |Llamada|NA                              |null           |null     |null       |null      |null    |null             |null       |null            |NA_NA                                                     |null          |null        |Raw_Informacion|null            |null      |null         |null                     |null         |NA                       |null             |null    |1  |1        |\n",
      "|67302557O|658111636|201804           |2018|4    |27 |INFORMACION  |MACROINCIDENCIA|FACTURA               |CONTROL      |De entrada|IVR123PostA      |vf    |Llamada|NA                              |null           |null     |null       |null      |null    |null             |null       |null            |NA_NA                                                     |null          |null        |Raw_Informacion|null            |null      |null         |null                     |null         |NA                       |null             |null    |1  |1        |\n",
      "|67302557O|672655052|201803           |2018|3    |20 |FACTURA      |DESCUENTOS     |VIGENTE               |INFORMACION  |De entrada|Konecta Platino  |vf    |Llamada|Sub_Bucket_Invoice clarification|null           |null     |null       |null      |null    |null             |Raw_Factura|null            |Bucket_Sub_Bucket_Billing - Postpaid_Invoice clarification|null          |null        |null           |null            |null      |null         |Raw_Resultado_Informacion|null         |Bucket_Billing - Postpaid|null             |null    |0  |0        |\n",
      "|67302557O|658111636|201803           |2018|3    |28 |INFORMACION  |FACTURA        |PS OK                 |CONTROL      |De entrada|IVR123PostA      |vf    |Llamada|NA                              |null           |null     |null       |null      |null    |null             |null       |null            |NA_NA                                                     |null          |null        |Raw_Informacion|null            |null      |null         |null                     |null         |NA                       |null             |null    |1  |1        |\n",
      "|67302557O|658111636|201804           |2018|4    |27 |INFORMACION  |FACTURA        |PS OK                 |CONTROL      |De entrada|IVR123PostA      |vf    |Llamada|NA                              |null           |null     |null       |null      |null    |null             |null       |null            |NA_NA                                                     |null          |null        |Raw_Informacion|null            |null      |null         |null                     |null         |NA                       |null             |null    |1  |1        |\n",
      "|67302557O|658111636|201804           |2018|4    |27 |INFORMACION  |FACTURA        |PS OK                 |CONTROL      |De entrada|IVR123PostA      |vf    |Llamada|NA                              |null           |null     |null       |null      |null    |null             |null       |null            |NA_NA                                                     |null          |null        |Raw_Informacion|null            |null      |null         |null                     |null         |NA                       |null             |null    |1  |1        |\n",
      "|67302557O|658111636|201804           |2018|4    |27 |INFORMACION  |COMPLETADA     |N/A                   |N/A          |De entrada|IVREncuestasA    |vf    |Llamada|NA                              |null           |null     |null       |null      |null    |null             |null       |null            |NA_NA                                                     |null          |null        |Raw_Informacion|null            |null      |null         |null                     |null         |NA                       |null             |null    |1  |1        |\n",
      "|67302557O|658111636|201803           |2018|3    |28 |INFORMACION  |P.PROACTIVA    |CLIENTE SIN DEUDA OTRA|CONTROL      |De entrada|IVR123PostA      |vf    |Llamada|NA                              |null           |null     |null       |null      |null    |null             |null       |null            |NA_NA                                                     |null          |null        |Raw_Informacion|null            |null      |null         |null                     |null         |NA                       |null             |null    |1  |1        |\n",
      "|67302557O|658111636|201803           |2018|3    |20 |INFORMACION  |P.PROACTIVA    |CLIENTE SIN DEUDA OTRA|CONTROL      |De entrada|IVR123PostA      |vf    |Llamada|NA                              |null           |null     |null       |null      |null    |null             |null       |null            |NA_NA                                                     |null          |null        |Raw_Informacion|null            |null      |null         |null                     |null         |NA                       |null             |null    |1  |1        |\n",
      "|67302557O|658111636|201804           |2018|4    |27 |INFORMACION  |P.PROACTIVA    |CLIENTE SIN DEUDA OTRA|CONTROL      |De entrada|IVR123PostA      |vf    |Llamada|NA                              |null           |null     |null       |null      |null    |null             |null       |null            |NA_NA                                                     |null          |null        |Raw_Informacion|null            |null      |null         |null                     |null         |NA                       |null             |null    |1  |1        |\n",
      "|67302557O|658111636|201804           |2018|4    |27 |INFORMACION  |P.PROACTIVA    |CLIENTE SIN DEUDA OTRA|CONTROL      |De entrada|IVR123PostA      |vf    |Llamada|NA                              |null           |null     |null       |null      |null    |null             |null       |null            |NA_NA                                                     |null          |null        |Raw_Informacion|null            |null      |null         |null                     |null         |NA                       |null             |null    |1  |1        |\n",
      "|67302557O|658111636|201804           |2018|4    |27 |TRANSFERENCIA|CONVERGENTE    |TIPOLOGIA CONVERGENTE |NULL         |De entrada|IVR123PostA      |vf    |Llamada|NA                              |null           |null     |null       |null      |null    |null             |null       |null            |NA_NA                                                     |null          |null        |null           |null            |null      |null         |null                     |null         |NA                       |Raw_Transferencia|null    |1  |1        |\n",
      "+---------+---------+-----------------+----+-----+---+-------------+---------------+----------------------+-------------+----------+-----------------+------+-------+--------------------------------+---------------+---------+-----------+----------+--------+-----------------+-----------+----------------+----------------------------------------------------------+--------------+------------+---------------+----------------+----------+-------------+-------------------------+-------------+-------------------------+-----------------+--------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_all2.show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2921.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 262.0 failed 4 times, most recent failure: Lost task 0.3 in stage 262.0 (TID 12018, vgddp367hr.dc.sedc.internal.vodafone.com, executor 31): java.io.FileNotFoundException: File does not exist: /user/csanc109/projects/churn/data/ccc_model/df_calls_for_portouts_filename_nif_encuestas_20180501_20180630_60/part-00067-4d72064d-cd6d-4b3b-9e0a-398e595deb29.csv\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:66)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:56)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:2007)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1977)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1890)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:572)\n\tat org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getBlockLocations(AuthorizationProviderProxyClientProtocol.java:89)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2216)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2212)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1796)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2210)\n\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:175)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:333)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2371)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2370)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2377)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2113)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2112)\n\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2795)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2112)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2327)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:248)\n\tat sun.reflect.GeneratedMethodAccessor110.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.FileNotFoundException: File does not exist: /user/csanc109/projects/churn/data/ccc_model/df_calls_for_portouts_filename_nif_encuestas_20180501_20180630_60/part-00067-4d72064d-cd6d-4b3b-9e0a-398e595deb29.csv\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:66)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:56)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:2007)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1977)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1890)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:572)\n\tat org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getBlockLocations(AuthorizationProviderProxyClientProtocol.java:89)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2216)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2212)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1796)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2210)\n\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:175)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-232f5eab3597>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_agg_ccc_encuestas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"REASON_ENCUESTA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"FRAUD\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"num_interactions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misNotNull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NIF\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CAT2_MODE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cat2_list\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cat1_list\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"num_interactions\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2921.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 262.0 failed 4 times, most recent failure: Lost task 0.3 in stage 262.0 (TID 12018, vgddp367hr.dc.sedc.internal.vodafone.com, executor 31): java.io.FileNotFoundException: File does not exist: /user/csanc109/projects/churn/data/ccc_model/df_calls_for_portouts_filename_nif_encuestas_20180501_20180630_60/part-00067-4d72064d-cd6d-4b3b-9e0a-398e595deb29.csv\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:66)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:56)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:2007)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1977)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1890)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:572)\n\tat org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getBlockLocations(AuthorizationProviderProxyClientProtocol.java:89)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2216)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2212)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1796)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2210)\n\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:175)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:333)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2371)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2765)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2370)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2377)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2113)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2112)\n\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2795)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2112)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2327)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:248)\n\tat sun.reflect.GeneratedMethodAccessor110.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.FileNotFoundException: File does not exist: /user/csanc109/projects/churn/data/ccc_model/df_calls_for_portouts_filename_nif_encuestas_20180501_20180630_60/part-00067-4d72064d-cd6d-4b3b-9e0a-398e595deb29.csv\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:66)\n\tat org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:56)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:2007)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1977)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1890)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:572)\n\tat org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getBlockLocations(AuthorizationProviderProxyClientProtocol.java:89)\n\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:365)\n\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\n\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2216)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2212)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1796)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2210)\n\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:175)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:826)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:99)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "df_agg_ccc_encuestas.where( (col(\"REASON_ENCUESTA\")==\"FRAUD\") & (col(\"num_interactions\").isNotNull())).select(\"NIF\", \"CAT2_MODE\", \"cat2_list\", \"cat1_list\", \"num_interactions\").show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----+\n",
      "|REASON_ENCUESTA  |CAT2_MODE        |count|\n",
      "+-----------------+-----------------+-----+\n",
      "|null             |null             |95221|\n",
      "|null             |BILLING          |15921|\n",
      "|null             |TECNICO          |10580|\n",
      "|null             |TERMINAL         |2408 |\n",
      "|PRECIO           |null             |2264 |\n",
      "|SERVICIO/ATENCION|null             |1175 |\n",
      "|FRAUD            |null             |928  |\n",
      "|SITUACION        |null             |666  |\n",
      "|CONTENIDOS       |null             |662  |\n",
      "|null             |FRAUD            |293  |\n",
      "|BILLING          |null             |268  |\n",
      "|null             |SERVICIO/ATENCION|78   |\n",
      "|OTROS            |null             |52   |\n",
      "|NO_FIBRA         |null             |10   |\n",
      "|SERVICIO/ATENCION|TECNICO          |3    |\n",
      "|PRECIO           |BILLING          |3    |\n",
      "|SERVICIO/ATENCION|BILLING          |2    |\n",
      "|PRECIO           |FRAUD            |1    |\n",
      "|PRECIO           |TECNICO          |1    |\n",
      "|FRAUD            |BILLING          |1    |\n",
      "+-----------------+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_agg_ccc_encuestas.select(\"CAT2_MODE\", \"REASON_ENCUESTA\").groupBy(\"REASON_ENCUESTA\", \"CAT2_MODE\").agg(sql_count(\"*\").alias(\"count\")).sort(desc(\"count\")).show(150, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WRITER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "llist = [('bob', '2015-01-13', 4), ('alice', '2015-04-23',10)]\n",
    "left = spark.createDataFrame(llist, ['name','date','duration'])\n",
    "right = spark.createDataFrame([('fffff', 100),('bob', 23)],['name','upload'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fbb_services_nif',\n",
       " 'fbb_fx_first_nif',\n",
       " 'mobile_services_nif',\n",
       " 'mobile_fx_first_nif',\n",
       " 'tv_services_nif',\n",
       " 'tv_fx_first_nif',\n",
       " 'prepaid_services_nif',\n",
       " 'prepaid_fx_first_nif',\n",
       " 'bam_mobile_services_nif',\n",
       " 'bam_mobile_fx_first_nif',\n",
       " 'fixed_services_nif',\n",
       " 'fixed_fx_first_nif',\n",
       " 'bam_services_nif',\n",
       " 'bam_fx_first_nif',\n",
       " 'flag_prepaid_nif',\n",
       " 'seg_pospaid_nif',\n",
       " 'num_football_nif',\n",
       " 'total_football_price_nif',\n",
       " 'total_num_services_nif',\n",
       " 'fx_bam_fx_first_nif',\n",
       " 'days_since_bam_fx_first_nif',\n",
       " 'fx_bam_mobile_fx_first_nif',\n",
       " 'days_since_bam_mobile_fx_first_nif',\n",
       " 'fx_fbb_fx_first_nif',\n",
       " 'days_since_fbb_fx_first_nif',\n",
       " 'fx_fixed_fx_first_nif',\n",
       " 'days_since_fixed_fx_first_nif',\n",
       " 'fx_mobile_fx_first_nif',\n",
       " 'days_since_mobile_fx_first_nif',\n",
       " 'fx_prepaid_fx_first_nif',\n",
       " 'days_since_prepaid_fx_first_nif',\n",
       " 'fx_tv_fx_first_nif',\n",
       " 'days_since_tv_fx_first_nif']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[col_ for col_ in df.columns if col_.endswith(\"nif\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tablename = \"/data/udf/vf_es/churn/ccc_model/comercial/df_20180501_20180531_n60_comercial\"\n",
    "df = spark.read.option(\"delimiter\", \"|\").option(\"header\", True).csv(tablename)\n",
    "#df = spark.read.load(tablename)\n",
    "df = df.where(col(\"label\")!=-1)\n",
    "#df = df.repartition(1)\n",
    "#df.write.mode('overwrite').format('csv').option('sep', '|').option('header', 'true').save(\"/data/udf/vf_es/churn/ccc_model/comercial/df_20180501_20180630_n60_comercial_csv\")\n",
    "#len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running system command: hdfs dfs -copyToLocal hdfs://vgddp349hr.dc.sedc.internal.vodafone.com/data/udf/vf_es/churn/ccc_model/comercial/df_20180501_20180531_n60_comercial/ /var/SP/data/home/csanc109/tmp/kk/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pykhaos.utils.hdfs_functions import move_hdfs_dir_to_local\n",
    "\n",
    "move_hdfs_dir_to_local(\"hdfs://vgddp349hr.dc.sedc.internal.vodafone.com/data/udf/vf_es/churn/ccc_model/comercial/df_20180501_20180531_n60_comercial/\", \"/var/SP/data/home/csanc109/tmp/kk/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100%\n"
     ]
    }
   ],
   "source": [
    "df = h2o.import_file(\"hdfs://vgddp349hr.dc.sedc.internal.vodafone.com/data/udf/vf_es/churn/ccc_model/comercial/df_20180501_20180531_n60_comercial\", pattern = \".*\\.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('HOLAAAAA', <logging.RootLogger object at 0x7f2b7898ba90>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20181208-121935 [INFO ] ***** locked=False healthy=True\n",
      "20181208-121935 [INFO ] Trying to import '/data/udf/vf_es/churn/ccc_model/comercial/df_20180501_20180531_n60_comercial' hdfs=True parquet=False\n",
      "20181208-121937 [INFO ] Trying to import hdfs://vgddp349hr.dc.sedc.internal.vodafone.com/data/udf/vf_es/churn/ccc_model/comercial/df_20180501_20180531_n60_comercial\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20181208-121937 [INFO ] file hdfs://vgddp349hr.dc.sedc.internal.vodafone.com/data/udf/vf_es/churn/ccc_model/comercial/df_20180501_20180531_n60_comercial found at node 0\n"
     ]
    }
   ],
   "source": [
    "from pykhaos.modeling.h2o.h2o_functions import import_file_to_h2o_frame\n",
    "\n",
    "df_h2o_imported = import_file_to_h2o_frame(\"/data/udf/vf_es/churn/ccc_model/comercial/df_20180501_20180531_n60_comercial\", hdfs=True, parquet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "20181208-122104 [INFO ] Trying to import '/data/udf/vf_es/churn/ccc_model/comercial/df_20180501_20180531_n60_comercial' hdfs=True parquet=False\n",
    "20181208-122105 [INFO ] Trying to import hdfs://vgddp349hr.dc.sedc.internal.vodafone.com/data/udf/vf_es/churn/ccc_model/comercial/df_20180501_20180531_n60_comercial\n",
    "20181208-122105 [ERROR] file hdfs://vgddp349hr.dc.sedc.internal.vodafone.com/data/udf/vf_es/churn/ccc_model/comercial/df_20180501_20180531_n60_comercial could not be found at node 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54323..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: java version \"1.7.0_79\"; Java(TM) SE Runtime Environment (build 1.7.0_79-b15); Java HotSpot(TM) 64-Bit Server VM (build 24.79-b02, mixed mode)\n",
      "  Starting server from /opt/cloudera/parcels/Anaconda-2.5.0/lib/python2.7/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmpw07zOW\n",
      "  JVM stdout: /tmp/tmpw07zOW/h2o_csanc109_started_from_python.out\n",
      "  JVM stderr: /tmp/tmpw07zOW/h2o_csanc109_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54323\n",
      "Connecting to H2O server at http://127.0.0.1:54323... successful.\n",
      "Warning: Your H2O cluster version is too old (6 months and 16 days)! Please download and install the latest version from http://h2o.ai/download/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>01 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Europe/Rome</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.18.0.10</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>6 months and 16 days !!!</td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_csanc109_6wbgof</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>5.333 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>56</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>56</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54323</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>AutoML, XGBoost, Algos, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>2.7.11 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------\n",
       "H2O cluster uptime:         01 secs\n",
       "H2O cluster timezone:       Europe/Rome\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.18.0.10\n",
       "H2O cluster version age:    6 months and 16 days !!!\n",
       "H2O cluster name:           H2O_from_python_csanc109_6wbgof\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    5.333 Gb\n",
       "H2O cluster total cores:    56\n",
       "H2O cluster allowed cores:  56\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54323\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         AutoML, XGBoost, Algos, Core V3, Core V4\n",
       "Python version:             2.7.11 final\n",
       "--------------------------  ----------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h2o\n",
    "#h2o.cluster().shutdown()\n",
    "\n",
    "h2o.init(nthreads=-1, port=\"54323\", max_mem_size=\"6g\")\n",
    "\n",
    "# from pykhaos.utils.hdfs_functions import move_hdfs_dir_to_local, move_hdfs_file_to_local\n",
    "\n",
    "# #local_filename = move_hdfs_file_to_local('/data/udf/vf_es/churn/ccc_model/comercial/df_20180501_20180630_n60_comercial_csv/', \"/var/SP/data/home/csanc109/tmp/\")\n",
    "# print(local_filename)\n",
    "# df_h2o_imported = h2o.import_file(local_filename, pattern = \".*\\.csv\") #, sep = \"|\",header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('amdocs_table_reader', 'all_amdocs_inf_dataset', '20180531')\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "u'Path does not exist: hdfs://nameservice1/data/udf/vf_es/amdocs_inf_dataset/all_amdocs_inf_dataset/year=2018/month=5/day=31;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-719964cb0872>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                                                                     int(closing_day[6:]))\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u'Path does not exist: hdfs://nameservice1/data/udf/vf_es/amdocs_inf_dataset/all_amdocs_inf_dataset/year=2018/month=5/day=31;'"
     ]
    }
   ],
   "source": [
    "closing_day = \"20180531\"\n",
    "table_name = \"all_amdocs_inf_dataset\"\n",
    "print(\"amdocs_table_reader\", table_name, closing_day)\n",
    "table_name = '/data/udf/vf_es/amdocs_inf_dataset/{}/year={}/month={}/day={}'.format(table_name, int(closing_day[:4]),\n",
    "                                                                                                    int(closing_day[4:6]),\n",
    "                                                                                                    int(closing_day[6:]))\n",
    "\n",
    "df_src = spark.read.load(table_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
