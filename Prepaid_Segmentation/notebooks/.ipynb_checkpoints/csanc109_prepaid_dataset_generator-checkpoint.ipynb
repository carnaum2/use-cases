{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Ended spark session: 0.0262041091919 secs | default parallelism=10\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # THIS MUST BE THE FIRST CELL IN THE NOTEBOOK\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    import time\n",
    "    import src.pycharm_workspace.lib_csanc109.utils.pyspark_configuration as pyspark_config\n",
    "\n",
    "    start_time = time.time()\n",
    "    sc, spark, sql_context = pyspark_config.get_spark_session(app_name=\"Euphoria Prepaid Data\", log_level=\"OFF\")\n",
    "    print(\"Ended spark session: {} secs | default parallelism={}\".format(time.time()-start_time, sc.defaultParallelism))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20180721-230201 [INFO ] Logging to file /var/SP/data/home/csanc109/logging/euphoria_20180721_230201.log\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os, sys\n",
    "import datetime as dt\n",
    "import src.pycharm_workspace.lib_csanc109.utils.custom_logger as clogger\n",
    "\n",
    "HOME_SRC = os.path.join(os.environ.get('BDA_USER_HOME', ''), \"src\")\n",
    "\n",
    "PREPAID_EUPHORIA_SRC = os.path.join(os.environ.get('BDA_USER_HOME', ''), \"src\", \"pycharm_workspace\", \"PrepaidRevenueBoosting\")\n",
    "if PREPAID_EUPHORIA_SRC not in sys.path: \n",
    "    sys.path.append(PREPAID_EUPHORIA_SRC)\n",
    "    \n",
    "WHOAMI= os.getenv(\"USER\")\n",
    "    \n",
    "    \n",
    "# Create logger object (messages to stdout and file)\n",
    "logging_file = os.path.join(os.environ.get('BDA_USER_HOME', ''),\"logging\",\"euphoria_\"+dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\")+\".log\")\n",
    "logger = clogger.get_custom_logger(logging_file, std_channel=sys.stderr)\n",
    "logger.info(\"Logging to file {}\".format(logging_file))\n",
    "                      \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20180721-230201 [INFO ] ######################################################################\n",
      "20180721-230201 [INFO ] ##### EJECUTING PREPAID EUPHORIA CAMPAIGN AT 2018-07-21 23:02:01 #####\n",
      "20180721-230201 [INFO ] ######################################################################\n"
     ]
    }
   ],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql.functions import substring\n",
    "from pyspark.sql.functions import (unix_timestamp, udf,col,max as sql_max, stddev as sql_stddev, when, count, isnull, concat, lpad, trim, lit, sum as sql_sum, length, upper)\n",
    "import datetime as dt\n",
    "from prepaid_euphoria import PrepaidEuphoria\n",
    "\n",
    "right_now = dt.datetime.now()\n",
    "\n",
    "\n",
    "ANALYSIS_PERIOD_START = \"20180201\"\n",
    "ANALYSIS_PERIOD_END   = \"20180531\"\n",
    "\n",
    "\n",
    "analysis_start_obj = dt.datetime.strptime(ANALYSIS_PERIOD_START, \"%Y%m%d\")\n",
    "analysis_end_obj = dt.datetime.strptime(ANALYSIS_PERIOD_END, \"%Y%m%d\")\n",
    "\n",
    "save_table = True\n",
    "\n",
    "reference_month = ANALYSIS_PERIOD_END[:6]\n",
    "\n",
    "print_string = '##### EJECUTING PREPAID EUPHORIA CAMPAIGN AT {right_now} #####'\\\n",
    "    .format(right_now=right_now.strftime('%Y-%m-%d %X'))\n",
    "logger.info('#'*len(print_string))\n",
    "logger.info(print_string)\n",
    "logger.info('#'*len(print_string))\n",
    "\n",
    "num_samples = 100\n",
    "debug_mode = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepago "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_prepago = (spark.read.table(\"raw_es.vf_pre_ac_final\") # se calcula a cierre de mes, en concreto el dia 2 del mes sgte\n",
    "              .where(col(\"estado_servicio\").isin(['CA', 'FCA', 'AC', 'FEX']))\n",
    "              .where((col(\"year\") == int(reference_month[:4]))\n",
    "                   & (col(\"month\") == int(reference_month[4:])))\n",
    "              .where(col(\"X_FECHA_NACIMIENTO\") != \"X_FE\")\n",
    "              .withColumn(\"X_FECHA_NACIMIENTO\", when(length(col(\"X_FECHA_NACIMIENTO\"))>0,col(\"X_FECHA_NACIMIENTO\")))\n",
    "              .withColumn(\"NUM_DOCUMENTO_CLIENTE\", when(length(col(\"NUM_DOCUMENTO_CLIENTE\"))>0,col(\"NUM_DOCUMENTO_CLIENTE\")))\n",
    "              .withColumn(\"NUM_DOCUMENTO_COMPRADOR\", when(length(col(\"NUM_DOCUMENTO_COMPRADOR\"))>0,col(\"NUM_DOCUMENTO_COMPRADOR\")))\n",
    "              #.withColumn(\"age_in_years\", get_customer_age_udf(col(\"X_FECHA_NACIMIENTO\"), reference_month))\n",
    "              .withColumn(\"age_in_years\", lit(1))\n",
    "             # .withColumn(\"nacionalidad\", substitute_crappy_characters_udf(col(\"nacionalidad\")))\n",
    "             .select([\"Fecha_ejecucion\", \"msisdn\", \"nacionalidad\", \"num_prepago\", \"estado_servicio\",\n",
    "                      \"num_pospago\", \"age_in_years\", \"tipo_documento_comprador\", \"codigo_plan_precios\",\n",
    "                      \"x_fecha_nacimiento\", \"fx_1llamada\", \"year\", \"month\"])\n",
    ".withColumnRenamed(\"year\", \"year_prepaid\")\n",
    ".withColumnRenamed(\"month\", \"month_prepaid\"))\n",
    "\n",
    "#################################\n",
    "# Reduce the number of rows if debug mode - this will reduce the size of the dataframes due to left joins\n",
    "if debug_mode: # and len(df_prepago.take(1)) > 0:\n",
    "    print(\"Taking a sample of {}\".format(num_samples))\n",
    "    df_prepago = spark.createDataFrame(df_prepago.take(num_samples))\n",
    "\n",
    "\n",
    "\n",
    "if len(df_prepago.head(1)) == 0 :\n",
    "    logger.error(\"Program does not make sense if prepago dataframe is empty. Exitting...\")\n",
    "    assert(False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date Master "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_peuph_date_master = (spark.read.table(\"tests_es.csanc109_dates_master\")\n",
    "                     .where((col(\"beginning_of_day_ts\") >=  analysis_start_obj) &\n",
    "                            (col(\"beginning_of_day_ts\") <=  analysis_end_obj)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transferencias de Saldo - add_transfer_balance_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add_transfer_balance_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, struct, concat_ws\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "conv_to_timestamp_udf = udf(lambda x: from_unixtime(unix_timestamp(str(x[0])+\"/\"+str(x[1])+\"/\"+str(x[2]), 'dd/MM/yyy')), TimestampType())\n",
    "\n",
    "# checked\n",
    "df_balance_receptor = (spark.read.table(\"raw_es.prepaid_transfbalance\")\n",
    "              .withColumn('entry_ts', from_unixtime(unix_timestamp(concat_ws('/',\"day\", \"month\", \"year\"), 'dd/MM/yyy')))\n",
    "                        .where((col(\"entry_ts\") >=  analysis_start_obj) &\n",
    "                            (col(\"entry_ts\") <=  analysis_end_obj))\n",
    "                                      .groupBy(\"msisdn_receptor\",\n",
    "                                               \"year\",\n",
    "                                               \"month\",\n",
    "                                               \"day\")\n",
    "                                      .agg(sql_sum(\"importe_traspasado\").alias(\"importe_traspasado_receptor\"),\n",
    "                                           count(\"importe_traspasado\").alias(\"num_rec\"))\n",
    "                                      .select(col(\"msisdn_receptor\"), \n",
    "                                              col(\"year\"), \n",
    "                                              col(\"month\"), \n",
    "                                              col(\"day\"), \n",
    "                                              col(\"importe_traspasado_receptor\"),\n",
    "                                             col(\"num_rec\"))\n",
    "                       .withColumnRenamed(\"year\", \"year_receptor\")\n",
    "                       .withColumnRenamed(\"month\", \"month_receptor\")\n",
    "                       .withColumnRenamed(\"day\", \"day_receptor\")\n",
    "                      )\n",
    "                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#checked\n",
    "df_balance_emisor = (spark.read.table(\"raw_es.prepaid_transfbalance\")\n",
    "              .withColumn('entry_ts', from_unixtime(unix_timestamp(concat_ws('/',\"day\", \"month\", \"year\"), 'dd/MM/yyy')))\n",
    "                        .where((col(\"entry_ts\") >=  analysis_start_obj) &\n",
    "                            (col(\"entry_ts\") <=  analysis_end_obj))\n",
    "                                      .groupBy(\"msisdn_emisor\",\n",
    "                                               \"year\",\n",
    "                                               \"month\",\n",
    "                                               \"day\")\n",
    "                                      .agg(sql_sum(\"importe_traspasado\").alias(\"importe_traspasado_emisor\"),\n",
    "                                           sql_sum(\"importe_cargo\").alias(\"importe_cargo_emisor\"),\n",
    "                                           count(\"importe_traspasado\").alias(\"num_em\"))\n",
    "                                      .select(col(\"msisdn_emisor\"), \n",
    "                                              col(\"year\"), \n",
    "                                              col(\"month\"), \n",
    "                                              col(\"day\"), \n",
    "                                              col(\"importe_traspasado_emisor\"),\n",
    "                                              col(\"importe_cargo_emisor\"),\n",
    "                                             col(\"num_em\"))\n",
    "                                            .withColumnRenamed(\"year\", \"year_emisor\")\n",
    "                                            .withColumnRenamed(\"month\", \"month_emisor\")\n",
    "                                            .withColumnRenamed(\"day\", \"day_emisor\"))\n",
    "                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPUPS - add_topup_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "udf_parse_importe = udf(lambda x:int(x.replace(\"+\",\"\"))/10000,IntegerType())\n",
    "\n",
    "#checked\n",
    "df_topups = (spark.read.table(\"raw_es.billingtopsups_rechargescash\")\n",
    "                            .where((col(\"fechaoperacion\") >=  analysis_start_obj) &\n",
    "                            (col(\"fechaoperacion\") <=  analysis_end_obj))\n",
    "                       .withColumn('importe_int',udf_parse_importe('importe'))\n",
    "                       .groupBy(\"ndc_msisdn\",\n",
    "                                \"year\",\n",
    "                                \"month\",\n",
    "                                \"day\")\n",
    "                        .agg(sql_sum(\"importe_int\").alias(\"tu_amount\"),\n",
    "                               count(\"importe_int\").alias(\"tu_num\"))\n",
    "                        .select(col(\"ndc_msisdn\"), \n",
    "                                col(\"year\"), \n",
    "                                col(\"month\"), \n",
    "                                col(\"day\"), \n",
    "                                #      col(\"tu_ts\"),\n",
    "                                col(\"tu_amount\"),\n",
    "                                col(\"tu_num\"))\n",
    "                       .withColumn(\"tu_bin\", when(col(\"tu_num\")>0, 1).otherwise(0))\n",
    "                       .withColumnRenamed(\"year\", \"year_topups\")\n",
    "                       .withColumnRenamed(\"month\", \"month_topups\")\n",
    "                       .withColumnRenamed(\"day\", \"day_topups\")\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adelantos de saldo - add_advance_balance_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add_advance_balance_information\n",
    "#         Method used to add advance balance data to a mega hive table. The fields to be added are:\n",
    "#         abal_amount: The amount of balance that was advanced.\n",
    "#         abal_num: Number of advance balances.\n",
    "#         abal_payment: The amount that was paid due to balance advances.\n",
    "#         Advance balance payments are only dealt with partially.\n",
    "#         :param input_table_name: Input mega Hive table name.\n",
    "#         :param output_table_name: Output mega Hive table name.\n",
    "#         :param drop_input_table: Whether to drop input Hive table.\n",
    "\n",
    "# day/month/year == f_solicitud\n",
    "# f_recuperacion\n",
    "\n",
    "df_advance_solicitado = (spark.read.table(\"raw_es.prepaid_advancebalance\")\n",
    "                        .where((col(\"f_solicitud\") >=  analysis_start_obj) &\n",
    "                               (col(\"f_solicitud\") <=  analysis_end_obj))\n",
    "                       .groupBy(\"msisdn\",\n",
    "                                \"year\",\n",
    "                                \"month\",\n",
    "                                \"day\")\n",
    "                        .agg(sql_sum(\"importe_anticipo\").alias(\"abal_amount\"),\n",
    "                               count(\"importe_anticipo\").alias(\"abal_num\"))\n",
    "                        .select(col(\"msisdn\"), \n",
    "                                col(\"year\"), \n",
    "                                col(\"month\"), \n",
    "                                col(\"day\"), \n",
    "                                col(\"abal_amount\"),\n",
    "                                col(\"abal_num\"))\n",
    "                       .withColumnRenamed(\"year\", \"year_advance\")\n",
    "                       .withColumnRenamed(\"month\", \"month_advance\")\n",
    "                       .withColumnRenamed(\"day\", \"day_advance\")\n",
    "                       .withColumnRenamed(\"msisdn\", \"msisdn_advance\")\n",
    "                        )\n",
    "\n",
    "                         \n",
    "df_advance_recuperado = (spark.read.table(\"raw_es.prepaid_advancebalance\")\n",
    "                       .where((col(\"f_recuperacion\") >=  analysis_start_obj) &\n",
    "                              (col(\"f_recuperacion\") <=  analysis_end_obj))\n",
    "                       .groupBy(\"msisdn\",\n",
    "                                \"year\",\n",
    "                                \"month\",\n",
    "                                \"day\")\n",
    "                       .agg(sql_sum(\"imp_recuperado\").alias(\"abal_payment\"),\n",
    "                            count(\"imp_recuperado\").alias(\"abal_payment_num\"))\n",
    "                       .select(col(\"msisdn\"), \n",
    "                               col(\"year\"), \n",
    "                               col(\"month\"), \n",
    "                               col(\"day\"), \n",
    "                               col(\"abal_payment\"),\n",
    "                               col(\"abal_payment_num\"))\n",
    "                       .withColumnRenamed(\"msisdn\", \"msisdn_rec\")\n",
    "                       .withColumnRenamed(\"year\", \"year_rec\")\n",
    "                       .withColumnRenamed(\"month\", \"month_rec\")\n",
    "                       .withColumnRenamed(\"day\", \"day_rec\"))\n",
    "   \n",
    "              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_advance_solicitado.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance de saldo (ppio y final del dia) - add_client_balance_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (expr)\n",
    "from pyspark.sql.functions import (asc,lag, udf,col,max as sql_max, when, countDistinct, isnull, concat, lpad, trim, lower, lit, min as sql_min, sum as sql_avg, sum as sql_avg, length, upper)\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# bal_bod: Balance at the beginning of the day.\n",
    "# bal_eod: Balance at the end of the day.\n",
    "# bal_diff: bal_eod - bal_bod.\n",
    "\n",
    "df_balance = (spark.read.table(\"raw_es.prepaid_clientbalance\")\n",
    "                        .where((col(\"fecha\") >=  analysis_start_obj) &\n",
    "                               (col(\"fecha\") <=  analysis_end_obj))\n",
    "                       .groupBy(\"msisdn\",\n",
    "                                \"fecha\",\n",
    "                                \"year\",\n",
    "                                \"month\",\n",
    "                                \"day\")\n",
    "                        .agg(sql_avg(\"saldo\").alias(\"saldo\"))\n",
    "                        .select(col(\"saldo\"), \n",
    "                                col(\"year\"), \n",
    "                                col(\"month\"), \n",
    "                                col(\"day\"),\n",
    "                                col(\"fecha\"),\n",
    "                                col(\"msisdn\")) \n",
    "                       .withColumnRenamed(\"year\", \"year_balance\")\n",
    "                       .withColumnRenamed(\"month\", \"month_balance\")\n",
    "                       .withColumnRenamed(\"day\", \"day_balance\")\n",
    "                       .withColumnRenamed(\"msisdn\", \"msisdn_balance\")\n",
    "                       .withColumnRenamed(\"saldo\", \"bal_bod\"))\n",
    "\n",
    "# FIX Me compute previous using a window function!!!!\n",
    "my_window = Window.partitionBy(\"msisdn_balance\").orderBy(col(\"fecha\").cast(\"timestamp\"))\n",
    "# https://www.arundhaj.com/blog/calculate-difference-with-previous-row-in-pyspark.html\n",
    "\n",
    "# df_balance_previous = (df_balance\n",
    "#                        .withColumnRenamed(\"bal_bod\", \"bal_eod\")\n",
    "#                        .withColumnRenamed(\"msisdn_balance\", \"msisdn_balance_previous\")\n",
    "#                        .withColumn(\"fecha_previous\", col(\"fecha\").cast(\"timestamp\") - expr(\"INTERVAL 1 DAYS\")))\n",
    "\n",
    "# df_balance_previous = df_balance_previous.drop(*[\"year_balance\", \"month_balance\", \"day_balance\", \"fecha\"])\n",
    "# df_balance_compared = df_balance.join(df_balance_previous,\n",
    "#                                             how=\"left\",\n",
    "#                                             on=( (df_balance[\"fecha\"]==df_balance_previous[\"fecha_previous\"]) &\n",
    "#                                                   (df_balance[\"msisdn_balance\"]==df_balance_previous[\"msisdn_balance_previous\"])))\n",
    "                                           \n",
    "df_balance = (df_balance.withColumnRenamed(\"saldo\", \"bal_bod\")\n",
    "                        .withColumn(\"bal_eod\", lag(col('bal_bod'),1,0).over(Window.partitionBy(\"msisdn_balance\").orderBy(asc(\"fecha\"))))\n",
    "                       .withColumn(\"bal_diff\", col(\"bal_eod\") - col(\"bal_bod\"))\n",
    "              )\n",
    "\n",
    "                                                     \n",
    "df_balance = df_balance.drop(*[\"fecha_previous\",\"fecha\", \"msisdn_balance_previous\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic (voice and sms) - add_voice_sms_traffic_information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# voice_num - Number of calls made on a particular day,\n",
    "# voice_num_distinct_rec - Number of distinct receivers,\n",
    "# voice_total_duration - Total time spent making calls on a given day,\n",
    "# voice_avg_duration - Average time spent on calls on a given day,\n",
    "# voice_amount - Amount spend on calls on a particular day,\n",
    "# sms_num - Number of SMS made on a particular day,\n",
    "# sms_num_distinct_rec - Number of distinct SMS recipients on a given day.\n",
    "# sms_amount - Amount spent on SMS on a particular day.\n",
    "\n",
    "\n",
    "df_voicesms = (spark.read.table(\"raw_es.prepaid_trafficvoicesms\")\n",
    "                        .where((col(\"dia\") >=  analysis_start_obj) &\n",
    "                               (col(\"dia\") <=  analysis_end_obj) \n",
    "                              # (col(\"day\") == 29) &\n",
    "                              #  (col(\"numeroorigen\")==\"692371698\")\n",
    "                              )\n",
    "                 .withColumn(\"numeroorigen\", trim(col(\"numeroorigen\")))\n",
    "                 .withColumn(\"numerodestino\", trim(col(\"numerodestino\")))\n",
    "                 .withColumn(\"vozsms\", lower(trim(col(\"vozsms\"))))      \n",
    "                 .withColumn(\"voice_amount\", when(lower(col(\"vozsms\")) == 'voz', col(\"importecobrado\")).otherwise(0))\n",
    "                 .withColumn(\"sms_amount\", when(lower(col(\"vozsms\")) == 'sms', col(\"importecobrado\")).otherwise(0))\n",
    "              .groupBy(\"numeroorigen\", \"year\", \"month\", \"day\")\n",
    "              .agg(sql_sum(\"voice_amount\").alias(\"voice_amount\"),\n",
    "                   sql_sum(\"sms_amount\").alias(\"sms_amount\"),\n",
    "                   sql_sum(\"importecobrado\").alias(\"voicesms_amount\"),\n",
    "                   count(when((lower(col(\"vozsms\")) == 'voz'), col(\"vozsms\")).otherwise(None)).alias(\"voice_num\"),\n",
    "                   count(when((lower(col(\"vozsms\")) == 'sms'), col(\"vozsms\")).otherwise(None)).alias(\"sms_num\"),\n",
    "                   sql_sum(when((lower(col(\"vozsms\")) == 'voz'), col(\"airduration\")).otherwise(None)).alias(\"voice_duration\"),\n",
    "                   sql_avg(when((lower(col(\"vozsms\")) == 'voz'), col(\"airduration\")).otherwise(None)).alias(\"voice_avg_duration\"),\n",
    "                   countDistinct(when((lower(col(\"vozsms\")) == 'voz'), col(\"numerodestino\")).otherwise(None)).alias(\"voice_num_distinct_rec\"),\n",
    "                   countDistinct(when((lower(col(\"vozsms\")) == 'sms'), col(\"numerodestino\")).otherwise(None)).alias(\"sms_num_distinct_rec\"))\n",
    "             .withColumnRenamed(\"year\", \"year_voicesms\")\n",
    "             .withColumnRenamed(\"month\", \"month_voicesms\")\n",
    "             .withColumnRenamed(\"day\", \"day_voicesms\")\n",
    "             .withColumnRenamed(\"numeroorigen\", \"msisdn_voicesms\")  \n",
    "               )\n",
    "\n",
    "#df_voicesms.take(10)                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_mb - total amount of MBs consumed by data sessions.\n",
    "# data_amount - total amount spent on data sessions.\n",
    "\n",
    "\n",
    "# volumen data\n",
    "df_data_consumed = (spark.read.table(\"raw_es.prepaid_trafficdata\")\n",
    "                 .where((col(\"dateofcall\") >=  analysis_start_obj) &\n",
    "                        (col(\"dateofcall\") >=  analysis_end_obj))\n",
    "                 .withColumn(\"data_mb\", col(\"volumen\")/(1024*1024))\n",
    "                 .groupBy(\"msisdn\", \"year\", \"month\", \"day\")\n",
    "                 .agg(sql_sum(\"data_mb\").alias(\"data_mb\"),\n",
    "                      sql_sum(\"CARGOREAL\").alias(\"data_amount\"),\n",
    "                      count(\"data_mb\").alias(\"num_conexions\"))\n",
    "                 .select(*[\"year\", \"month\", \"day\", \"data_mb\", \"msisdn\", \"data_amount\", \"num_conexions\"])\n",
    "                 .withColumnRenamed(\"year\", \"year_data\")\n",
    "                 .withColumnRenamed(\"month\", \"month_data\")\n",
    "                 .withColumnRenamed(\"day\", \"day_data\")\n",
    "                 .withColumnRenamed(\"msisdn\", \"msisdn_data\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mergeando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this dataframe has N prepago by M days \n",
    "df_prepago_cross_join = df_prepago.join(df_peuph_date_master,\n",
    "                                           how=\"left_outer\",\n",
    "                                           on=(#(df_prepago[\"month_prepaid\"]==df_peuph_date_master[\"month\"]) &\n",
    "                                               (df_prepago[\"year_prepaid\"]==df_peuph_date_master[\"year\"]))\n",
    "                                           )\n",
    "df_prepago_cross_join = df_prepago_cross_join.drop(*[\"month_prepaid\", \"year_prepaid\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_prepago_cross_join.where(col(\"msisdn\")==\"692689397\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_prepaid_join_1 = df_prepago_cross_join.join(df_balance_receptor,\n",
    "                                           how=\"left\",\n",
    "                                           on=((df_prepago_cross_join[\"month\"]==df_balance_receptor[\"month_receptor\"]) &\n",
    "                                               (df_prepago_cross_join[\"year\"]==df_balance_receptor[\"year_receptor\"]) &\n",
    "                                               (df_prepago_cross_join[\"msisdn\"]==df_balance_receptor[\"msisdn_receptor\"]) &\n",
    "                                               (df_prepago_cross_join[\"day\"]==df_balance_receptor[\"day_receptor\"]))\n",
    "                                              )\n",
    "df_prepaid_join_1 = df_prepaid_join_1.drop(*[\"month_receptor\", \"year_receptor\", \"day_receptor\", \"msisdn_receptor\"])\n",
    "\n",
    "df_prepaid_join_2 = df_prepaid_join_1.join(df_balance_emisor,\n",
    "                                           how=\"left\",\n",
    "                                           on=((df_prepaid_join_1[\"month\"]==df_balance_emisor[\"month_emisor\"]) &\n",
    "                                               (df_prepaid_join_1[\"year\"]==df_balance_emisor[\"year_emisor\"]) &\n",
    "                                               (df_prepaid_join_1[\"msisdn\"]==df_balance_emisor[\"msisdn_emisor\"]) &\n",
    "                                               (df_prepaid_join_1[\"day\"]==df_balance_emisor[\"day_emisor\"]))\n",
    "                                              )\n",
    "df_prepaid_join_2 = df_prepaid_join_2.drop(*[\"month_emisor\", \"year_emisor\", \"day_emisor\", \"msisdn_emisor\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Número de transferencias en las que el MSISDN ha sido receptor de saldo (tbal_rec_num).\n",
    "# Número de transferencias en las que el MSISDN ha sido el donante de saldo (tbal_tra_num).\n",
    "# Saldo recibido en transferencias (tbal_rec_amount). \n",
    "# Saldo donado en transferencias (tbal_tra_amount).\n",
    "\n",
    "df_prepaid_join_3 = (df_prepaid_join_2\n",
    "    .withColumn(\"tbal_rec_amount\", 1.21*col(\"importe_traspasado_receptor\"))\n",
    "    .withColumnRenamed(\"num_rec\", \"tbal_rec_num\")\n",
    "    .withColumn(\"tbal_tra_amount\", 1.21*col(\"importe_traspasado_emisor\") + 1.21*col(\"importe_cargo_emisor\"))\n",
    "    .withColumnRenamed(\"num_em\", \"tbal_tra_num\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_prepaid_join_4 = df_prepaid_join_3.join(df_topups,\n",
    "                                           how=\"left\",\n",
    "                                           on=((df_prepaid_join_3[\"month\"]==df_topups[\"month_topups\"]) &\n",
    "                                               (df_prepaid_join_3[\"year\"]==df_topups[\"year_topups\"]) &\n",
    "                                               (df_prepaid_join_3[\"msisdn\"]==df_topups[\"ndc_msisdn\"]) &\n",
    "                                               (df_prepaid_join_3[\"day\"]==df_topups[\"day_topups\"]))\n",
    "                                              )\n",
    "df_prepaid_join_4 = df_prepaid_join_4.drop(*[\"year_topups\", \"month_topups\", \"day_topups\", \"ndc_msisdn\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_prepaid_join_4.where(col(\"msisdn\")==\"611068129\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_prepaid_join_5 = df_prepaid_join_4.join(df_voicesms,\n",
    "                                           how=\"left\",\n",
    "                                           on=((df_prepaid_join_4[\"month\"]==df_voicesms[\"month_voicesms\"]) &\n",
    "                                               (df_prepaid_join_4[\"year\"]==df_voicesms[\"year_voicesms\"]) &\n",
    "                                               (df_prepaid_join_4[\"msisdn\"]==df_voicesms[\"msisdn_voicesms\"]) &\n",
    "                                               (df_prepaid_join_4[\"day\"]==df_voicesms[\"day_voicesms\"]))\n",
    "                                              )\n",
    "df_prepaid_join_5 = df_prepaid_join_5.drop(*[\"year_voicesms\", \"month_voicesms\", \"day_voicesms\", \"msisdn_voicesms\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_prepaid_join_5.where(col(\"msisdn\")==\"611068129\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_prepaid_join_6 = df_prepaid_join_5.join(df_data_consumed,\n",
    "                                           how=\"left\",\n",
    "                                           on=((df_prepaid_join_5[\"month\"]==df_data_consumed[\"month_data\"]) &\n",
    "                                               (df_prepaid_join_5[\"year\"]==df_data_consumed[\"year_data\"]) &\n",
    "                                               (df_prepaid_join_5[\"msisdn\"]==df_data_consumed[\"msisdn_data\"]) &\n",
    "                                               (df_prepaid_join_5[\"day\"]==df_data_consumed[\"day_data\"]))\n",
    "                                              )\n",
    "df_prepaid_join_6 = df_prepaid_join_6.drop(*[\"year_data\", \"month_data\", \"day_data\", \"msisdn_data\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_prepaid_join_7 = df_prepaid_join_6.join(df_balance,\n",
    "                                           how=\"left\",\n",
    "                                           on=((df_prepaid_join_6[\"month\"]==df_balance[\"month_balance\"]) &\n",
    "                                               (df_prepaid_join_6[\"year\"]==df_balance[\"year_balance\"]) &\n",
    "                                               (df_prepaid_join_6[\"msisdn\"]==df_balance[\"msisdn_balance\"]) &\n",
    "                                               (df_prepaid_join_6[\"day\"]==df_balance[\"day_balance\"]))\n",
    "                                              )\n",
    "df_prepaid_join_7 = df_prepaid_join_7.drop(*[\"year_balance\", \"month_balance\", \"day_balance\", \"msisdn_balance\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_prepaid_join_7.where(col(\"msisdn\")==\"611068129\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_prepaid_join_7_1 = df_prepaid_join_7.join(df_advance_solicitado,\n",
    "                                           how=\"left\",\n",
    "                                           on=((df_prepaid_join_7[\"month\"]==df_advance_solicitado[\"month_advance\"]) &\n",
    "                                               (df_prepaid_join_7[\"year\"]==df_advance_solicitado[\"year_advance\"]) &\n",
    "                                               (df_prepaid_join_7[\"msisdn\"]==df_advance_solicitado[\"msisdn_advance\"]) &\n",
    "                                               (df_prepaid_join_7[\"day\"]==df_advance_solicitado[\"day_advance\"]))\n",
    "                                              )\n",
    "\n",
    "df_prepaid_join_7_1 = df_prepaid_join_7_1.drop(*[\"year_advance\", \"month_advance\", \"day_advance\", \"msisdn_advance\"])\n",
    "\n",
    "\n",
    "df_mega = df_prepaid_join_7_1.join(df_advance_recuperado,\n",
    "                                           how=\"left\",\n",
    "                                           on=((df_prepaid_join_7_1[\"month\"]==df_advance_recuperado[\"month_rec\"]) &\n",
    "                                               (df_prepaid_join_7_1[\"year\"]==df_advance_recuperado[\"year_rec\"]) &\n",
    "                                               (df_prepaid_join_7_1[\"msisdn\"]==df_advance_recuperado[\"msisdn_rec\"]) &\n",
    "                                               (df_prepaid_join_7_1[\"day\"]==df_advance_recuperado[\"day_rec\"]))\n",
    "                                              )\n",
    "\n",
    "df_mega = df_mega.drop(*[\"year_rec\", \"month_rec\", \"day_rec\", \"msisdn_rec\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SAVE DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20180721-233857 [INFO ] df_mega saved as table 'tests_es.csanc109_prepaid_mega_20180201_20180531_complete_20180721_230257' -- 2159.58803892 secs\n"
     ]
    }
   ],
   "source": [
    "if save_table:\n",
    "    \n",
    "    start_time = time.time()\n",
    "    right_now = dt.datetime.now()\n",
    "    formatted_datenow = right_now.strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "    table_df_mega = \"tests_es.csanc109_prepaid_mega_{}_{}_{}_{}\".format(ANALYSIS_PERIOD_START,\n",
    "                                                                ANALYSIS_PERIOD_END,\n",
    "                                                                num_samples if debug_mode else \"complete\",\n",
    "                                                                formatted_datenow)\n",
    "\n",
    "    (df_mega.where(col('msisdn').isNotNull())                           \n",
    "                                 .write\n",
    "                                 .format('parquet')\n",
    "                                 .mode('overwrite')\n",
    "                                 .saveAsTable(table_df_mega))\n",
    "\n",
    "    logger.info(\"df_mega saved as table '{}' -- {} secs\".format(table_df_mega, time.time()-start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-183-fa3f6d411be7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
