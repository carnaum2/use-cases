{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#spark.stop()\n",
    "APP_NAME = \"Prepaid Segmentation Analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ended spark session: 35.3114328384 secs | default parallelism=2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    import sys, os\n",
    "    HOME_SRC = os.path.join(os.environ.get('BDA_USER_HOME', ''), \"src\")\n",
    "    if HOME_SRC not in sys.path:\n",
    "        sys.path.append(HOME_SRC)\n",
    "\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    import time\n",
    "    import src.pycharm_workspace.lib_csanc109.utils.pyspark_configuration as pyspark_config\n",
    "\n",
    "    start_time = time.time()\n",
    "    sc, spark, sql_context = pyspark_config.get_spark_session(app_name=APP_NAME, log_level=\"OFF\")\n",
    "    print(\"Ended spark session: {} secs | default parallelism={}\".format(time.time()-start_time, sc.defaultParallelism))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20180730-093939 [INFO ] Logging to file /var/SP/data/home/csanc109/logging/Prepaid_Segmentation_Analysis_20180730_093939.log\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/\n",
    "\n",
    "import logging\n",
    "import os, sys\n",
    "import datetime as dt\n",
    "import src.pycharm_workspace.lib_csanc109.utils.custom_logger as clogger\n",
    "\n",
    "USE_CASES_PATH = os.path.join(HOME_SRC, \"pycharm_workspace\", \"use-cases\")\n",
    "if USE_CASES_PATH not in sys.path:\n",
    "    sys.path.append(USE_CASES_PATH)\n",
    "\n",
    "WHOAMI= os.getenv(\"USER\")\n",
    "    \n",
    "    \n",
    "# Create logger object (messages to stdout and file)\n",
    "logging_file = os.path.join(os.environ.get('BDA_USER_HOME', ''),\"logging\",APP_NAME.replace(\" \",\"_\")+\"_\"+dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\")+\".log\")\n",
    "logger = clogger.get_custom_logger(logging_file, std_channel=sys.stderr)\n",
    "logger.info(\"Logging to file {}\".format(logging_file))\n",
    "                      \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20180730-093939 [INFO ] #######################################################################\n",
      "20180730-093939 [INFO ] ##### EJECUTING PREPAID SEGMENTATION GRAPHS - 2018-07-30 09:39:39 #####\n",
      "20180730-093939 [INFO ] #######################################################################\n",
      "20180730-093939 [INFO ] Plots will be saved in '/var/SP/data/home/csanc109/data/plots/PrepaidSegmentationAnalysis'\n"
     ]
    }
   ],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql.functions import substring\n",
    "from pyspark.sql.functions import (unix_timestamp, udf,col,max as sql_max, stddev as sql_stddev, when, count, isnull, concat, lpad, trim, lit, sum as sql_sum, length, upper)\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import asc, datediff, lag\n",
    "import datetime as dt\n",
    "import Prepaid_Segmentation.src.graphs\n",
    "from Prepaid_Segmentation.src.graphs import GraphGeneration\n",
    "import Prepaid_Segmentation.src.prepaid_mega_table\n",
    "from Prepaid_Segmentation.src.prepaid_mega_table import MegaTable\n",
    "from Prepaid_Segmentation.src.prepaid_mega_agg_table import MegaAggTable\n",
    "from pyspark.sql.functions import asc, datediff, lag, min as sql_min, max as sql_max, avg as sql_avg\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime\n",
    "# Set up logs.\n",
    "right_now = dt.datetime.now()\n",
    "\n",
    "\n",
    "ANALYSIS_PERIOD_START = \"20180201\"\n",
    "ANALYSIS_PERIOD_END   = \"20180531\"\n",
    "FORMATTED_TIME =  \"20180721_230257\"\n",
    "\n",
    "analysis_start_obj = dt.datetime.strptime(ANALYSIS_PERIOD_START, \"%Y%m%d\")\n",
    "analysis_end_obj = dt.datetime.strptime(ANALYSIS_PERIOD_END, \"%Y%m%d\")\n",
    "save_table = False\n",
    "\n",
    "reference_month = ANALYSIS_PERIOD_END[:6]\n",
    "\n",
    "print_string = '##### EJECUTING PREPAID SEGMENTATION GRAPHS - {right_now} #####'\\\n",
    "    .format(right_now=right_now.strftime('%Y-%m-%d %X'))\n",
    "logger.info('#'*len(print_string))\n",
    "logger.info(print_string)\n",
    "logger.info('#'*len(print_string))\n",
    "\n",
    "num_samples = 100\n",
    "debug_mode = False\n",
    "LOAD_TABLE_MEGA_AGG = False\n",
    "\n",
    "MegaTableObj = MegaTable(spark)\n",
    "\n",
    "GraphGenerationObj = GraphGeneration(os.path.join(os.environ.get('BDA_USER_HOME', ''), \n",
    "                                                               \"data\", \n",
    "                                                               \"plots\", \n",
    "                                                               APP_NAME.replace(\" \",\"\")))           \n",
    "                                                            \n",
    "                              \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Load Mega Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20180730-093942 [INFO ] Reading table tests_es.csanc109_prepaid_mega_20180201_20180531_complete_20180721_230257\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if LOAD_TABLE_MEGA_AGG == False:\n",
    "\n",
    "    table_df_mega = \"tests_es.csanc109_prepaid_mega_{}_{}_{}_{}\".format(ANALYSIS_PERIOD_START,\n",
    "                                                                        ANALYSIS_PERIOD_END,\n",
    "                                                                        num_samples if debug_mode else \"complete\",\n",
    "                                                                        FORMATTED_TIME)\n",
    "    df_mega = MegaTableObj.load_mega_table(table_df_mega)\n",
    "    MegaAggTableObj = MegaAggTable(spark, df_mega)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if LOAD_TABLE_MEGA_AGG == False:\n",
    "    df_agg_topups  = MegaAggTableObj.agg_topup_info()\n",
    "    df_agg_balance = MegaAggTableObj.agg_balance_info()\n",
    "    df_agg_traffic = MegaAggTableObj.agg_traffic_info()\n",
    "    df_agg_tariffs = MegaAggTableObj.agg_tariffs_info(analysis_start_obj, analysis_end_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "table_speed_bucket = \"tests_es.csanc109_SPEEDBUCKETS_PRE\"\n",
    "table_bonos        = \"tests_es.csanc109_BONOS_PRE\"\n",
    "\n",
    "\n",
    "if LOAD_TABLE_MEGA_AGG == False:\n",
    "\n",
    "    df_tariffs_params = pd.read_csv(os.path.join(USE_CASES_PATH, \"Prepaid_Segmentation\", \"resources\", \"prepaid_tariffs_params.csv\"),\n",
    "                                    sep=\";\", decimal=\".\")\n",
    "\n",
    "    df_speedbucket = pd.read_csv(os.path.join(USE_CASES_PATH, \"Prepaid_Segmentation\", \"resources\", \"SPEEDBUCKETS_PRE_20180201_TODATE.TXT\"),\n",
    "                                        sep=\"|\", decimal=\".\", parse_dates=[1, 2], infer_datetime_format=True)\n",
    "    #df_speedbucket = (self.spark.read.table(table_speed_bucket))\n",
    "\n",
    "\n",
    "    df_bonos = pd.read_csv(os.path.join(USE_CASES_PATH, \"Prepaid_Segmentation\", \"resources\", \"BONOS_PRE_20180201_TODATE.TXT\"),\n",
    "                                         sep=\"|\", decimal=\".\", parse_dates=[1, 2, 8], infer_datetime_format=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #df_bonos = (self.spark.read.table(table_bonos))\n",
    "\n",
    "#     df_bonos_agg = df_bonos.groupby(\"MSISDN\")[\"BONO\"].agg(\n",
    "#         {\n",
    "#         'BONO': [\n",
    "#             ('diff_bonos', lambda gg: gg.nunique()),\n",
    "#             ('total_bonos', lambda gg: gg.count())\n",
    "#                  ]\n",
    "#         }\n",
    "#     )    \n",
    "\n",
    "\n",
    "#     df_speed_agg = df_speed.groupby(\"MSISDN\")[\"Codigo_SpeedBucket\"].agg(\n",
    "#         {\n",
    "#         'Codigo_SpeedBucket': [\n",
    "#             ('diff_speed', lambda gg: gg.nunique()),\n",
    "#             ('total_speed', lambda gg: gg.count())\n",
    "#                  ]\n",
    "#         }\n",
    "#     )     \n",
    "    \n",
    "#     df_speed_agg.columns = df_speed_agg.columns.droplevel()\n",
    "#     df_bonos_agg.columns = df_bonos_agg.columns.droplevel()\n",
    "\n",
    "#     df_bonos_agg.reset_index(inplace=True)\n",
    "#     df_speed_agg.reset_index(inplace=True)\n",
    "\n",
    "#     df_bonos_agg.rename(columns={\"MSISDN\" : \"msisdn_bonos\"}, inplace=True)\n",
    "#     df_speed_agg.rename(columns={\"MSISDN\" : \"msisdn_speed\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table_speed_bucket = \"tests_es.csanc109_SPEEDBUCKETS_PRE_20180201_20180725\"\n",
    "table_bonos        = \"tests_es.csanc109_BONOS_PRE_20180201_20180725\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2018-06-12 23:59:59\n",
       "1                         NaN\n",
       "2         2018-04-23 23:59:59\n",
       "3         2018-07-05 23:59:59\n",
       "4         2018-03-19 23:59:59\n",
       "5         2018-06-28 23:59:59\n",
       "6                         NaN\n",
       "7                         NaN\n",
       "8                         NaN\n",
       "9         2018-06-14 23:59:59\n",
       "10        2018-04-16 23:59:59\n",
       "11        2018-03-15 23:59:59\n",
       "12        2018-05-16 23:59:59\n",
       "13        2018-02-12 23:59:59\n",
       "14                        NaN\n",
       "15        2018-02-18 23:59:59\n",
       "16        2018-07-13 23:59:59\n",
       "17        2018-05-16 23:59:59\n",
       "18        2018-03-19 23:59:59\n",
       "19        2018-06-14 23:59:59\n",
       "20        2018-04-17 23:59:59\n",
       "21                        NaN\n",
       "22                        NaN\n",
       "23        2018-03-17 23:59:59\n",
       "24                        NaN\n",
       "25        2018-07-09 23:59:59\n",
       "26                        NaN\n",
       "27        2018-02-10 23:59:59\n",
       "28        2018-04-06 23:59:59\n",
       "29        2018-07-15 23:59:59\n",
       "                 ...         \n",
       "886820    2018-03-08 23:59:59\n",
       "886821                    NaN\n",
       "886822    2018-06-29 23:59:59\n",
       "886823                    NaN\n",
       "886824    2018-04-23 23:59:59\n",
       "886825    2018-02-20 23:59:59\n",
       "886826    2018-06-09 23:59:59\n",
       "886827    2018-03-22 23:59:59\n",
       "886828                    NaN\n",
       "886829                    NaN\n",
       "886830                    NaN\n",
       "886831                    NaN\n",
       "886832                    NaN\n",
       "886833                    NaN\n",
       "886834                    NaN\n",
       "886835    2018-05-27 23:59:59\n",
       "886836                    NaN\n",
       "886837                    NaN\n",
       "886838                    NaN\n",
       "886839    2018-02-03 23:59:59\n",
       "886840    2018-02-08 23:59:59\n",
       "886841    2018-06-07 23:59:59\n",
       "886842    2018-07-07 23:59:59\n",
       "886843    2018-02-14 23:59:59\n",
       "886844    2018-05-08 23:59:59\n",
       "886845    2018-03-26 23:59:59\n",
       "886846                    NaN\n",
       "886847                    NaN\n",
       "886848                    NaN\n",
       "886849                    NaN\n",
       "Name: Fecha_Deactivation, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_to_python_datetime(df):\n",
    "    df_copy = df.copy()\n",
    "    for column_name, column in df_copy.iteritems():\n",
    "        if column.dtype.kind == 'M':\n",
    "            print(column)\n",
    "            df_copy[column_name] = pd.Series(column.dt.to_pydatetime(), dtype=object)\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "df_bonos2 = convert_to_python_datetime(df_bonos)\n",
    "df_bonos2[\"Fecha_Deactivation\"]\n",
    "# df_speedbucket2 = convert_to_python_datetime(df_speedbucket)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #df_bonos2[\"Fecha_Inicio\"] = df_bonos2[\"Fecha_Inicio\"].apply(lambda dd: None if type(dd) == pd.tslib.NaTType else dd)\n",
    "# #df_bonos2[\"Fecha_Deactivation\"].apply(lambda dd: type(dd) == pd.tslib.NaTType)\n",
    "\n",
    "\n",
    "# df_bonos2[\"Fecha_Deactivation\"] = df_bonos2[\"Fecha_Deactivation\"].apply(lambda dd: None if type(dd) == pd.tslib.NaTType else dd)\n",
    "# # df_bonos2  \n",
    "# df_bonos2[\"Fecha_Deactivation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TimestampType can not accept object '2018-05-27 12:40:29' in type <type 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-7f800dca9c8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mbonosSchema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStructType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MSISDN\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;33m,\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fecha_Inicio\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTimestampType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;33m,\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fecha_Fin\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTimestampType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;33m,\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Status\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;33m,\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"BONO\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;33m,\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Precio\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDoubleType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;33m,\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Canal\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;33m,\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Volumen\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDoubleType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;33m,\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fecha_Deactivation\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTimestampType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;33m,\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TACADA\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mdf_bonos_pyspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_bonos2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbonosSchema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mspeedSchema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStructType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MSISDN\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;33m,\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fx_Inicio\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTimestampType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;33m,\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fx_Fin\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTimestampType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;33m,\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Estado_SB\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;33m,\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Codigo_SpeedBucket\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;33m,\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cuota\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDoubleType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;33m,\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Canal_de_Activacion\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;33m,\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Volumen\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDoubleType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;33m,\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TACADA\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m                        \u001b[1;33m,\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Volumen_restante\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDoubleType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m                       \u001b[1;33m,\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Tipo_de_cliente\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIntegerType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36m_createFromLocal\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;31m# make sure data could consumed multiple times\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/session.pyc\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    507\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mStructType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m                 \u001b[0mverify_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    510\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/types.pyc\u001b[0m in \u001b[0;36m_verify_type\u001b[1;34m(obj, dataType, nullable)\u001b[0m\n\u001b[0;32m   1358\u001b[0m                                  \"length of fields (%d)\" % (len(obj), len(dataType.fields)))\n\u001b[0;32m   1359\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataType\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1360\u001b[1;33m                 \u001b[0m_verify_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnullable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1361\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__dict__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1362\u001b[0m             \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/cloudera/parcels/SPARK2/lib/spark2/python/pyspark/sql/types.pyc\u001b[0m in \u001b[0;36m_verify_type\u001b[1;34m(obj, dataType, nullable)\u001b[0m\n\u001b[0;32m   1322\u001b[0m         \u001b[1;31m# subclass of them can not be fromInternal in JVM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_acceptable_types\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1324\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s can not accept object %r in type %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdataType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mByteType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: TimestampType can not accept object '2018-05-27 12:40:29' in type <type 'str'>"
     ]
    }
   ],
   "source": [
    "#### TEST https://stackoverflow.com/questions/42584369/timestamp-roundtrip-from-spark-python-to-pandas-and-back\n",
    "\n",
    "from pyspark.sql.types import StringType, TimestampType, IntegerType, DoubleType, StructType, StructField, DateType\n",
    "\n",
    "\n",
    "table_speed_bucket = \"tests_es.csanc109_SPEEDBUCKETS_PRE\"\n",
    "table_bonos        = \"tests_es.csanc109_BONOS_PRE\"\n",
    "\n",
    "bonosSchema = StructType([ StructField(\"MSISDN\", StringType(), True)\\\n",
    "                       ,StructField(\"Fecha_Inicio\", TimestampType(), True)\\\n",
    "                       ,StructField(\"Fecha_Fin\", TimestampType(), True)\\\n",
    "                       ,StructField(\"Status\", StringType(), True)\\\n",
    "                       ,StructField(\"BONO\", StringType(), True)\\\n",
    "                       ,StructField(\"Precio\", DoubleType(), True)\\\n",
    "                       ,StructField(\"Canal\", StringType(), True)\\\n",
    "                       ,StructField(\"Volumen\", DoubleType(), True)\\\n",
    "                       ,StructField(\"Fecha_Deactivation\", TimestampType(), True)\\\n",
    "                       ,StructField(\"TACADA\", StringType(), True)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_bonos_pyspark = spark.createDataFrame(df_bonos2)\n",
    "\n",
    "\n",
    "'''\n",
    "speedSchema = StructType([ StructField(\"MSISDN\", StringType(), True)\\\n",
    "                       ,StructField(\"Fx_Inicio\", TimestampType(), True)\\\n",
    "                       ,StructField(\"Fx_Fin\", TimestampType(), True)\\\n",
    "                       ,StructField(\"Estado_SB\", StringType(), True)\\\n",
    "                       ,StructField(\"Codigo_SpeedBucket\", StringType(), True)\\\n",
    "                       ,StructField(\"Cuota\", DoubleType(), True)\\\n",
    "                       ,StructField(\"Canal_de_Activacion\", StringType(), True)\\\n",
    "                       ,StructField(\"Volumen\", DoubleType(), True)\\\n",
    "                       ,StructField(\"TACADA\", StringType(), True) \\\n",
    "                       ,StructField(\"Volumen_restante\", DoubleType(), True)\\\n",
    "                       ,StructField(\"Tipo_de_cliente\", IntegerType(), True)])\n",
    "\n",
    "df_speed_pyspark = spark.createDataFrame(df_speedbucket, speedSchema)\n",
    "'''\n",
    "(df_bonos_pyspark.where(col('MSISDN').isNotNull())                           \n",
    "                             .write\n",
    "                             .format('parquet')\n",
    "                             .mode('overwrite')\n",
    "                             .saveAsTable(table_bonos))\n",
    "'''\n",
    "(df_speed_pyspark.where(col('MSISDN').isNotNull())                           \n",
    "                             .write\n",
    "                             .format('parquet')\n",
    "                             .mode('overwrite')\n",
    "                             .saveAsTable(table_speed_bucket))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_bonos[\"Fecha_Inicio\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if LOAD_TABLE_MEGA_AGG == False:\n",
    "\n",
    "    df_prepago_estado = (spark.read.table(\"raw_es.vf_pre_ac_final\") # se calcula a cierre de mes, en concreto el dia 2 del mes sgte\n",
    "                  .where(col(\"estado_servicio\").isin(['CA', 'FCA', 'AC', 'FEX']))\n",
    "                  .where((col(\"year\") == int(reference_month[:4]))\n",
    "                       & (col(\"month\") == int(reference_month[4:])))\n",
    "                  .select([\"msisdn\", \"estado_servicio\", \"codigo_plan_precios\"]))\n",
    "\n",
    "\n",
    "    df_prepago_tariffs = (df_prepago_estado.join(df_agg_tariffs,\n",
    "                                               how=\"left\",\n",
    "                                               on=(df_prepago_estado[\"msisdn\"]==df_agg_tariffs[\"msisdn_tariff\"])))\n",
    "\n",
    "    df_prepago_tariffs = df_prepago_tariffs.drop(*[\"msisdn\"])\n",
    "\n",
    "\n",
    "    df_mega_agg = (df_prepago_tariffs         \n",
    "                           .join(df_agg_topups,\n",
    "                                 how=\"left\",\n",
    "                                 on=(df_prepago_tariffs[\"msisdn_tariff\"]==df_agg_topups[\"msisdn_topups\"])))\n",
    "\n",
    "\n",
    "    df_mega_agg = ((df_mega_agg.join(df_agg_traffic,\n",
    "                                               how=\"left\",\n",
    "                                               on=(df_mega_agg[\"msisdn_tariff\"]==df_agg_traffic[\"msisdn_traffic\"])))\n",
    "                           .join(df_agg_balance,\n",
    "                                 how=\"left\",\n",
    "                                 on=(df_mega_agg[\"msisdn_tariff\"]==df_agg_balance[\"msisdn_balance\"]))\n",
    "                           .withColumnRenamed(\"msisdn_traffic\", \"msisdn\")) # renamed one of them\n",
    "\n",
    "    df_mega_agg = (df_mega_agg.join(df_speed_agg,\n",
    "                                               how=\"left\",\n",
    "                                               on=(df_mega_agg[\"msisdn_tariff\"]==df_speed_agg[\"msisdn_speed\"])))\n",
    "\n",
    "    df_mega_agg = (df_mega_agg.join(df_bonos_agg,\n",
    "                                               how=\"left\",\n",
    "                                               on=(df_mega_agg[\"msisdn_tariff\"]==df_bonos_agg[\"msisdn_bonos\"])))\n",
    "\n",
    "    df_mega_agg = df_mega_agg.drop(*[\"msisdn_topups\", \"msisdn_balance\", \"msisdn_tariff\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Aggregated Mega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not LOAD_TABLE_MEGA_AGG and save_table:\n",
    "\n",
    "    right_now = datetime.now()\n",
    "    formatted_datenow = right_now.strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    table_df_mega_agg = \"tests_es.csanc109_prepaid_mega_agg_{}_{}_{}_{}\".format(ANALYSIS_PERIOD_START,\n",
    "                                                          ANALYSIS_PERIOD_END,\n",
    "                                                          num_samples if debug_mode else \"complete\",\n",
    "                                                                               formatted_datenow)\n",
    "\n",
    "    (df_mega_agg.where(col('msisdn').isNotNull())                           \n",
    "                                 .write\n",
    "                                 .format('parquet')\n",
    "                                 .mode('overwrite')\n",
    "                                 .saveAsTable(table_df_mega_agg))\n",
    "\n",
    "    logger.info(\"df_mega_agg saved as table '{}'\".format(table_df_mega_agg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if LOAD_TABLE_MEGA_AGG:\n",
    "    table_df_mega_agg = \"tests_es.csanc109_prepaid_mega_agg_{}_{}_{}_{}\".format(ANALYSIS_PERIOD_START,\n",
    "                                                                        ANALYSIS_PERIOD_END,\n",
    "                                                                        num_samples if debug_mode else \"complete\",\n",
    "                                                                        FORMATTED_TIME)\n",
    "    df_mega_agg = MegaAggTableObj.load_mega_agg_table(table_df_mega_agg)\n",
    "    logger.info(\"df_mega_agg loaded '{}'\".format(table_df_mega_agg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas time! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "df_mega_agg_pandas = df_mega_agg.toPandas()\n",
    "logger.info(\"Elapsed time {} secs\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting lines by PlanPrecios, type and estado_servicio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_mega_agg_activelines = df_mega_agg_pandas[(df_mega_agg_pandas[\"type\"] ==\"AC\")].copy()\n",
    "df_mega_agg_pandas_tariffs = pd.merge(df_mega_agg_pandas, df_tariffs_params, left_on=\"codigo_plan_precios\", right_on=\"PlanPrecios\")\n",
    "df_tarifas_count = pd.DataFrame(df_mega_agg_pandas_tariffs.groupby([\"codigo_plan_precios\", \"type\", \"estado_servicio\"], as_index=False)[\"msisdn\"].count())\n",
    "df_tarifas_count.rename(columns={\"msisdn\" : \"num_lines\"}, inplace=True)\n",
    "df_tariffs_params_count = pd.merge(df_tarifas_count,df_tariffs_params, left_on=\"codigo_plan_precios\", right_on=\"PlanPrecios\").sort_values(by=\"num_lines\", ascending=False)\n",
    "df_tariffs_params_count[[\"codigo_plan_precios\", \"AddOn\", \"type\", \"estado_servicio\", \"num_lines\"]]\n",
    "\n",
    "df_tarifa_id = pd.DataFrame({\"total_lines\" : df_tariffs_params_count[[\"codigo_plan_precios\", \"AddOn\", \"num_lines\"]].groupby([\"codigo_plan_precios\", \"AddOn\"])[\"num_lines\"].sum()})\n",
    "df_tarifa_id.sort_values(by=\"total_lines\", ascending=False, inplace=True)\n",
    "df_tarifa_id.reset_index(inplace=True, drop=False)\n",
    "df_tarifa_id[\"%\"] = 100.0*df_tarifa_id[\"total_lines\"]/df_tarifa_id[\"total_lines\"].sum()\n",
    "df_tarifa_id[\"cumsum_%\"] = np.cumsum(df_tarifa_id[\"%\"])\n",
    "df_tarifa_id[\"label\"] = df_tarifa_id.index+1\n",
    "df_tarifa_id[\"label_shorted\"] = df_tarifa_id.apply(lambda row: str(row[\"label\"]) if row[\"cumsum_%\"]<93 else \"Other\", axis=1)\n",
    "\n",
    "\n",
    "#\"Estado del servicio. Puede tomar los valores AC ,FCA ,CA o FEX .\n",
    "#El ciclo de vida en prepago es:\n",
    "#AC --> 9 Meses desde última recarga --> FCA (estado transitorio 1 o 2 días)\n",
    "#   --> CA (sólo puede recibir llamadas y 1 mes) --> FEX (Estado transitorio 1 o 2 días) \n",
    "#   --> EX (Desactivado)\"\n",
    "\n",
    "df_mega_agg_pandas_tariffs = pd.merge(df_mega_agg_pandas_tariffs, df_tarifa_id[[\"codigo_plan_precios\", \"label_shorted\"]], left_on=\"codigo_plan_precios\", right_on=\"codigo_plan_precios\")\n",
    "df_mega_agg_pandas_tariffs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param2 = \"tu_avg_diff_days\" # tiempo medio entre recargas\n",
    "param4 = \"sum_tu_num\" # numero de recargas\n",
    "param6 = \"bal_avg\" # saldo medio durante el periodo\n",
    "param9 = \"amount_sum\"\n",
    "param5 = \"sum_tu_amount\" # cantidad recargada\n",
    "\n",
    "param3 = \"bal_avg_tu_day\" # saldo el día de la recarga\n",
    "param7 = \"voicesms_amount_sum\" # cantidad de dinero gastada\n",
    "param8 = \"num_accesos_sum\"\n",
    "param1 = \"data_amount_sum\" # cantidad de datos\n",
    "\n",
    "param10  = \"Cuota\"\n",
    "\n",
    "# features = [param2, param4, param5]\n",
    "features = [param6, param4, param7]\n",
    "\n",
    "# RECARGAS PROFILE\n",
    "features = [param2, param5, param3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_outliers(df_used_lines, remove_rows=False, cols=None, remove_aux_cols=True):\n",
    "\n",
    "    # REMOVING OUTLIERS\n",
    "    df_data_filtered = df_used_lines.copy()\n",
    "    \n",
    "    if not cols:\n",
    "        cols = df_used_lines.columns\n",
    "        \n",
    "    skipped_cols=[]\n",
    "    for col in cols:\n",
    "        try:\n",
    "            Q1 = df_data_filtered[col].quantile(0.25)\n",
    "            Q3 = df_data_filtered[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            print(col, Q1-1.5*IQR,Q3+1.5*IQR)\n",
    "            # Filtering Values between Q1-1.5IQR and Q3+1.5IQR\n",
    "            df_data_filtered[\"is_outlier_{}\".format(col)] = ~df_data_filtered[col].between(Q1-1.5*IQR, \n",
    "                                                                                           Q3+1.5*IQR, \n",
    "                                                                                           inclusive=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            logger.warning(\"Skipping computation of outliers of col={}\".format(col))\n",
    "            skipped_cols.append(col)\n",
    "            \n",
    "    df_data_filtered['outlier'] = False\n",
    "\n",
    "    for col in cols:\n",
    "        if col in skipped_cols:\n",
    "            continue\n",
    "        df_data_filtered.loc[df_data_filtered[\"is_outlier_{}\".format(col)]==True, 'outlier'] = True\n",
    "        logger.info(\"Col={} had {} outliers\".format(col, len(df_data_filtered[df_data_filtered[\"is_outlier_{}\".format(col)]==True])))\n",
    "        \n",
    "        '''\n",
    "        plt.figure(figsize=(10,8))\n",
    "        plt.subplot(211)\n",
    "        plt.xlim(df_data_filtered[col].min(), df_data_filtered[col].max()*1.1)\n",
    "        plt.axvline(x=Q1-1.5*IQR, color='r')\n",
    "        plt.axvline(x=Q3+1.5*IQR, color='r')\n",
    "        ax = df_data_filtered[col].plot(kind='kde')\n",
    "\n",
    "        plt.subplot(212)\n",
    "        plt.xlim(df_data_filtered[col].min(), df_data_filtered[col].max()*1.1)\n",
    "        sns.boxplot(x=df_data_filtered[col])\n",
    "        plt.axvline(x=Q1-1.5*IQR, color='r')\n",
    "        plt.axvline(x=Q3+1.5*IQR, color='r')\n",
    "        '''\n",
    "\n",
    "    before_rows=len(df_data_filtered)\n",
    "    #df_used_lines.loc[df_data_filtered[\"outlier\"]==True, 'type'] = \"used_line_heavy\"\n",
    "    \n",
    "    if remove_rows:\n",
    "        df_data_filtered = df_data_filtered[~df_data_filtered[\"outlier\"]]\n",
    "        logger.info(\"df_data_filtered had {} outliers\".format(before_rows-len(df_data_filtered)))\n",
    "        if remove_aux_cols==True:\n",
    "            df_data_filtered = df_data_filtered[df_used_lines.columns]\n",
    "\n",
    "    return df_data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import src.pycharm_workspace.lib_csanc109.utils.segmentation as segmentation\n",
    "\n",
    "n_sampling = 150000\n",
    "\n",
    "logger.info(\"before={} removing sum_tu_num>1\".format(len(df_mega_agg_pandas_tariffs)))\n",
    "df_mega_agg_pandas_tariffs = df_mega_agg_pandas_tariffs[df_mega_agg_pandas_tariffs[\"sum_tu_num\"]>1]\n",
    "#df_mega_agg_pandas = df_mega_agg_pandas[df_mega_agg_pandas[\"sum_tu_num\"]<500]\n",
    "logger.info(\"after={} removing sum_tu_num>1\".format(len(df_mega_agg_pandas_tariffs)))\n",
    "\n",
    "df_mega_agg_pandas_filtered = df_mega_agg_pandas_tariffs[features+[\"msisdn\", \"label_shorted\"]].copy()\n",
    "df_mega_agg_pandas_filtered.fillna(0, inplace=True)\n",
    "df_mega_agg_pandas_clean = detect_outliers(df_mega_agg_pandas_filtered, remove_rows=True, cols=[\"sum_tu_amount\"], remove_aux_cols=True)\n",
    "\n",
    "logger.info(\"before={} after={}\".format(len(df_mega_agg_pandas_filtered),len(df_mega_agg_pandas_clean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_clusters_list = [2,3,4,5,6,7,8]\n",
    "centroides_list, clusters_labels_list, inertia_list, scaler = segmentation.loop_segmentation_kmeans(df_mega_agg_pandas_clean, \n",
    "                                                                                                    n_clusters_list, \n",
    "                                                                                                    features, \n",
    "                                                                                                    max_iter=1000, \n",
    "                                                                                                    show_plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segmentation.plot_elbow(n_clusters_list, inertia_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N_CLUSTERS = 4\n",
    "\n",
    "X_unnormal=np.array(df_mega_agg_pandas_clean[features])\n",
    "cluster_labels = clusters_labels_list[n_clusters_list.index(N_CLUSTERS)]\n",
    "cluster_centroides = centroides_list[n_clusters_list.index(N_CLUSTERS)]\n",
    "\n",
    "\n",
    "\n",
    "df_centroides_unnormaliz=segmentation.compute_realspace_centroids(cluster_centroides,\n",
    "                                                     cluster_labels, # labels  got from kmeans\n",
    "                                                     features, # features passed to kmeans\n",
    "                                                     scaler) # scaler object got from kmeans\n",
    "df_centroides_unnormaliz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centers_realspace=np.array(df_centroides_unnormaliz.T) # N_CLUSTERS * N_FEATURES\n",
    "\n",
    "segmentation.plot_2D_segm_combinations(cluster_labels, features, X_unnormal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segmentation.plot_3D_segm_combinations(cluster_labels, features, X_unnormal, centers_realspace,\n",
    "                              clusters_colors=[\"red\", \"blue\", \"green\", \"yellow\"],\n",
    "                              clusters_markers=[\"o\", \".\", \"x\", \"^\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NUEVO PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_mega_agg_pandas_clean[\"cluster\"] = cluster_labels\n",
    "\n",
    "df_plan_by_cluster = pd.DataFrame({\"# lines\" : df_mega_agg_pandas_clean.groupby([\"cluster\", \"label_shorted\"])[\"label_shorted\"].count()})\n",
    "df_plan_by_cluster[\"%\"] = 100.0*df_plan_by_cluster[\"# lines\"]/df_plan_by_cluster[\"# lines\"].sum()\n",
    "df_plan_by_cluster[\"group_%\"] = df_plan_by_cluster.groupby(level=0)[\"# lines\"].apply(lambda x: 100 * x / float(x.sum()))\n",
    "df_plan_by_cluster.reset_index(inplace=True)\n",
    "df_plan_by_cluster.sort_values(by=[\"cluster\", \"group_%\"], ascending=[True,False], inplace=True)\n",
    "df_plan_by_cluster2 = pd.merge(df_plan_by_cluster, df_centroides_unnormaliz.T, left_on=\"cluster\", right_index=True)\n",
    "df_plan_by_cluster2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "clusters_colors=[\"red\", \"blue\", \"green\", \"yellow\"]\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig_size = [20,9]\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "data=np.array(df_mega_agg_pandas_clean[features+[\"label_shorted\"]])\n",
    "\n",
    "f1 = 0\n",
    "f2 = 1\n",
    "f3 = 2\n",
    "label_shorted = 3\n",
    "fig = plt.figure()\n",
    "ax_all = Axes3D(fig)\n",
    "#loop through labels and plot each cluster\n",
    "for i, cc in enumerate(cluster_labels):\n",
    "    print(cc)\n",
    "    x_list = data[cluster_labels==cc,f1]\n",
    "    y_list = data[cluster_labels==cc,f2]\n",
    "    z_list = data[cluster_labels==cc,f3]\n",
    "    label_list = data[cluster_labels==cc,label_shorted]\n",
    "    \n",
    "    for x,y,z,label in zip(x_list, y_list, z_list,label_list):\n",
    "        _ = ax_all.text(x, y, z, '%s' % (label), alpha=0.8, color=clusters_colors[cc])\n",
    "\n",
    "    _ = ax_all.set_xlim(min(data[:,f1]), max(data[:,f1]))\n",
    "    _ = ax_all.set_ylim(min(data[:,f2]), max(data[:,f2]))\n",
    "    _ = ax_all.set_zlim(min(data[:,f3]), max(data[:,f3]))\n",
    "        \n",
    "    _ = ax_all.set_xlabel(features[f1]);\n",
    "    _ = ax_all.set_ylabel(features[f2]);\n",
    "    _ = ax_all.set_zlabel(features[f3]);\n",
    "\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#MegaTableObj.get_mega_info_msisdn(my_msisdn, cols=[\"day\", \"month\", \"tu_num\", \"bal_bod\", \"tu_amount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "PLOT_BY_CLUSTER = 30\n",
    "\n",
    "right_now = dt.datetime.now()\n",
    "formatted_datenow = right_now.strftime('%Y%m%d_%H%M%S')\n",
    "for c_label in [0]: # range(0,N_CLUSTERS):\n",
    "    \n",
    "    print(\"***************** CLUSTER {} ********************\".format(c_label))\n",
    "\n",
    "    cluster_idx = cluster_labels==c_label\n",
    "\n",
    "    msisdn_cluster = df_mega_agg_pandas_clean[cluster_idx][\"msisdn\"].tolist()\n",
    "\n",
    "    idx_list = np.random.choice(len(msisdn_cluster), PLOT_BY_CLUSTER) # range, nb of samples\n",
    "    df_graph = df_mega[df_mega[\"msisdn\"].isin([msisdn_cluster[ii] for ii in idx_list])].toPandas()\n",
    "\n",
    "    plot_filename = 'balanceevolution_{1}_cluster{0}.pdf'.format(c_label,formatted_datenow)\n",
    "\n",
    "    df_graph[\"fecha\"] = df_graph.apply(lambda row: dt.datetime.strptime(str(row[\"year\"]) + \"-\" + str(row[\"month\"]) + \"-\" + str(row[\"day\"]), \"%Y-%m-%d\"), axis=1)\n",
    "    \n",
    "    \n",
    "    df_graph = pd.merge(df_graph,df_tariffs_params, left_on=\"codigo_plan_precios\", right_on=\"PlanPrecios\")\n",
    "    \n",
    "    df_graph.sort_values(by=[\"msisdn\",\"fecha\"], ascending=[True, True], inplace=True)\n",
    "\n",
    "    GraphGenerationObj.plot_advanced_balance_evolution(df_graph, \n",
    "                                                    filename=plot_filename, \n",
    "                                                    start_date=ANALYSIS_PERIOD_START, \n",
    "                                                    end_date=ANALYSIS_PERIOD_END, \n",
    "                                                    saldo_column='bal_bod')\n",
    "\n",
    "print(\"END\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger.info(\"THE END!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from analytics import AnalyticsTools\n",
    "sns.set_style('white')\n",
    "from utils import get_correct_dates, float_to_int\n",
    "\n",
    "\n",
    "PLOTS_PATH = \"/var/SP/data/home/csanc109/data/plots/euphoria/\"\n",
    "right_now = dt.datetime.now()\n",
    "formatted_datenow = right_now.strftime('%Y-%m-%d %H%M%S')\n",
    "plot_filename = os.path.join(PLOTS_PATH, 'msisdn_examples_{}.pdf'.format(formatted_datenow))\n",
    "\n",
    "list_of_msisdn = ['630736935', '662377777']\n",
    "df_graph = df_mega.where(col(\"msisdn\").isin(['630736935', '662377777'])).toPandas()\n",
    "df_graph[\"fecha\"] = df_graph.apply(lambda row: dt.datetime.strptime(str(row[\"year\"]) + \"-\" + str(row[\"month\"]) + \"-\" + str(row[\"day\"]), \"%Y-%m-%d\"), axis=1)\n",
    "\n",
    "\n",
    "msisdn = '630736935'\n",
    "pdf_0 = df_graph[df_graph['msisdn'] == msisdn]\n",
    "tu_amount_field = \"tu_amount\" if \"tu_amount\" in pdf_0.keys() else \"tu_sum_amounts\"\n",
    "new_pdf = pdf_0[pdf_0[tu_amount_field] > 0]\n",
    "saldo_column = 'bal_bod'\n",
    "for _, row in new_pdf.iterrows():\n",
    "    print(row['fecha'], row[saldo_column] + row[tu_amount_field])\n",
    "\n",
    "\n",
    "#plot_advanced_balance_evolution(df_graph, start_date=ANALYSIS_PERIOD_START, end_date=ANALYSIS_PERIOD_END, save_path=plot_filename)\n",
    "\n",
    "logger.info(\"Created file {}\".format(plot_filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "columns = ['msisdn', 'tu_bin', 'date', 'tu_amount', 'tu_num']\n",
    "vals = [\n",
    "     (\"61051\", 1, \"03/07/2018\", 5, 1),\n",
    "     (\"61051\", 0, \"04/07/2018\", 0, 0),\n",
    "     (\"61051\", 0, \"05/07/2018\", 0, 0),\n",
    "     (\"61051\", 1, \"06/07/2018\", 15, 2),\n",
    "     (\"61051\", 0, \"07/07/2018\", 0, 0),\n",
    "     (\"61051\", 0, \"08/07/2018\", 0, 0),\n",
    "     (\"61051\", 0, \"09/07/2018\", 0, 0),\n",
    "     (\"61051\", 1, \"10/07/2018\", 10, 1),\n",
    "]\n",
    "\n",
    "\n",
    "# create DataFrame\n",
    "df = spark.createDataFrame(vals, columns)\n",
    "\n",
    "\n",
    "df = df.withColumn('beginning_of_day_ts', from_unixtime(unix_timestamp('date', 'dd/MM/yyy')))\n",
    "df.show()\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "df_agg_topups = (df.where(col(\"tu_bin\")>0)\n",
    "                 .withColumn(\"lag_period_ts\", datediff(col('beginning_of_day_ts'),\n",
    "                                                       lag(col('beginning_of_day_ts'),1,0).over(Window.partitionBy(\"msisdn\").orderBy(asc(\"beginning_of_day_ts\")))))\n",
    "                 .groupBy(\"msisdn\")\n",
    "                 .agg(sql_min(\"beginning_of_day_ts\").alias(\"first_period_ts\"),\n",
    "                      sql_max(\"beginning_of_day_ts\").alias(\"tu_latest_ts\"),\n",
    "                      sql_avg(\"tu_amount\").alias(\"avg_tu_amount\"),\n",
    "                      sql_sum(\"tu_amount\").alias(\"sum_tu_amount\"),\n",
    "                      sql_sum(\"tu_num\").alias(\"sum_tu_num\"),\n",
    "                      sql_sum(\"tu_bin\").alias(\"tu_num_distinct_days\"),\n",
    "                      sql_avg(\"lag_period_ts\").alias(\"tu_avg_diff_days\"),\n",
    "                      sql_min(\"lag_period_ts\").alias(\"tu_min_diff_days\"),\n",
    "                      sql_max(\"lag_period_ts\").alias(\"tu_max_diff_days\"),\n",
    "                      sql_stddev(\"lag_period_ts\").alias(\"tu_stddev_diff_days\")\n",
    "                  ).withColumnRenamed(\"msisdn\", \"msisdn_topups\"))\n",
    "\n",
    "df_agg_topups.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
