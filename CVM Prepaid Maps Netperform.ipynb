{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --master yarn --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.kryoserializer.buffer.max=1g --py-files /var/SP/data/home/adesant3/artifacts/bda-core-ra-complete-assembly-2.0.0.jar,/var/SP/data/home/adesant3/artifacts/common.zip,/var/SP/data/home/adesant3/artifacts/graphframes.zip,/var/SP/data/home/adesant3/artifacts/scripts.zip,/var/SP/data/home/adesant3/artifacts/xgboost4j-spark-2.1.1-0.7-jar-with-dependencies.jar --files /var/SP/data/home/adesant3/scripts/properties/red_agent/nodes-de.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/nodes-es.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/nodes-ie.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/nodes-it.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/nodes.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/nodes-pt.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/nodes-uk.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/nodes-ut.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/spark.properties --jars /var/SP/data/home/adesant3/artifacts/bda-core-ra-complete-assembly-2.0.0.jar --conf spark.akka.threads=32 --conf spark.akka.frameSize=500 --conf spark.driver.maxResultSize=8g --conf spark.local.dir=/var/SP/data/home/adesant3/local-spark-dir/ --conf spark.driver.port=58035 --conf spark.blockManager.port=58036 --conf spark.broadcast.port=58037 --conf spark.replClassServer.port=58038 --conf spark.ui.port=58039 --conf spark.executor.port=58040 --conf spark.fileserver.port=58041 --executor-memory 32g --driver-memory 16g --conf spark.shuffle.manager=tungsten-sort  --queue root.datascience.normal --conf spark.dynamicAllocation.enabled=true --conf spark.shuffle.service.enabled=true --conf spark.dynamicAllocation.maxExecutors=15 --conf spark.dynamicAllocation.minExecutors=1 --conf spark.dynamicAllocation.executorIdleTimeout=120 --conf spark.port.maxRetries=100 --files /var/SP/data/home/adesant3/scripts/properties/red_agent/nodes.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/nodes-de.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/nodes-es.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/nodes-ie.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/nodes-it.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/nodes-pt.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/nodes-uk.properties\n",
      " --master yarn --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.kryoserializer.buffer.max=1g --py-files /var/SP/data/home/adesant3/artifacts/bda-core-ra-complete-assembly-2.0.0.jar,/var/SP/data/home/adesant3/artifacts/common.zip,/var/SP/data/home/adesant3/artifacts/graphframes.zip,/var/SP/data/home/adesant3/artifacts/scripts.zip,/var/SP/data/home/adesant3/artifacts/xgboost4j-spark-2.1.1-0.7-jar-with-dependencies.jar --files /var/SP/data/home/adesant3/scripts/properties/red_agent/nodes-de.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/nodes-es.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/nodes-ie.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/nodes-it.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/nodes.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/nodes-pt.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/nodes-uk.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/nodes-ut.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/spark.properties --jars /var/SP/data/home/adesant3/artifacts/bda-core-ra-complete-assembly-2.0.0.jar --conf spark.akka.threads=32 --conf spark.akka.frameSize=500 --conf spark.driver.maxResultSize=8g --conf spark.local.dir=/var/SP/data/home/adesant3/local-spark-dir/ --conf spark.driver.port=58035 --conf spark.blockManager.port=58036 --conf spark.broadcast.port=58037 --conf spark.replClassServer.port=58038 --conf spark.ui.port=58039 --conf spark.executor.port=58040 --conf spark.fileserver.port=58041 --executor-memory 32g --driver-memory 16g --conf spark.shuffle.manager=tungsten-sort  --queue root.datascience.normal --conf spark.dynamicAllocation.enabled=true --conf spark.shuffle.service.enabled=true --conf spark.dynamicAllocation.maxExecutors=15 --conf spark.dynamicAllocation.minExecutors=1 --conf spark.dynamicAllocation.executorIdleTimeout=120 --conf spark.port.maxRetries=100 --files /var/SP/data/home/adesant3/scripts/properties/red_agent/nodes.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/nodes-de.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/nodes-es.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/nodes-ie.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/nodes-it.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/nodes-pt.properties,/var/SP/data/home/adesant3/scripts/properties/red_agent/nodes-uk.properties pyspark-shell \n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from common.src.main.python.utils.hdfs_generic import *\n",
    "import os\n",
    "\n",
    "MAX_N_EXECUTORS=15\n",
    "MIN_N_EXECUTORS=1\n",
    "N_CORES_EXECUTOR=4\n",
    "EXECUTOR_IDLE_MAX_TIME=120\n",
    "EXECUTOR_MEMORY='32g'\n",
    "DRIVER_MEMORY='16g'\n",
    "N_CORES_DRIVER=1\n",
    "MEMORY_OVERHEAD=N_CORES_EXECUTOR*2048\n",
    "QUEUE=\"root.datascience.normal\"\n",
    "BDA_CORE_VERSION=\"1.0.0\"\n",
    "\n",
    "SPARK_COMMON_OPTS=os.environ.get('SPARK_COMMON_OPTS', '')\n",
    "SPARK_COMMON_OPTS+=\" --executor-memory %s --driver-memory %s\" % (EXECUTOR_MEMORY, DRIVER_MEMORY)\n",
    "SPARK_COMMON_OPTS+=\" --conf spark.shuffle.manager=tungsten-sort\"\n",
    "SPARK_COMMON_OPTS+=\"  --queue %s\" % QUEUE\n",
    "\n",
    "# Dynamic allocation configuration\n",
    "SPARK_COMMON_OPTS+=\" --conf spark.dynamicAllocation.enabled=true\"\n",
    "SPARK_COMMON_OPTS+=\" --conf spark.shuffle.service.enabled=true\"\n",
    "SPARK_COMMON_OPTS+=\" --conf spark.dynamicAllocation.maxExecutors=%s\" % (MAX_N_EXECUTORS)\n",
    "SPARK_COMMON_OPTS+=\" --conf spark.dynamicAllocation.minExecutors=%s\" % (MIN_N_EXECUTORS)\n",
    "SPARK_COMMON_OPTS+=\" --conf spark.dynamicAllocation.executorIdleTimeout=%s\" % (EXECUTOR_IDLE_MAX_TIME)\n",
    "SPARK_COMMON_OPTS+=\" --conf spark.port.maxRetries=100\"\n",
    "\n",
    "BDA_ENV = os.environ.get('BDA_USER_HOME', '')\n",
    "\n",
    "# Attach bda-core-ra codebase\n",
    "SPARK_COMMON_OPTS+=\" --files \\\n",
    "{}/scripts/properties/red_agent/nodes.properties,\\\n",
    "{}/scripts/properties/red_agent/nodes-de.properties,\\\n",
    "{}/scripts/properties/red_agent/nodes-es.properties,\\\n",
    "{}/scripts/properties/red_agent/nodes-ie.properties,\\\n",
    "{}/scripts/properties/red_agent/nodes-it.properties,\\\n",
    "{}/scripts/properties/red_agent/nodes-pt.properties,\\\n",
    "{}/scripts/properties/red_agent/nodes-uk.properties\".format(*[BDA_ENV]*7)\n",
    "\n",
    "os.environ[\"SPARK_COMMON_OPTS\"] = SPARK_COMMON_OPTS\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"%s pyspark-shell \" % SPARK_COMMON_OPTS\n",
    "\n",
    "print os.environ.get('SPARK_COMMON_OPTS', '')\n",
    "print os.environ.get('PYSPARK_SUBMIT_ARGS', '')\n",
    "\n",
    "sc, sparkSession, sqlContext = run_sc()\n",
    "print sc.defaultParallelism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Spark utils\n",
    "from pyspark.sql.functions import (udf, col, decode, when, lit, lower, \n",
    "                                   translate, count, sum as sql_sum, max as sql_max, \n",
    "                                   isnull,\n",
    "                                   substring, size, length,\n",
    "                                   desc)\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .appName(\"Maps from Netperform\")\n",
    "         .master(\"yarn\")\n",
    "         .config(\"spark.submit.deployMode\", \"client\")\n",
    "         .config(\"spark.ui.showConsoleProgress\", \"true\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate()\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nameMonth = '201805'\n",
    "monthsHistoricData = [2,3,4]\n",
    "nameTableAnonymized = 'tests_es.ads_tmp_'+nameMonth+'_maps_notprepared'\n",
    "nameTableDeanonymized = 'tests_es.ads_tmp_'+nameMonth+'_maps_prepared'\n",
    "nameFileHDFS = nameMonth+'_hdfs_maps.csv'\n",
    "nameFileDelivery = 'delivery_'+nameMonth+'_maps.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "netPerform = (spark.read.table(\"raw_es.netperform_1_1\")\n",
    "                  .where((col(\"month\").isin(monthsHistoricData))\n",
    "                        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture output\n",
    "\n",
    "netPerform.groupBy('month').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract information from NetPerform in relation to maps apps consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data from NetPerform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, I keep the information concerning maps per user (id_client). There are three different types of information attributes that are relevant:\n",
    "- **Application Starts Hourly:** Number of times, per hour, a user has started an application. It can be regarded as the number of times a user has entered the application also. The *value* associated with this attribute is a counter/integer.\n",
    "    - The reason why it is per hour (and now per Day) it is because, the per Day attribute is almost empty.\n",
    "- ** Application Usage Time Hourly:** Minutes of times, per hour, a user has been actively using an application. The *value* associated with this attribute is in minutes.\n",
    "    - The reason why it is per hour (and now per Day) it is because, the per Day attribute is almost empty.\n",
    "- ** App Data Traffic - Daily (DL+UL):** Kb of data both downloaded/uploaded when using this application. The *value* associated with this attribute is in kb.\n",
    "\n",
    "In addition, the apps associated with maps are:\n",
    "- Google Maps\n",
    "- Waze\n",
    "- Google Earth\n",
    "- Coyote\n",
    "- TomTom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mapsUsage = (netPerform\n",
    "            .select('year','month','day','id_client','measurement_type_name','app_identifier','value')\n",
    "            .where(col('measurement_type_name').like(\"%Application Starts Hourly%\") |\n",
    "                   col('measurement_type_name').like(\"%Application Usage Time Hourly%\") |\n",
    "                   col('measurement_type_name').like(\"%App Data Traffic - Daily (DL+UL)%\"))\n",
    "            .where(col('app_identifier').like('%maps%') | \n",
    "                   col('app_identifier').like('%com.waze%') |\n",
    "                   col('app_identifier').like('%com.google.earth%') |\n",
    "                   col('app_identifier').like('%coyotesystems%') |\n",
    "                   col('app_identifier').like('%com.tomtom%')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each *measurement_type_name*, values are accumulated by the *sum* operator. The idea is to *cumsum* all the values (without taking averages or the count of times the *sum* has been aggregated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Obtain the aggregation per customer, year, month, day and type of measurement.\n",
    "# At this moment, the app information is lost as we are making no distinction among the apps.\n",
    "mapsUsagePerCustomerAndDay = \\\n",
    "(mapsUsage\n",
    " .groupBy(['id_client','year','month','day','measurement_type_name'])\n",
    " .agg({'value':'sum'})\n",
    " .withColumnRenamed('sum(value)','SumValuePerDay'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Netperform Mapping Table \n",
    "\n",
    "From this table *raw_es.netperform_mapping*, we will only keep the columns of *msisdn* and *client_id* in order to ojin with *netperform* table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data from netperform mapping, taking only the needed attributes (especially msisdn)\n",
    "netPerformMapping = (spark.read.table('raw_es.netperform_mapping')\n",
    "                     .select('year','month','day','client_id','msisdn')\n",
    "                     .where(col('month').isin(monthsHistoricData))\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's join both tables so that it is possible to connect information from netperform with customerbase (msisdn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Alias are used to avoid duplicates names when joining two tables\n",
    "alias_mapsUsagePerCustomerAndDay = mapsUsagePerCustomerAndDay.alias('mapsUsagePerCustomerAndDay')\n",
    "alias_netPerformMapping = netPerformMapping.alias('netPerformMapping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Join netperform_mapping with data from netperform so that at this moment the data is at a msisdn level.\n",
    "mapsUsagePerMsisdn = \\\n",
    "(alias_mapsUsagePerCustomerAndDay.join(alias_netPerformMapping,\n",
    "                                   (alias_mapsUsagePerCustomerAndDay.id_client == alias_netPerformMapping.client_id) &\n",
    "                                   (alias_mapsUsagePerCustomerAndDay.year == alias_netPerformMapping.year) &\n",
    "                                   (alias_mapsUsagePerCustomerAndDay.month == alias_netPerformMapping.month) &\n",
    "                                   (alias_mapsUsagePerCustomerAndDay.day == alias_netPerformMapping.day) ,\n",
    "                                   how='inner')\n",
    "                                  .select('netPerformMapping.msisdn',\n",
    "                                          'mapsUsagePerCustomerAndDay.SumValuePerDay',\n",
    "                                          'mapsUsagePerCustomerAndDay.measurement_type_name',\n",
    "                                          'mapsUsagePerCustomerAndDay.month',\n",
    "                                          'mapsUsagePerCustomerAndDay.day'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another alternative to drop dublicates may be just to drop every single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pivot the table so that the measurement_type_name are columns.\n",
    "mapsUsagePerMsisdnPivot = \\\n",
    "(mapsUsagePerMsisdn\n",
    " .groupBy('month','day','msisdn')\n",
    " .pivot('measurement_type_name')\n",
    " # TODO: I think this line is useless as it is summing up only one value.\n",
    " .agg(sql_sum('SumValuePerDay'))\n",
    " .na.fill(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Former cell is very important, as aggregating by month, day and msisdn, it is possible to include a default value instead of null. That value is 1. If the aggregation would have been done on the same command (just by aggregating at a *msisdn* level), then the *na.fill* command would have filled with 1 at a *msisdn* level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aggregate all the information at a msisdn level, forgetting about month and day.\n",
    "mapsUsagePerMsisdnDelivery = \\\n",
    "(mapsUsagePerMsisdnPivot.groupBy('msisdn').sum('App Data Traffic - Daily (DL+UL)',\n",
    "                                                      'Application Starts Hourly',\n",
    "                                                      'Application Usage Time Hourly'\n",
    "                                                     ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mapsUsagePerMsisdnDelivery.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the documentation, the function *withColumn(colName, col)* returns a new DataFrame by adding a column or replacing the existing column that has the same name. Thus, it is possible to use the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(mapsUsagePerMsisdnDelivery\n",
    " .withColumn('msisdn',(substring('msisdn',3,9)))\n",
    " .withColumnRenamed('sum(App Data Traffic - Daily (DL+UL))','Total_data_kb')\n",
    " .withColumnRenamed('sum(Application Starts Hourly)','Total_starts')\n",
    " .withColumnRenamed('sum(Application Usage Time Hourly)','Total_time')\n",
    " .where(col('msisdn').isNotNull())\n",
    " .where(length(col('msisdn')) == 9)\n",
    " .write\n",
    " .format('parquet')\n",
    " .mode('overwrite')\n",
    " .saveAsTable(nameTableAnonymized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from subprocess import Popen, PIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = (Popen(['sh',\n",
    "            '/home/jsotovi2/desanonimizar.sh',\n",
    "            '--fields',\n",
    "            'msisdn=DE_MSISDN',\n",
    "            '--overwrite',\n",
    "            nameTableAnonymized,\n",
    "            nameTableDeanonymized], stdin=PIPE, stdout=PIPE, stderr=PIPE))\n",
    "output, err = p.communicate(input = \"s\\n\")\n",
    "rc = p.returncode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[info] Unravel Sensor 4.2.1056/1.3.9-unravel-1705 initializing.\\n\\xc2\\xbfEst\\xc3\\xa1s seguro de que quieres sobreescribir la tabla tests_es.ads_tmp_201804_maps_prepared? [s/n]: \\xc2\\xa1Completado!\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/SPARK2-2.1.0.cloudera1-1.cdh5.7.0.p0.120904/lib/spark2) overrides detected (/opt/cloudera/parcels/SPARK2/lib/spark2).\\nWARNING: Running spark-class from user-defined location.\\n\\r[Stage 0:>                                                          (0 + 0) / 7]\\r[Stage 0:>                                                          (0 + 1) / 7]\\r[Stage 0:>                                                          (0 + 3) / 7]\\r[Stage 0:>                                                          (0 + 4) / 7]\\r[Stage 0:>                                                          (0 + 5) / 7]\\r[Stage 0:>                                                          (0 + 7) / 7]\\r[Stage 0:========>                                                  (1 + 6) / 7]\\r[Stage 0:================>                                          (2 + 5) / 7]\\r[Stage 0:=========================>                                 (3 + 4) / 7]\\r[Stage 0:==========================================>                (5 + 2) / 7]\\r[Stage 0:==================================================>        (6 + 1) / 7]\\r                                                                                \\r'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mapsDeliveryTable = (spark.read.table(nameTableDeanonymized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- msisdn_anon: string (nullable = true)\n",
      " |-- Total_data_kb: double (nullable = true)\n",
      " |-- Total_starts: double (nullable = true)\n",
      " |-- Total_time: double (nullable = true)\n",
      " |-- msisdn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapsDeliveryTable.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mapsDeliveryTable.write.csv(nameFileHDFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = (Popen(['hadoop',\n",
    "            'fs',\n",
    "            '-copyToLocal',\n",
    "            nameFileHDFS,\n",
    "            '/var/SP/data/home/adesant3/data/download/.'], stdin=PIPE, stdout=PIPE, stderr=PIPE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = (Popen(['sed 1d /var/SP/data/home/adesant3/data/download/'+nameFileHDFS+'/*.csv > /var/SP/data/home/adesant3/data/delivery/'+nameFileDelivery], \n",
    "           stdin=PIPE, stdout=PIPE, stderr=PIPE, shell=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output, err = p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
