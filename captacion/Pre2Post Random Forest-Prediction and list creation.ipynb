{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App for making predictions using model from stage 1, once tested and validated in stage 2\n",
    "\n",
    "\n",
    "We have already created a model and trained it (Notebook #1), tested it in another month (Notebook #2).\n",
    "\n",
    "There's only one step left: actually using it to make predictions! This notebook does exactly that. Assumptions:\n",
    "\n",
    "1. We have a saved model (which in fact is a pyspark.ml.PipelineModel) in Julio's HDFS home directory for the project (`\"hdfs:///user/jsotovi2/pre2post/best_model_pre2post_yyyymmdd_hhmmss.sparkModel\"`), where `yyyymmdd_hhmmss` is the datetime at which the model was saved. By default, this code will always grab the latest model in presence of more than one.\n",
    "2. The model scored well in Notebook #2, meaning that the `auc_test` variable >= $0.8$.\n",
    "3. The following tables exist in Hive's metastore:\n",
    "    + `raw_es.vf_pre_ac_final`\n",
    "    + `raw_es.vf_pre_info_tarif`\n",
    "4. Date and time clocks in the Spark driver are accurate. This is important because we rely heavilly on date in order to compute *which is the next month after the current one* and that sort of stuff.\n",
    "\n",
    "With all that said, let's start:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The one and only line that we have to change between executions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand better the whole workflow:\n",
    "\n",
    "1. We use historical data from one month (eg. 2017/04) to train a couple of models, and keep/save the best one (this is done in notebook #1, which you do not currently have). Once we have got a good model, we just save it to HDFS. This *does not has to happen every single month*; as long as the model is not extremely outated, there should be no need for running this all months.\n",
    "2. We then use historical data from next month (2017/05) to get an unbiased measure on how good our saved model is. This should run all months; it is always important to keep track of model performance on a monthly basis.\n",
    "3. Finally, in order to make predictions, we will use the most recent data to predict customer behaviour (notebook #3). This can be ran as many times as we want, when needed (usually once a month). This notebook does exactly that.\n",
    "\n",
    "This is notebook (#3), which in theory will only have to be ran once a month (probably towards the end of the month), since its output will be used for marketing campaigns (which start at the beggining of the month after):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The yyyymm date to predict people who will\n",
    "# migrate from prepaid to postpaid in two months.\n",
    "# Should be manually changed.\n",
    "# We should get the most recent month for which\n",
    "# we have data.\n",
    "\n",
    "# Given that data needed for this notebook\n",
    "# comes from Spain CVM, the data of 201706\n",
    "# is usually available on 2017/07/15.\n",
    "month_for_getting_prepaid_data = \"2017801\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it. There are no other dates needed in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports and app setup\n",
    "\n",
    "Your usual stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This literal_eval is needed since \n",
    "# we have to read from a textfile\n",
    "# which is formatted as python objects.\n",
    "# It is totally safe.\n",
    "from ast import literal_eval\n",
    "\n",
    "# Standard Library stuff:\n",
    "from functools import partial\n",
    "from datetime import date, timedelta, datetime\n",
    "\n",
    "# Numpy stuff\n",
    "from numpy import nan as np_nan, round as np_round, int64 as np_int64\n",
    "\n",
    "# Spark stuff\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import (udf, col, decode, when, lit, lower, \n",
    "                                   translate, count, sum as sql_sum, max as sql_max, \n",
    "                                   isnull)\n",
    "from pyspark.sql.types import DoubleType, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .appName(\"Pre2Post prediction and list creation\")\n",
    "         .master(\"yarn\")\n",
    "         .config(\"spark.submit.deployMode\", \"client\")\n",
    "         .config(\"spark.ui.showConsoleProgress\", \"true\")\n",
    "         .enableHiveSupport()\n",
    "         .getOrCreate()\n",
    "         )\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data imports and first transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to notebooks #1 and #2, we only have to use two tables in this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first one is `raw_es.vf_pre_ac_final`, which contains information about *which VF clients were prepaid for a given month*, and very basic info about them (age, nationality, number of prepaid/postpaid services...)\n",
    "\n",
    "The following cell includes reading the table and some transformations (that might can be easily translated to RedAgent):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "useful_columns_from_acFinalPrepago = [\"FECHA_EJECUCION\",\n",
    "                                      \"MSISDN\",\n",
    "                                      \"NUM_DOCUMENTO_CLIENTE\",\n",
    "                                      \"NACIONALIDAD\",\n",
    "                                      \"NUM_PREPAGO\",\n",
    "                                      \"NUM_POSPAGO\",\n",
    "                                      #\"Tipo_Documento_Cliente\", Very uninformed\n",
    "                                      \"Tipo_Documento_Comprador\",\n",
    "                                      \"X_FECHA_NACIMIENTO\"]\n",
    "\n",
    "# Lots of tables in Hive have empty string instead\n",
    "# of null for missing values in StringType columns:\n",
    "def empty_str_to_null(string_value):\n",
    "    \"\"\"\n",
    "    Turns empty strings to None, that are\n",
    "    handled as nulls by Spark DataFrames:\n",
    "    \"\"\"\n",
    "    if string_value == \"\":\n",
    "        result = None\n",
    "    elif string_value == u\"\":\n",
    "        result = None\n",
    "    else:\n",
    "        result = string_value\n",
    "    return result\n",
    "\n",
    "# We register previous function as a udf:\n",
    "empty_string_to_null = udf(empty_str_to_null, StringType())\n",
    "\n",
    "# Function that returns customer age out of his/her birthdate:\n",
    "def get_customer_age_raw(birthdate, month_for_getting_prepaid_data):\n",
    "        if birthdate is None:\n",
    "            return np_nan\n",
    "        \n",
    "        # Now, they use only birth year:\n",
    "        #parsed_date = datetime.strptime(str(int(birthdate)), \"%Y%m%d\")\n",
    "        parsed_date = datetime(int(birthdate), 6, 1)\n",
    "        timedelta = datetime.strptime(month_for_getting_prepaid_data, \"%Y%m\") - parsed_date\n",
    "        return timedelta.days / 365.25\n",
    "\n",
    "# We register previous function as a udf:\n",
    "def get_customer_age_udf(birthdate, month):\n",
    "    return udf(partial(get_customer_age_raw, month_for_getting_prepaid_data=month), DoubleType())(birthdate)\n",
    "\n",
    "# Self-explanatory.\n",
    "def subsitute_crappy_characters(string_column):\n",
    "    \"\"\"\n",
    "    I really hate charset encoding.\n",
    "    \"\"\"\n",
    "    return (string_column\n",
    "            .replace(u\"\\ufffd\", u\"Ã±\")\n",
    "            # add more here in the future if needed\n",
    "           )\n",
    "\n",
    "# We register previous function as a udf:\n",
    "substitute_crappy_characters_udf = udf(subsitute_crappy_characters, StringType())\n",
    "\n",
    "# And we finally read raw_es.vf_pre_ac_final,\n",
    "# filtering by date, and with new columns\n",
    "# that we create using our UDFs:\n",
    "acFinalPrepago = (spark.read.table(\"raw_es.vf_pre_ac_final\")\n",
    "                  .where((col(\"year\") == int(month_for_getting_prepaid_data[:4]))\n",
    "                         & (col(\"month\") == int(month_for_getting_prepaid_data[4:]))\n",
    "                        )\n",
    "                  .where(col(\"X_FECHA_NACIMIENTO\") != \"X_FE\")\n",
    "                  #.select(*useful_columns_from_acFinalPrepago)\n",
    "                  .withColumn(\"X_FECHA_NACIMIENTO\", empty_string_to_null(col(\"X_FECHA_NACIMIENTO\")))\n",
    "                  .withColumn(\"NUM_DOCUMENTO_CLIENTE\", empty_string_to_null(col(\"NUM_DOCUMENTO_CLIENTE\")))\n",
    "                  .withColumn(\"NUM_DOCUMENTO_COMPRADOR\", empty_string_to_null(col(\"NUM_DOCUMENTO_COMPRADOR\")))\n",
    "                  .withColumn(\"age_in_years\", get_customer_age_udf(col(\"X_FECHA_NACIMIENTO\"),\n",
    "                                                                   month_for_getting_prepaid_data)\n",
    "                             )\n",
    "                  .withColumn(\"NACIONALIDAD\", substitute_crappy_characters_udf(col(\"NACIONALIDAD\")))\n",
    "                 )\n",
    "\n",
    "# Good old repartition for underpartitioned tables:\n",
    "acFinalPrepago = acFinalPrepago.repartition(int(acFinalPrepago.count() / 500)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In this acFinalPrepago DF we have a column (nationality)\n",
    "# with lot's of different values (high cardinality), which\n",
    "# is terrible for ML models, so we will get the most frequent\n",
    "# countries, and replace all others with \"Other\":\n",
    "\n",
    "most_frequent_countries = [u\"EspaÃ±a\",\n",
    "                           u\"Marruecos\",\n",
    "                           u\"Rumania\",\n",
    "                           u\"Colombia\",\n",
    "                           u\"Italia\",\n",
    "                           u\"Ecuador\",\n",
    "                           u\"Alemania\",\n",
    "                           u\"Estados Unidos\",\n",
    "                           u\"Francia\",\n",
    "                           u\"Brasil\",\n",
    "                           u\"Argentina\",\n",
    "                           u\"Afganistan\",\n",
    "                           u\"Bolivia\",\n",
    "                           u\"Gran BretaÃ±a\",\n",
    "                           u\"Portugal\",\n",
    "                           u\"Paraguay\",\n",
    "                           u\"China\",\n",
    "                           u\"Gran Bretana\",\n",
    "                           u\"Venezuela\",\n",
    "                           u\"Honduras\",\n",
    "                           u\"Corea del Sur\"]\n",
    "\n",
    "\n",
    "acFinalPrepago = acFinalPrepago.withColumn(\"NACIONALIDAD\", when(col(\"NACIONALIDAD\").isin(most_frequent_countries),\n",
    "                                                                  col(\"NACIONALIDAD\")\n",
    "                                                                 ).otherwise(lit(\"Other\"))\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is only one data source remaining, which is the one with pretty much all customer consumption patterns (MOU, MB, number of monthly calls...). We just have to read it, and join it with `acFinalPrepago`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will read raw_es.vf_pre_info_tarif.\n",
    "# The columns that we care about are the following:\n",
    "\n",
    "useful_columns_from_tarificadorPre = ['MSISDN',\n",
    "                                      'MOU',\n",
    "                                      'TOTAL_LLAMADAS',\n",
    "                                      'TOTAL_SMS',\n",
    "                                      'MOU_Week',\n",
    "                                      'LLAM_Week',\n",
    "                                      'SMS_Week',\n",
    "                                      'MOU_Weekend',\n",
    "                                      'LLAM_Weekend',\n",
    "                                      'SMS_Weekend',\n",
    "                                      'MOU_VF',\n",
    "                                      'LLAM_VF',\n",
    "                                      'SMS_VF',\n",
    "                                      'MOU_Fijo',\n",
    "                                      'LLAM_Fijo',\n",
    "                                      'SMS_Fijo',\n",
    "                                      'MOU_OOM',\n",
    "                                      'LLAM_OOM',\n",
    "                                      'SMS_OOM',\n",
    "                                      'MOU_Internacional',\n",
    "                                      'LLAM_Internacional',\n",
    "                                      'SMS_Internacional',\n",
    "                                      'ActualVolume',\n",
    "                                      'Num_accesos',\n",
    "                                      'Plan',\n",
    "                                      'Num_Cambio_Planes',\n",
    "                                      #'TOP_Internacional', # No idea of what is\n",
    "                                      'LLAM_COMUNIDAD_SMART',\n",
    "                                      'MOU_COMUNIDAD_SMART',\n",
    "                                      'LLAM_SMS_COMUNIDAD_SMART',\n",
    "                                      'Flag_Uso_Etnica',\n",
    "                                      'cuota_SMART8',\n",
    "                                      'cuota_SMART12',\n",
    "                                      'cuota_SMART16']\n",
    "\n",
    "# Read raw_es.vf_pre_info_tarif + yyyymm predicates + \n",
    "# column selection:\n",
    "tarificadorPre = (spark.read.table(\"raw_es.vf_pre_info_tarif\")\n",
    "                  .where((col(\"year\") == int(month_for_getting_prepaid_data[:4]))\n",
    "                         & (col(\"month\") == int(month_for_getting_prepaid_data[4:]))\n",
    "                        )\n",
    "                  .select(*useful_columns_from_tarificadorPre)\n",
    "                 )\n",
    "\n",
    "# Good old repartition for underpartitioned tables:\n",
    "tarificadorPre = tarificadorPre.repartition(int(tarificadorPre.count() / 500)+1)\n",
    "\n",
    "# Just as it happend with Nationlity column,\n",
    "# Plan is a column with very high cardenality.\n",
    "# We will replace any category not included\n",
    "# in the following list with \"Other\":\n",
    "plan_categories = ['PPIB7',\n",
    "                   'PPFCL',\n",
    "                   'PPIB4',\n",
    "                   'PPXS8',\n",
    "                   'PPIB8',\n",
    "                   'PPIB9',\n",
    "                   'PPTIN',\n",
    "                   'PPIB1',\n",
    "                   'PPVIS',\n",
    "                   'PPREX',\n",
    "                   'PPIB5',\n",
    "                   'PPREU',\n",
    "                   'PPRET',\n",
    "                   'PPFCS',\n",
    "                   'PPIB6',\n",
    "                   'PPREY',\n",
    "                   'PPVSP',\n",
    "                   'PPIB2',\n",
    "                   'PPIB3',\n",
    "                   'PPRE2',\n",
    "                   'PPRE5',\n",
    "                   'PPVE2',\n",
    "                   'PPVE1',\n",
    "                   'PPRES',\n",
    "                   'PPJ24',\n",
    "                   'PPVE3',\n",
    "                   'PPJAT',\n",
    "                   'PPJMI']\n",
    "\n",
    "tarificadorPre_2 = tarificadorPre.withColumn(\"Plan\",\n",
    "                                             when(col(\"Plan\").isin(plan_categories),\n",
    "                                                  col(\"Plan\")\n",
    "                                                 ).otherwise(lit(\"Other\"))\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only one step left:\n",
    "prepaid_dataset_1 = acFinalPrepago.join(tarificadorPre_2,\n",
    "                                        how=\"inner\",\n",
    "                                        on=\"MSISDN\").persist(StorageLevel.DISK_ONLY_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`prepaid_dataset_1` is the DF that we will use for model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100% analogous to notebook #2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numeric_columns = ['NUM_PREPAGO',\n",
    "                   'NUM_POSPAGO',\n",
    "                   'age_in_years',\n",
    "                   #'documenttype_Other',\n",
    "                   #'documenttype_cif',\n",
    "                   #'documenttype_nif',\n",
    "                   #'documenttype_pasaporte',\n",
    "                   #'documenttype_tarj_residente',\n",
    "                   #'nationality_Afganistan',\n",
    "                   #'nationality_Alemania',\n",
    "                   #'nationality_Argentina',\n",
    "                   #'nationality_Bolivia',\n",
    "                   #'nationality_Brasil',\n",
    "                   #'nationality_China',\n",
    "                   #'nationality_Colombia',\n",
    "                   #'nationality_Corea_del_Sur',\n",
    "                   #'nationality_Ecuador',\n",
    "                   #'nationality_EspaÃ±a',\n",
    "                   #'nationality_Estados_Unidos',\n",
    "                   #'nationality_Francia',\n",
    "                   #'nationality_Gran_Bretana',\n",
    "                   #'nationality_Gran_BretaÃ±a',\n",
    "                   #'nationality_Honduras',\n",
    "                   #'nationality_Italia',\n",
    "                   #'nationality_Marruecos',\n",
    "                   #'nationality_Other',\n",
    "                   #'nationality_Paraguay',\n",
    "                   #'nationality_Portugal',\n",
    "                   #'nationality_Rumania',\n",
    "                   #'nationality_Venezuela',\n",
    "                   #'migrated_to_postpaid',\n",
    "                   'MOU',\n",
    "                   'TOTAL_LLAMADAS',\n",
    "                   'TOTAL_SMS',\n",
    "                   'MOU_Week',\n",
    "                   'LLAM_Week',\n",
    "                   'SMS_Week',\n",
    "                   'MOU_Weekend',\n",
    "                   'LLAM_Weekend',\n",
    "                   'SMS_Weekend',\n",
    "                   'MOU_VF',\n",
    "                   'LLAM_VF',\n",
    "                   'SMS_VF',\n",
    "                   'MOU_Fijo',\n",
    "                   'LLAM_Fijo',\n",
    "                   'SMS_Fijo',\n",
    "                   'MOU_OOM',\n",
    "                   'LLAM_OOM',\n",
    "                   'SMS_OOM',\n",
    "                   'MOU_Internacional',\n",
    "                   'LLAM_Internacional',\n",
    "                   'SMS_Internacional',\n",
    "                   'ActualVolume',\n",
    "                   'Num_accesos',\n",
    "                   'Num_Cambio_Planes',\n",
    "                   'LLAM_COMUNIDAD_SMART',\n",
    "                   'MOU_COMUNIDAD_SMART',\n",
    "                   'LLAM_SMS_COMUNIDAD_SMART',\n",
    "                   #'Flag_Uso_Etnica',\n",
    "                   'cuota_SMART8',\n",
    "                   #'cuota_SMART12',\n",
    "                   #'cuota_SMART16',\n",
    "                   #'plan_PPFCL',\n",
    "                   #'plan_PPFCS',\n",
    "                   #'plan_PPIB1',\n",
    "                   #'plan_PPIB2',\n",
    "                   #'plan_PPIB3',\n",
    "                   #'plan_PPIB4',\n",
    "                   #'plan_PPIB5',\n",
    "                   #'plan_PPIB6',\n",
    "                   #'plan_PPIB7',\n",
    "                   #'plan_PPIB8',\n",
    "                   #'plan_PPIB9',\n",
    "                   #'plan_PPJ24',\n",
    "                   #'plan_PPJAT',\n",
    "                   #'plan_PPJMI',\n",
    "                   #'plan_PPRE2',\n",
    "                   #'plan_PPRE5',\n",
    "                   #'plan_PPRES',\n",
    "                   #'plan_PPRET',\n",
    "                   #'plan_PPREU',\n",
    "                   #'plan_PPREX',\n",
    "                   #'plan_PPREY',\n",
    "                   #'plan_PPTIN',\n",
    "                   #'plan_PPVE1',\n",
    "                   #'plan_PPVE2',\n",
    "                   #'plan_PPVE3',\n",
    "                   #'plan_PPVIS',\n",
    "                   #'plan_PPVSP',\n",
    "                   #'plan_PPXS8'\n",
    "                  ]\n",
    "\n",
    "categorical_columns = [\"tipo_documento_comprador\", \"NACIONALIDAD\", \"Plan\"]\n",
    "\n",
    "prepaid_dataset_2 = prepaid_dataset_1\n",
    "\n",
    "for column in numeric_columns:\n",
    "    prepaid_dataset_2 = prepaid_dataset_2.withColumn(column, col(column).cast(DoubleType()))\n",
    "    \n",
    "prepaid_dataset_2 = (prepaid_dataset_2\n",
    "                     .repartition(int(prepaid_dataset_2.count() / 50000)+1)\n",
    "                     .persist(StorageLevel.DISK_ONLY_2)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load machine learning model from HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100% analogous to notebook #2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "directory_list = subprocess.check_output([\"hadoop\", \"fs\", \"-ls\", \"/user/jsotovi2/pre2post_v2\"]).split(\"\\n\")\n",
    "files_list = [item.split(\" \")[-1].split(\"/\")[-1] for item in directory_list if \".\" in item.split(\" \")[-1].split(\"/\")[-1]]\n",
    "history_list = [\"_\".join(theFile.replace(\".txt\",\"\").split(\"_\")[-2:]) \n",
    "                for theFile in files_list \n",
    "                if \"model_pre2post_results\" in theFile]\n",
    "\n",
    "most_recent_model_date = list(reversed(sorted([datetime.strptime(a_date, \"%Y%m%d_%H%M%S\") for a_date in history_list])))[0]\n",
    "most_recent_model_date_str = most_recent_model_date.strftime(\"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20170810_130214'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_recent_model_date_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "most_recent_model = PipelineModel.load(\"hdfs:///user/jsotovi2/pre2post_v2/best_model_pre2post_\"\n",
    "                                       + most_recent_model_date_str + \".sparkModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final data preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "100% analogous to notebook #2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the median value for age:\n",
    "training_results_file = sc.textFile(\"hdfs:///user/jsotovi2/pre2post_v2/best_model_pre2post_results_\"\n",
    "                                    + most_recent_model_date_str + \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_results = dict([literal_eval(row) for row in training_results_file.collect()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The actual imputation:\n",
    "prepaid_dataset_2_filled = (prepaid_dataset_2\n",
    "                            .na.fill(float(training_results[\"age_median_value\"]), subset=[\"age_in_years\"])\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prepaid_dataset_2_filled_filtered = (prepaid_dataset_2_filled\n",
    "                                     .where(col(\"NACIONALIDAD\")!=\"Rumania\")\n",
    "                                     .where(col(\"NACIONALIDAD\")!=\"Marruecos\")\n",
    "                                     .where(col(\"NACIONALIDAD\")!=\"Colombia\")\n",
    "                                     .where(col(\"NACIONALIDAD\")!=\"Ecuador\")\n",
    "                                     .where(col(\"NACIONALIDAD\")!=\"Bolivia\")\n",
    "                                     .where(col(\"NACIONALIDAD\")!=\"Gran Bretana\")\n",
    "                                     .where(col(\"NACIONALIDAD\")!=\"Argentina\")\n",
    "                                     .where(col(\"tipo_documento_comprador\")!=\"Pasaporte\")\n",
    "                                     .where(col(\"tipo_documento_cliente\")!=\"Pasaporte\")\n",
    "                                     .where(col(\"tipo_documento_comprador\")!=\"PASAPORTE\")\n",
    "                                     .where(col(\"tipo_documento_cliente\")!=\"PASAPORTE\")\n",
    "                                     .where(col(\"tipo_documento_comprador\")!=\"CIF\")\n",
    "                                     .where(col(\"tipo_documento_cliente\")!=\"CIF\")\n",
    "                                     .where(col(\"tipo_documento_comprador\")!=\"Otros\")\n",
    "                                     .where(col(\"tipo_documento_cliente\")!=\"Otros\")\n",
    "                                     .where(col(\"Plan\")!=\"PPVE3\")\n",
    "                                     .where(col(\"Plan\")!=\"PPJAT\")\n",
    "                                     .where(col(\"Plan\")!=\"PPJ24\")\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = most_recent_model.transform(prepaid_dataset_2_filled_filtered).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In order to export the predictions, \n",
    "# we only care about two columns:\n",
    "# MSISDN and the second (first-indexed)\n",
    "# column of te probability column created\n",
    "# by our model.transform (this probability\n",
    "# column is of type org.apache.spark.sql.types.VectorUDTType):\n",
    "results = (predictions\n",
    "           .select(col(\"MSISDN\"),\n",
    "                   udf(lambda x: x.tolist()[1], DoubleType())\n",
    "                   (col(\"probability\")).alias(\"raw_score\")\n",
    "                  )\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1020.approxQuantile.\n: java.util.NoSuchElementException: next on empty iterator\n\tat scala.collection.Iterator$$anon$2.next(Iterator.scala:39)\n\tat scala.collection.Iterator$$anon$2.next(Iterator.scala:37)\n\tat scala.collection.IndexedSeqLike$Elements.next(IndexedSeqLike.scala:63)\n\tat scala.collection.IterableLike$class.head(IterableLike.scala:107)\n\tat scala.collection.mutable.ArrayOps$ofRef.scala$collection$IndexedSeqOptimized$$super$head(ArrayOps.scala:186)\n\tat scala.collection.IndexedSeqOptimized$class.head(IndexedSeqOptimized.scala:126)\n\tat scala.collection.mutable.ArrayOps$ofRef.head(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.last(TraversableLike.scala:431)\n\tat scala.collection.mutable.ArrayOps$ofRef.scala$collection$IndexedSeqOptimized$$super$last(ArrayOps.scala:186)\n\tat scala.collection.IndexedSeqOptimized$class.last(IndexedSeqOptimized.scala:132)\n\tat scala.collection.mutable.ArrayOps$ofRef.last(ArrayOps.scala:186)\n\tat org.apache.spark.sql.catalyst.util.QuantileSummaries.query(QuantileSummaries.scala:189)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$$anonfun$multipleApproxQuantiles$1$$anonfun$apply$1.apply$mcDD$sp(StatFunctions.scala:92)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$$anonfun$multipleApproxQuantiles$1$$anonfun$apply$1.apply(StatFunctions.scala:92)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$$anonfun$multipleApproxQuantiles$1$$anonfun$apply$1.apply(StatFunctions.scala:92)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$$anonfun$multipleApproxQuantiles$1.apply(StatFunctions.scala:92)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$$anonfun$multipleApproxQuantiles$1.apply(StatFunctions.scala:92)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.multipleApproxQuantiles(StatFunctions.scala:92)\n\tat org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:73)\n\tat org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:84)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-78555ee31753>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m                        .approxQuantile(\"raw_score\", \n\u001b[0;32m     11\u001b[0m                                        \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m100.0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m101\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                                        relativeError=0.0)\n\u001b[0m\u001b[0;32m     13\u001b[0m                   ))\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/cloudera/parcels/SPARK2-2.1.0.cloudera1-1.cdh5.7.0.p0.120904/lib/spark2/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mapproxQuantile\u001b[1;34m(self, col, probabilities, relativeError)\u001b[0m\n\u001b[0;32m   1391\u001b[0m         \u001b[0mrelativeError\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrelativeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1393\u001b[1;33m         \u001b[0mjaq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapproxQuantile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelativeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1394\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjaq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/cloudera/parcels/SPARK2-2.1.0.cloudera1-1.cdh5.7.0.p0.120904/lib/spark2/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/cloudera/parcels/SPARK2-2.1.0.cloudera1-1.cdh5.7.0.p0.120904/lib/spark2/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/cloudera/parcels/SPARK2-2.1.0.cloudera1-1.cdh5.7.0.p0.120904/lib/spark2/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1020.approxQuantile.\n: java.util.NoSuchElementException: next on empty iterator\n\tat scala.collection.Iterator$$anon$2.next(Iterator.scala:39)\n\tat scala.collection.Iterator$$anon$2.next(Iterator.scala:37)\n\tat scala.collection.IndexedSeqLike$Elements.next(IndexedSeqLike.scala:63)\n\tat scala.collection.IterableLike$class.head(IterableLike.scala:107)\n\tat scala.collection.mutable.ArrayOps$ofRef.scala$collection$IndexedSeqOptimized$$super$head(ArrayOps.scala:186)\n\tat scala.collection.IndexedSeqOptimized$class.head(IndexedSeqOptimized.scala:126)\n\tat scala.collection.mutable.ArrayOps$ofRef.head(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.last(TraversableLike.scala:431)\n\tat scala.collection.mutable.ArrayOps$ofRef.scala$collection$IndexedSeqOptimized$$super$last(ArrayOps.scala:186)\n\tat scala.collection.IndexedSeqOptimized$class.last(IndexedSeqOptimized.scala:132)\n\tat scala.collection.mutable.ArrayOps$ofRef.last(ArrayOps.scala:186)\n\tat org.apache.spark.sql.catalyst.util.QuantileSummaries.query(QuantileSummaries.scala:189)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$$anonfun$multipleApproxQuantiles$1$$anonfun$apply$1.apply$mcDD$sp(StatFunctions.scala:92)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$$anonfun$multipleApproxQuantiles$1$$anonfun$apply$1.apply(StatFunctions.scala:92)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$$anonfun$multipleApproxQuantiles$1$$anonfun$apply$1.apply(StatFunctions.scala:92)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$$anonfun$multipleApproxQuantiles$1.apply(StatFunctions.scala:92)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$$anonfun$multipleApproxQuantiles$1.apply(StatFunctions.scala:92)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\n\tat org.apache.spark.sql.execution.stat.StatFunctions$.multipleApproxQuantiles(StatFunctions.scala:92)\n\tat org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:73)\n\tat org.apache.spark.sql.DataFrameStatFunctions.approxQuantile(DataFrameStatFunctions.scala:84)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "# This cell will add one column more, called percentiles,\n",
    "# which are the percentiles of each raw_score.\n",
    "\n",
    "# This code is a little harder to understand, but that's OK.\n",
    "\n",
    "\n",
    "# Percentile computation in Spark DFs is as counter-intuitive as it gets...\n",
    "percentiles = list(zip(list(reversed([i/100.0 for i in range(1, 101, 1)])),\n",
    "                       results\n",
    "                       .approxQuantile(\"raw_score\", \n",
    "                                       list(reversed([i/100.0 for i in range(1, 101, 1)])),\n",
    "                                       relativeError=0.0)\n",
    "                  ))\n",
    "\n",
    "# Broadcasting this list is not really neccesary,\n",
    "# but may help understanding the code.\n",
    "# If you decide to remove the broadcast,\n",
    "# remember to subsitute percentiles.value with\n",
    "# just percentiles in the next function.\n",
    "percentiles_broadcast = sc.broadcast(percentiles)\n",
    "\n",
    "def get_percentile(row, percentiles):\n",
    "    \"\"\"\n",
    "    For each row of a column,\n",
    "    returns the corresponding percentile.\n",
    "    \n",
    "    percentiles argument must be a broadcast\n",
    "    value.\n",
    "    \"\"\"\n",
    "    resulting_percentile = 1.0\n",
    "    for p, q in percentiles.value:\n",
    "        if row <= q:\n",
    "            resulting_percentile = p\n",
    "    return resulting_percentile\n",
    "\n",
    "def get_percentile_udf(column, percentiles):\n",
    "    \"\"\"\n",
    "    Computes the corresponding percentiles\n",
    "    for one column.\n",
    "    \n",
    "    Args:\n",
    "        column -> A Spark DF column\n",
    "        percentiles -> A broadcasted list of tuples (percentile, value)\n",
    "        \n",
    "    Returns:\n",
    "        A Spark DF column with the percentile each row belongs to.\n",
    "    \"\"\"\n",
    "    return udf(partial(get_percentile, percentiles=percentiles),\n",
    "                       DoubleType())(column)\n",
    "\n",
    "results_with_percentile = (results\n",
    "                           .withColumn(\"percentile\", \n",
    "                                       get_percentile_udf(col(\"raw_score\"), \n",
    "                                                          percentiles_broadcast)\n",
    "                                      )\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_with_percentile.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_with_percentile.write.format(\"parquet\").saveAsTable(\"tests_es.output_pre2post_201802_v2_notprepared\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are done! There are only two tasks left:\n",
    "\n",
    "+ Deanonymize the `MSISDN` column\n",
    "+ Figure out a way to return `results_with_percentile` (with the `MSISDN` column deanonymized) back to Spain CVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
